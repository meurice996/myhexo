<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习 on zn.yan</title>
        <link>https://demo.stack.jimmycai.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 机器学习 on zn.yan</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 30 Nov 2022 11:01:19 +0000</lastBuildDate><atom:link href="https://demo.stack.jimmycai.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>图神经网络</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Wed, 30 Nov 2022 11:01:19 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;h1 id=&#34;graph-neural-network&#34;&gt;Graph Neural Network
&lt;/h1&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;h3 id=&#34;graph&#34;&gt;Graph
&lt;/h3&gt;&lt;p&gt;图$G$=点集$N$+边集$E$&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/graph_sample.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;why&#34;&gt;Why?
&lt;/h3&gt;&lt;h4 id=&#34;classification&#34;&gt;Classification
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;根据分子结构预测其性质&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原子用节点表示，化学键用边表示。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/classfication.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;考虑角色之间的关系来预测一个人是不是凶手&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/classification2.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;h4 id=&#34;generation&#34;&gt;Generation
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Generator生成出想要的分子结构&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/generation.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h3 id=&#34;how&#34;&gt;How?
&lt;/h3&gt;&lt;p&gt;对于一个无标签的数据，如何利用仅有的有标签数据和它与其他节点的关系？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solution1：类似CNN的方法，CNN利用卷积来获取一张图片的特征图，在卷积的乘法、加法过程中，利用了&lt;em&gt;邻近&lt;/em&gt;像素点的信息。&amp;raquo; &lt;strong&gt;Spatial-based convolution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Solution2：将信号转到Fourier domain，在&lt;em&gt;频域&lt;/em&gt;上对信号和滤波器做相乘，再做傅立叶反变换。 &amp;raquo; &lt;strong&gt;Spectral-based convolution&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gnn-roadmap&#34;&gt;GNN Roadmap
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/roadmap.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h2 id=&#34;tasks-dataset-and-benchmark&#34;&gt;Tasks, Dataset, and Benchmark
&lt;/h2&gt;&lt;h2 id=&#34;spatial-based-gnn&#34;&gt;Spatial-based GNN
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Aggregate：用邻居的feature更新下一层的hidden state&lt;/li&gt;
&lt;li&gt;Readout：所有节点的feature集合起来代表整个图&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/spatial-based%20convolution.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;nn4g-neural-networks-for-graph&#34;&gt;NN4G (Neural Networks for Graph)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/4773279&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neural Network for Graphs: A Contextual Constructive Approach | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如输入的图有5个节点$v_{0\dots5}$，5条边，每个节点都有自己的feature，例如对于某种化学物质，将每个节点（原子）的性质作为这个节点的feature，。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/input_layer.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;首先需要对每个节点的feature做embeding，得到$h_{0\dots 5}^0$，得到Hidden layer 0。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/hidden0.png&#34; style=&#34;zoom:33%;&#34; /&gt;
$$
h_3^1=\hat{w}_{1,0}(h_0^0+h_2^0+h_4^0)+h_3^0
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/hidden1.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在做Readout时，将各层的节点feature加起来取平均，各自做一个transform后加起来代表整个图的feature。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/readout.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;dcnn-diffusion-convolution-neural-network&#34;&gt;DCNN (Diffusion Convolution Neural Network)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1511.02136&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1511.02136] Diffusion-Convolutional Neural Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
h_3^1=w_3^1MEAN(d(3,\cdot)=2)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/DCNN.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
$$
\begin{bmatrix}
h_1^k \\
\vdots \\
h_1^1 \\
h_1^0
\end{bmatrix}
\times
W=y_1
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/DCNN1.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;dgc-diffusion-graph-convolution&#34;&gt;DGC (Diffusion Graph Convolution)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1707.01926.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1707.01926.pdf (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与DCNN类似，把各层加起来。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/DGC.png&#34; style=&#34;zoom:30%;&#34; /&gt;
&lt;h3 id=&#34;monet-mixture-model-networks&#34;&gt;MoNET (Mixture Model Networks)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1611.08402&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1611.08402] Geometric deep learning on graphs and manifolds using mixture model CNNs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
\mathbf{u}(x,y)=(\frac{1}{\sqrt{\deg(x)}},\frac{1}{\sqrt{\deg{y}}})^T
$$&lt;p&gt;将距离$\mathbf{u}$（与节点的度有关）做一个transform后，再做加权求和。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/MoNET.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;graphsage&#34;&gt;GraphSAGE
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.02216&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1706.02216] Inductive Representation Learning on Large Graphs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在做Aggregation时，使用mean、max-pooling或LSTM。&lt;/p&gt;
&lt;p&gt;对一个节点来说，邻居是无序的，用LSTM来做时，每次update时都随便sample出一个顺序，来忽略顺序的影响，学到比较好表示。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/GraphSAGE.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h3 id=&#34;gat-graph-attention-networks&#34;&gt;GAT (Graph Attention Networks)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1710.10903&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1710.10903] Graph Attention Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不只要做加权求和，权重让网络自己去学，对邻居做Attention。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：节点feature $\mathbf{h}={\vec{h}_1,\vec{h}_2,\dots,\vec{h}_N},\vec{h}_i\in\mathbb{R}^F$&lt;/li&gt;
&lt;li&gt;计算energy：$e_{ij}=\alpha(\mathbf{W}\vec{h}_i,\mathbf{W}\vec{h}_j)$&lt;/li&gt;
&lt;li&gt;通过邻居计算Attention分数：$\alpha_{ij}=\frac{\exp{(\text{LeakyReLU}(\vec{\mathbf{a}}^T[\mathbf{W}\vec{h}_i\Vert\mathbf{W}\vec{h}&lt;em&gt;j]))}}{\sum&lt;/em&gt;{k\in \mathcal{N}_i}\exp{(\text{LeakyReLU}(\mathbf{\vec{a}}^T[\mathbf{W}\vec{h}_i\Vert\mathbf{W}\vec{h}_k])})}$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/GAT.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;gin-graph-isomorphism-network&#34;&gt;GIN (Graph Isomorphism Network)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=ryGs6iA5Km&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How Powerful are Graph Neural Networks? | OpenReview&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
h_v^{(k)}=\text{MLP}^{(k)}\left(\left(1+\epsilon^{(k)}\right)\cdot h_v^{(k-1)}+\sum_{u\in\mathcal{N}(v)}h_{u}^{(k-1)}\right)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/GIN.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;graph-signal-processing--spectral-based-gnn&#34;&gt;Graph Signal Processing &amp;amp; Spectral-Based GNN
&lt;/h2&gt;&lt;h3 id=&#34;signal-and-system&#34;&gt;Signal and System
&lt;/h3&gt;&lt;p&gt;时域上的卷积等于频域上的相乘。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/freq.png&#34; style=&#34;zoom:33%;&#34; /&gt;
$$
\vec{A}=\sum_{k=1}^Na_k\hat{v}_k
$$$$
a_j=\vec{A}\cdot \hat{v}_j
$$$$
\hat{v}_i\cdot\hat{v}_j=\delta_{ij}
$$$$
x(t)=\sum_{k=-\infty}^{\infty}a_ke^{jk\omega_0t}=\sum_{k=-\infty}^{\infty}a_k\phi_k(t)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/time-freq.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;傅立叶变换：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/fourier-trans.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;spectral-graph-theory&#34;&gt;Spectral Graph Theory
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;图$G=(V,E)$，节点数量$N=\vert V\vert$；&lt;/li&gt;
&lt;li&gt;邻接矩阵（权重矩阵）$A\in \mathbb{R}^{N\times N}$：$A_{i,j}=0\ \text{if}\ e_{i,j}\notin E\ \text{else}\ A_{i,j}=w(i,j)$&lt;/li&gt;
&lt;li&gt;度矩阵$D\in \mathbb{R}^{N\times N}$：&lt;/li&gt;
&lt;/ul&gt;
$$
D_{i,j}=\left\{
\begin{aligned} 
&amp;d(i)\ \ (\sum_kA_{i,k})\quad &amp;\text{if}\ i=j \\
&amp;0\quad &amp;\text{if}\ i\ne j
\end{aligned}\right.
$$&lt;ul&gt;
&lt;li&gt;$f:V\rightarrow \mathbb{R}^N$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于下所示的图，有节点$v_{0\dots 3}$，每个节点上面的信号$f(i)$，例如对于城市路网图，节点对应不同的城市，信号对应着城市的气温、人口等。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory1.png&#34; style=&#34;zoom:30%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;图拉普拉斯矩阵$L=D-A,L\succcurlyeq0$（半正定，对称）&lt;/li&gt;
&lt;li&gt;可以对$L$做谱分解：$L=U\Lambda U^T$&lt;/li&gt;
&lt;li&gt;对角矩阵$\Lambda=\text{diag}(\lambda_0,\dots,\lambda_{N-1})\in \mathbb{R}^{N\times N}$，$\lambda_l$：frequency&lt;/li&gt;
&lt;li&gt;正交矩阵$U=[u_0,\dots,u_{N-1}]\in \mathbb{R}^{N\times N}$，$u_l$：$\lambda_l$对应的basis&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/graph-laplacian.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h4 id=&#34;vertex-domain-signal&#34;&gt;Vertex domain signal
&lt;/h4&gt;$$
D=
\begin{bmatrix}
2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 
\end{bmatrix}
\quad
A=
\begin{bmatrix}
0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 
\end{bmatrix}
$$$$
L=
\begin{bmatrix}
2 &amp; -1 &amp; -1 &amp; 0 \\
-1 &amp; 3 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 2 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 1
\end{bmatrix}
$$$$
\Lambda=
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
\quad
U=
\begin{bmatrix}
0.5 &amp; -0.41 &amp; 0.71 &amp; -0.29 \\
0.5 &amp; 0 &amp; 0 &amp; 0.87 \\
0.5 &amp; -0.41 &amp; -0.71 &amp; -0.29 \\
0.5 &amp; 0.82 &amp; 0 &amp; -0.29
\end{bmatrix}
$$&lt;p&gt;
对应在图$G$上：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory2.png&#34; style=&#34;zoom:34%;&#34; /&gt;
&lt;h4 id=&#34;discrete-time-fourier-basis&#34;&gt;Discrete time Fourier basis
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/discrete-time-fourier-basis.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;interpreting-vertex-frequency&#34;&gt;Interpreting vertex frequency
&lt;/h4&gt;&lt;p&gt;把$L$看成是对图的一个运算，对于一个图信号$f$，$Lf=(D-A)f=Df-Af$，对于4个节点的图，会得到形如$Lf=\begin{bmatrix}a \ b \c \ d\end{bmatrix}$的结果，$Lf$代表着某一个节点和他周围节点的能量差异。&lt;/p&gt;
$$
\begin{aligned}
(Lf)(vi)&amp;=\sum_{v_j\in V}w(i,j)(f(v_i)-f(v_j)) \\
&amp;\text{where}\ w_{i,j}\ \text{is the}\ (i,j)^{th}\ \text{entry of}\ A \\
f^TLf&amp;=\sum_{v_i\in V}f(v_i)\sum_{v_j\in V}w_{i,j}(f(v_i)-f(v_j)) \\
&amp;=\cdots \\
&amp;=\frac{1}{2}\sum_{v_i\in V}\sum_{v_j\in V}w_{i,j}\left(f(v_i)-f(v_j)\right)^2
\end{aligned}
$$&lt;p&gt;
可以用$f^TLf$来量化某一个图信号的频率。&lt;/p&gt;
&lt;p&gt;有$u_i^TLu_i=u_i^T\lambda_iu_i=\lambda u_i^Tu_i=\lambda_i$，此时再回到前面$\lambda$和$u$对应在图$G$上的情况，比较小的$\lambda$对应着低通部分。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory2.png&#34; style=&#34;zoom:34%;&#34; /&gt;
&lt;p&gt;一个特例是一条线上的20个节点$v_{0\dots19}$：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/special-example.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在$\lambda=0$时是一个DC-component，随着$\lambda$越来越大，从一个低频sin变成高频sin。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/special-example-2.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h4 id=&#34;graph-fourier-transform-of-signal-x&#34;&gt;Graph Fourier Transform of signal $x$
&lt;/h4&gt;$$
\hat{x}=U^Tx
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/transform.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;分别乘内积，将各个节点的$f(i)$转为$u_i\cdot f=\lambda_i$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/transform2.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;h4 id=&#34;inverse-graph-fourier-transform-of-signal-hatx&#34;&gt;Inverse Graph Fourier Transform of signal $\hat{x}$
&lt;/h4&gt;$$
x=U\hat{x}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/inverse.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;filtering&#34;&gt;Filtering
&lt;/h4&gt;&lt;p&gt;前面将时域转到频域，是为了找到在图上做滤波的方式。&lt;/p&gt;
&lt;p&gt;例如对于频域上的信号$X(j\omega)$，有一个$H(j\omega)$的滤波，直接相乘会得到滤波结果$X^\prime(j\omega)$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering.png&#34; style=&#34;zoom:33%;&#34; /&gt;
$$
\hat{y}=g_\theta(\Lambda)\hat{x}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering2.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
$$
g_\theta(\lambda_i)=\theta_i
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering3.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
$$
\begin{aligned}
y=U\hat{y}&amp;=Ug_\theta(\Lambda)U^T \\
&amp;=g_\theta(U\Lambda U^T)x \\
&amp;=g_\theta(L)x
\end{aligned}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering4.png&#34; style=&#34;zoom:38%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;训练模型需要学习的参数是$g_\theta(\Lambda)$。&lt;/p&gt;
$$
g_\theta(L)=\log(I+L)=L-\frac{L^2}{2}+\frac{L^3}{3}+\cdots,\lambda_{\max}\lt 1
$$&lt;p&gt;
此时存在的问题是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算的复杂度为$O(N)$&lt;/li&gt;
&lt;li&gt;$g_\theta(L)$可能会让模型可能会学到一些本不希望学到的内容（not &lt;em&gt;localize&lt;/em&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;localize&#34;&gt;Localize
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory1.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;p&gt;对于上图，例如$g_\theta(L)=L$时，$y=Lx$：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/q-L1.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;p&gt;$L$在有些地方（例如1行4列）的值为0，这是因为$v_0$到$v_3$之间没有任何距离为1的路径，此时$v_3$不会影响到$v_0$。&lt;/p&gt;
&lt;p&gt;但当$g_\theta(L)=L^2$时，$y=L^2X$：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/q-L2.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;p&gt;$L^2$不再有为值为0的地方，对于任意节点，到其他节点的距离都小于2，此时$v_3$会影响到$v_0$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt; Let $G$ be a weighted graph, and $\mathscr{L}$ the graph Laplacian of $G$. Fix an integer $s\gt 0$, and pick vertices $m$ and $n$. Then $(\mathscr{L}^s)&lt;em&gt;{m,n}=0$ whenever $d&lt;/em&gt;{G}(m,n)\gt s$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这种情况不是Localize的，例如CNN中，当我们选择$3\times 3$的kernel时，就只能看到邻近区域的内容，希望这个区域变大的话可以选择更大的kernel。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/CNN-localize.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h3 id=&#34;chebnet&#34;&gt;ChebNet
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1606.09375&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1606.09375] Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多可参考：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://distill.pub/2021/gnn-intro/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Gentle Introduction to Graph Neural Networks (distill.pub)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>长短期记忆网络</title>
        <link>https://demo.stack.jimmycai.com/p/%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Thu, 27 Oct 2022 16:37:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Understanding LSTM Networks &amp;ndash; colah&amp;rsquo;s blog&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;long-short-term-memory&#34;&gt;Long Short-Term Memory
&lt;/h1&gt;&lt;h2 id=&#34;rnn&#34;&gt;RNN
&lt;/h2&gt;&lt;p&gt;传统神经网络不能做到根据前文来理解现在的内容，循环神经网络（Recurrent Neural Network, RNN）解决了这个问题，RNN的结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-rolled.png&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;对于神经网络$A$，接收输入$x_t$，输出$h_t$，其中的循环使得信息可以向后传递，RNN与普通神经网络的区别并不是很大，可以认为是同一网络的多个副本，每个副本将一条信息传递给后续的网络，其展开后的结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-unrolled.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;p&gt;链式结构表明RNN与序列数据密切相关。&lt;/p&gt;
&lt;h2 id=&#34;长程依赖问题&#34;&gt;长程依赖问题
&lt;/h2&gt;&lt;p&gt;RNN的亮点在于，其能够将以前的信息和当前的任务连接起来，例如要预测“the clouds are in the &lt;em&gt;[MASK]&lt;/em&gt;”，RNN会比较容易地通过前文预测出这个词是“&lt;em&gt;sky&lt;/em&gt;”，要预测的位置和相关信息距离很近，RNN可以学会使用过去的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-shorttermdepdencies.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;p&gt;下面考虑另一种情况，例如要预测“I grew up in France, &amp;hellip;, I speak fluent &lt;em&gt;[MASK]&lt;/em&gt;”，附近的信息表明这个词可能是某种语言的名称，但是如果要预测出“&lt;em&gt;French&lt;/em&gt;”，需要从获取更远位置的信息，然而，随着预测位置和相关信息的距离增大，RNN会难以使用过去的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-longtermdependencies.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;h2 id=&#34;lstm&#34;&gt;LSTM
&lt;/h2&gt;&lt;p&gt;长短期记忆网络（Long Short-Term Memory, LSTM）是一种特殊的RNN，能够学习到长期的依赖性。&lt;/p&gt;
&lt;p&gt;标准RNN中，重复模块具有非常简单的结构，例如单个tanh层：
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-SimpleRNN.png&#34; style=&#34;zoom:36%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;LSTM同样具有类似的链式结构，重复模块中有4个交互层：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-chain.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;p&gt;上图中，每条线携带一个完整的向量，从一个节点的输出到其他节点的输入，粉色圆圈代表逐点（point-wise）操作（例如向量加法），黄色框是神经网络层，行合并表示对矩阵的串联（concatenation）连接，行分叉表示内容被复制且副本转到不同的位置。&lt;/p&gt;
&lt;h2 id=&#34;lstm的核心思想&#34;&gt;LSTM的核心思想
&lt;/h2&gt;&lt;p&gt;相比RNN只有一个传递状态$h_t$，LSTM有两个传递状态，$C_t$（cell state）和$h_t$（hidden state）。&lt;/p&gt;
&lt;p&gt;cell state（顶部的水平线）是LSTM的关键，cell state只有一些微小的线性作用，改变很慢。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-C-line.png&#34; style=&#34;zoom:42%;&#34; /&gt;
&lt;p&gt;LSTM通过调整门控结构来选择性让信息通过。&lt;/p&gt;
&lt;p&gt;门由Sigmoid层与按位乘法操作（pointwise multiplication operation）组成。&lt;/p&gt;
&lt;h2 id=&#34;lstm的工作方式&#34;&gt;LSTM的工作方式
&lt;/h2&gt;&lt;h3 id=&#34;遗忘阶段&#34;&gt;遗忘阶段
&lt;/h3&gt;&lt;p&gt;LSTM的第一步是由遗忘门控$\sigma$决定要从cell state中丢弃哪些信息，输入$h_{t-1}$和$x_t$，并对cell state中的每一个元素输出一个0到1之间的数字，1代表完全保留而0代表完全遗忘。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-f.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
f_t=\sigma (W_f\cdot [h_{t-1},x_t]+b_f)
$$
&lt;h3 id=&#34;选择记忆阶段&#34;&gt;选择记忆阶段
&lt;/h3&gt;&lt;p&gt;LSTM的第二步是决定cell state要保留哪些信息，输入门控$\sigma$决定要更新哪些值，$\tanh$创建一个新候选值组成的向量$\widetilde{C}_t$，并加入到状态中。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-i.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
\begin{align}
i_t&amp;=\sigma(W_i\cdot [h_{t-1},x_t]+b_i)\\
\widetilde{C}_t&amp;=\tanh(W_C\cdot[h_{t-1},x_t]+b_C)
\end{align}
$$
至此，我们可以将cell state从$C_{t-1}$更新为$C_t$。将旧的cell state与$f_t$相乘，将决定遗忘的信息丢弃，然后添加$i_t*\widetilde{C}_t$。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-C.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
C_t=f_t*C_{t-1}+i_t*\widetilde{C}_t
$$
&lt;h3 id=&#34;输出阶段&#34;&gt;输出阶段
&lt;/h3&gt;&lt;p&gt;LSTM的最后一步是决定要输出什么，这个输出基于cell state。首先经过输出门控$\sigma$，决定我们要输出cell state的哪些部分，然后将cell state通过$\tanh$，并与输出门控的输出相乘，就得到了决定输出的部分$h_t$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-o.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
\begin{align}
o_t&amp;=\sigma(W_o\cdot[h_{t-1},x_t]+b_o) \\
h_t&amp;=o_t *\tanh(C_t)
\end{align}
$$</description>
        </item>
        <item>
        <title>图像质量评价</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/</link>
        <pubDate>Sun, 31 Jul 2022 22:24:28 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/</guid>
        <description>&lt;p&gt;本文总结了图像质量评估的相关内容，同时简要总结了基于深度学习的无参考图像质量评价方法。&lt;/p&gt;
&lt;h1 id=&#34;image-quality-assessment&#34;&gt;Image Quality Assessment
&lt;/h1&gt;$$
\text{QA}\left\{
\begin{aligned}
&amp;视频质量评估\text{VQA} \\
&amp;图像质量评估\text{IQA}
\left\{
\begin{aligned}
&amp;主观评估\text{S-IQA}\ ——\ 主观评分\\
&amp;客观评估\text{O-IQA}
\left\{
\begin{aligned}
&amp;全参考评估\text{FR-IQA} \\
&amp;半参考评估\text{RR-IQA} \\
&amp;无参考评估\text{NR-IQA} \\
\end{aligned}
\right.
\end{aligned}
\right.
\end{aligned}
\right.
$$&lt;h2 id=&#34;主观评估s-iqa&#34;&gt;主观评估S-IQA
&lt;/h2&gt;&lt;p&gt;主观评估方法主要可分为两种：绝对评价和相对评价。&lt;/p&gt;
&lt;p&gt;绝对评价是由观察者根据自己的知识和理解，按照某些特定评价性能对图像的绝对好坏进行评价。在具体执行过程中通常采用双刺激连续质量分级法（Double Stimulus Continuous Scale, DSCQS）将待评价图像和原始图像按一定规则交替播放持续一定时间给观察者，然后在播放后留出一定的时间间隔供观察者打分，最后将所有给出的分数取平均作为该序列的评价值。&lt;/p&gt;
&lt;p&gt;相对评估中没有原始图像作为参考，是由观察者对一批待评价图像进行相互比较，从而判断出每个图像的优劣顺序，并给出相应的评价值。在具体执行过程中通常采用单刺激连续质量评价方法（Single Stimulus Continuous QualityEvaluation, SSCQE）将一批待评价图像按照一定的序列播放，此时观察者在观看图像的同时给出待评图像相应的评价分值。&lt;/p&gt;
&lt;p&gt;平均意见得分（Mean Opinion Score, MOS）是图像质量最具代表性的主观评价方法，它通过对观察者的评价归一判断图像质量。类似的评价方式还有平均主观得分差异（Differential mean opinion score, DMOS）。&lt;/p&gt;
&lt;p&gt;绝对评价尺度：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;分数&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;质量尺度&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;妨碍尺度&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;丝毫看不出图像质量变坏&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;能看出图像质量变化但不妨碍观看&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;清楚看出图像质量变坏，对观看稍有妨碍&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一般&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;对观看有妨碍&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常严重的妨碍观看&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常差&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;相对评价尺度的评分标准：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;分数&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;相对测量尺度&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;绝对测量尺度&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一群中最好的&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;好于该群中平均水平&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;该群中的平均水平&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一般&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;差于该群中平均水平&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;该群中最差的&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常差&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;客观评估o-iqa&#34;&gt;客观评估O-IQA
&lt;/h2&gt;&lt;h3 id=&#34;评价指标&#34;&gt;评价指标
&lt;/h3&gt;&lt;p&gt;检验一种客观评估算法是否可靠的标准是它“是否与人的主观质量判断相一致”，为了确认某种客观评价指标与主观得分之间的一致性关系，常用四个指标：RMSE、PLCC、SROCC和KROCC。&lt;/p&gt;
&lt;h4 id=&#34;rmse&#34;&gt;RMSE
&lt;/h4&gt;$$
\text{RMSE}=\sqrt{\frac{\sum_{i=1}^n(s_i-p_i)^2}{n}}
$$&lt;p&gt;
&lt;strong&gt;RMSE越接近0，表示算法的性能越好&lt;/strong&gt;。为了避免MOS取值不一样而导致RMSE的计算受影响，所以计算前需要归一化。&lt;/p&gt;
&lt;h4 id=&#34;plcc皮尔逊系数&#34;&gt;PLCC（皮尔逊系数）
&lt;/h4&gt;$$
\text{PLCC}=\frac{Cov(S,P)}{\sigma(S)\cdot\sigma(P)}=\frac{\sum_{i=1}^n(p_i-\bar{p})(s_i-\bar{s})}{\sqrt{\sum_{i=1}^n(p_i-\bar{p})^2}\cdot\sqrt{\sum_{i=1}^n(s_i-\bar{s})^2}}
$$&lt;p&gt;PLCC取值范围为$[-1,1]$，当PLCC的值为零时，表示两组数据完全不相关，&lt;strong&gt;PLCC的值大于0时表示正相关，值越大表示正相关性越强&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;srocc&#34;&gt;SROCC
&lt;/h4&gt;&lt;p&gt;SROCC和KROCC将具体数值抽象为排序等级。&lt;/p&gt;
$$
\text{SROCC}=1-\frac{6\sum_{i=1}^nd_i^2}{n(n^2-1)} \tag{1}
$$&lt;p&gt;
其中$d$为$S$和$P$的等级之差（$rank(s)-rank(p)$）。&lt;/p&gt;
&lt;p&gt;SROCC计算过程可参考下例：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$S$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$P$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$rank(s)$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$rank(p)$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$d$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$d^2$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;56&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;66&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;9&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;25&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;75&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;70&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;45&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;40&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;10&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;10&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;71&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;60&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;7&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;9&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;62&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;65&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;6&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;64&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;56&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;9&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;16&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;58&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;59&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;8&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;8&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;80&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;77&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;76&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;67&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;61&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;63&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;7&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;6&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;带入$d^2$的值即可求得SROCC。&lt;/p&gt;
$$
\text{SROCC}=\frac{\sum_{i=1}^n(kp_i-\bar{kp})(ks_i-\bar{ks})}{\sqrt{\sum_{i=1}^n(kp_i-\bar{kp})^2}\cdot\sqrt{\sum_{i=1}^n(ks_i-\bar{ks})^2}}\tag{2}
$$&lt;p&gt;其中$kp$、$ks$分别表示$rank(s)$、$rank(p)$。&lt;/p&gt;
&lt;p&gt;例如，对于两个第二名，则将等级定位1.5（第一和第二的平均）；对于两个第三名，则将等级定位3.5。&lt;/p&gt;
&lt;p&gt;可以看出，SROCC就是“等级”的PLCC。&lt;/p&gt;
&lt;h4 id=&#34;krocc&#34;&gt;KROCC
&lt;/h4&gt;&lt;p&gt;将MOS和算法预测得分表示为数据对的形式：$(s_1,p_1),(s_2,p_2),\cdots,(s_n,p_n)$，从$n$个数据对中任选两对，组成$[(x_i,y_i),(x_j,y_j)]$（$i\neq j$），一共有$\frac{n(n+1)}{2}$对，将这些对按照下面的情况进行划分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P$：$x_i\gt x_j\ and\ y_i\gt y_j$ 或 $x_i\lt x_j\ and\ y_i\lt y_j$ 同序对&lt;/li&gt;
&lt;li&gt;$Q$：$x_i\gt x_j\ and\ y_i\lt y_j$ 或 $x_i\lt x_j\ and\ y_i\gt y_j$ 逆序对&lt;/li&gt;
&lt;li&gt;$X_0$：$x_i=x_j\ and\ y_i\gt y_j$ 或 $x_i=x_j\ and\ y_i\lt y_j$&lt;/li&gt;
&lt;li&gt;$Y_0$：$x_i\gt x_j\ and\ y_i=y_j$ 或 $x_i\lt x_j\ and\ y_i=y_j$&lt;/li&gt;
&lt;li&gt;$XY_0$：$x_i=x_j\ and\ y_i=y_j$&lt;/li&gt;
&lt;/ul&gt;
$$
\text{KROCC}=\frac{P-Q}{\sqrt{P+Q+X_0}\cdot\sqrt{P+Q+Y_0}}
$$&lt;h3 id=&#34;全参考评估&#34;&gt;全参考评估
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;MSE&lt;/li&gt;
&lt;li&gt;PSNR&lt;/li&gt;
&lt;li&gt;SSIM&lt;/li&gt;
&lt;li&gt;VIF&lt;/li&gt;
&lt;li&gt;FSIM&lt;/li&gt;
&lt;li&gt;GMSD&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mse&#34;&gt;MSE
&lt;/h4&gt;$$
\text{MSE}=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2
$$&lt;h4 id=&#34;psnr峰值信噪比&#34;&gt;PSNR（峰值信噪比）
&lt;/h4&gt;$$
\text{PSNR}=10\lg\left(\frac{MAX_I^2}{MSE}\right)
$$&lt;p&gt;
其中，$MAX_I$为图片可能的最大像素值，例如对于8 bit存储的图像，$MAX_I=2^8-1=255$。&lt;/p&gt;
&lt;h4 id=&#34;ssim&#34;&gt;SSIM
&lt;/h4&gt;&lt;p&gt;SSIM首先在文章Image Quality Assessment: From Error Visibility to Structural Similarity（IEEE-2004）被引入，作者提出两个要点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大多数图像质量评估技术依赖于量化参考图像和样本图像之间的误差。一种常用的度量是量化样本和参考图像之间对应的每个像素的值的差异（例如均方误差）。&lt;/li&gt;
&lt;li&gt;人类视觉感知系统能够从一个场景中识别结构信息，从而识别从参考场景和样本场景中提取的信息之间的差异。因此，复制此行为的指标将在涉及区分样本图像和参考图像的任务中执行得更好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SSIM从一幅图像中提取3个关键特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  \mu_x=\frac{1}{N}\sum_{i=1}^Nx_i
  $$&lt;p&gt;
其中$x_i$为图像$x$的第$i$个像素值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
$$
  \sigma_x=\sqrt{\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_x)^2}
  $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;结构：通过一个合并公式来完成&lt;/p&gt;
$$
  r(X,Y)=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}
  $$$$
  r=\frac{\sigma_{xy}}{\sigma_x\sigma_y}
  $$$$
  \sigma_{xy}=\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_x)(y_i-\mu_y)
  $$&lt;/li&gt;
&lt;/ul&gt;
$$
l(x,y)=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}
$$$$
c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}
$$$$
s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3}
$$$$
SSIM(x,y)=[l(x,y)]^\alpha\cdot[c(x,y)]^\beta\cdot [s(x,y)]^\gamma
$$&lt;p&gt;
$\alpha,\beta,\gamma$用来表示三个模块的重要性。&lt;/p&gt;
$$
SSIM(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}
$$&lt;h3 id=&#34;半参考评估&#34;&gt;半参考评估
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;RRED&lt;/li&gt;
&lt;li&gt;OSVP&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rred&#34;&gt;RRED
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Reduced reference entropic differencing for image quality assessment（IEEE Trans. Image Process，2012）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;osvp&#34;&gt;OSVP
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Orientation selectivity based visual pattern for reduced-reference image quality assessment（Information Science，2016）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;无参考评估&#34;&gt;无参考评估
&lt;/h3&gt;&lt;p&gt;由于没有无失真源图像的参考信息，无参考质量评估方法仅根据失真图像来学习预测图像质量分数，难度大于全参考和部分参考评估方法。&lt;/p&gt;
&lt;h4 id=&#34;传统图像清晰度评价算法&#34;&gt;传统图像清晰度评价算法
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Tenengrad梯度函数&lt;/li&gt;
&lt;li&gt;SMD（灰度方差）函数&lt;/li&gt;
&lt;li&gt;Brenner梯度函数&lt;/li&gt;
&lt;li&gt;方差函数&lt;/li&gt;
&lt;li&gt;能量梯度函数&lt;/li&gt;
&lt;li&gt;Vollath函数&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reblur&#34;&gt;Reblur
&lt;/h4&gt;&lt;p&gt;如果一幅图像已经模糊了，那么再对它进行一次模糊处理，高频分量变化不大；但如果原图是清楚的，对它进行一次模糊处理，则高频分量变化会非常大。因此可以通过对待评测图像进行一次高斯模糊处理，得到该图像的退化图像，然后再比较原图像和退化图像相邻像素值的变化情况，根据变化的大小确定清晰度值的高低，计算结果越小表明图像越清晰，反之越模糊，这种思路可称作基于二次模糊的清晰度算法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NRSS（梯度结构相似度）&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;nrss&#34;&gt;NRSS
&lt;/h5&gt;&lt;p&gt;NRSS算法的步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step1. 为待评价图像构造参考图像：定义待评价图像为$I$，NRSS算法首先参考图像$I_r=LPF(I)$，即对待评价图像$I$进行低通滤波得到参考$I_r$；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step2. 提取图像$I$和$I_r$的梯度信息：利用人眼对水平和竖直方向的边缘信息最为敏感的特性，使用Sobel算子分别提取水平和竖直方向的边缘信息，定义$I$和$I_r$的梯度图像是$G$和$G_r$；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step3. 找出梯度图像$G$中梯度信息最丰富的$N$个图像块：通过计算方差找出梯度图像$G$中梯度信息最丰富的$N$个图像块，方差越大说明梯度信息越丰富，根据找到的$G$中的前$N$个块，找出对应的$G_r$的前$N$个块；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
$$
  \text{NRSS}=1-\frac{1}{N}\sum_{i=1}^NSSIM(x_i,y_i)
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;distortion-specific&#34;&gt;Distortion specific
&lt;/h4&gt;&lt;p&gt;早期的传统方法通过假设存在特定某一类型的失真来评价图像质量，即量化特定失真类型，如块效应、模糊、振铃效应、噪声、压缩或传输损伤等。JNBM、CPBDM和LPCM专注于评价Blur类型的失真图像，NJQA和JPEG-NR分别评价噪声失真和JPEG压缩损伤失真。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPBDM&lt;/li&gt;
&lt;li&gt;LPCM&lt;/li&gt;
&lt;li&gt;NJQA&lt;/li&gt;
&lt;li&gt;JPEG-NR&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;nss&#34;&gt;NSS
&lt;/h4&gt;&lt;p&gt;近年来表现优良的无参考图像质量评估模型大部分都是基于自然场景统计特性 (Natural Scene Statistics, NSS)，在不对失真类型做任何假设的前提下设计提取图像特征，通过机器学习回归算法进行质量预测。所选特征具有广泛的感知相关性，且合适的回归模型能自适应地将特征映射到数据集中的主观质量分数，因此基于NSS特征的无参考图像质量评估方法比早期的模型更加通用和一般化。NSS表明经过适当规范化的高质量真实世界摄像图像会遵行一定的统计规律，基于NSS统计量的特征量更能准确预测图像失真。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BRISQUE&lt;/li&gt;
&lt;li&gt;GM-LOG&lt;/li&gt;
&lt;li&gt;HIGRADE&lt;/li&gt;
&lt;li&gt;FRIQUEE&lt;/li&gt;
&lt;li&gt;VBLINDS[V]&lt;/li&gt;
&lt;li&gt;VIDEVAL[V]&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;brisque&#34;&gt;BRISQUE
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;No-Reference Image Quality Assessment in the Spatial Domain（IEEE Trans. Image Process，2012）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BRISQUE是首歌将图像的自然场景统计特性应用到图像质量评估上的模型。其思想是从图像中提取MSCN系数（mean subtracted contrast normalized, 均值减去对比度归一化），将MSCN系数拟合成非对称性广义高斯分布（AGGD），提取拟合的高斯分布的特征，输入到支持向量机SVM中做回归，从而得到图像质量的评估结果。&lt;/p&gt;
&lt;p&gt;自然图像的像素强度分布与失真图像的像素强度分布不同。当我们对像素强度进行归一化并在这些归一化强度上计算分布时，分布上的差异更加明显。特别地，在归一化之后，自然图像的像素强度近似服从高斯分布（贝尔曲线），而非自然或失真图像的像素强度则不服从高斯分布。因此，分布曲线与理想高斯曲线的偏差是图像失真量的度量。&lt;/p&gt;
&lt;p&gt;BRISQUE的整体流程有三步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 提取自然场景统计信息（NSS）&lt;/li&gt;
&lt;li&gt;Step2. 计算特征向量&lt;/li&gt;
&lt;li&gt;Step3. 预测图像质量分数&lt;/li&gt;
&lt;/ul&gt;
$$
\hat{I}(i,j)=\frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}
$$$$
\mu=\mathbf{W}*\mathbf{I}\quad\sigma=\sqrt{\mathbf{W}*(\mathbf{I}-\mu)^2}
$$$$
\begin{aligned}
H(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i,j+1)} \\
V(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i+1,j)} \\
D1(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i+1,j+1)} \\
D2(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i+1,j-1)} \\
\end{aligned}
$$&lt;p&gt;
至此，已从原始图像中生成了5张图像：1张MSCN图像和4张成对乘积图像。&lt;/p&gt;
&lt;p&gt;接下来，我们将使用这5张图像来计算大小为$36\times1$的特征向量。&lt;/p&gt;
$$
\begin{aligned}
f(x;\alpha,\sigma^2)=\frac{\alpha}{2\beta\Gamma(1/\alpha)}\exp\left(-\left(\frac{|x|}{\beta}\right)^2\right)
\end{aligned}
$$$$
\beta=\sigma\sqrt{\frac{\Gamma(1/\alpha)}{\Gamma(3/\alpha)}},\quad\Gamma(a)=\int_0^\infty t^{a-1}e^{-t}dt\quad (a\gt0).
$$$$
f(x;\nu,\sigma_l^2,\sigma_r^2)=
\left\{
\begin{aligned}
&amp;\frac{\nu}{(\beta_l+\beta_r)\Gamma(1/\nu)}\exp\left(-\left(\frac{-x}{\beta_l}\right)^\nu\right)\quad x\lt 0 \\
&amp;\frac{\nu}{(\beta_l+\beta_r)\Gamma(1/\nu)}\exp\left(-\left(\frac{-x}{\beta_r}\right)^\nu\right)\quad x\geqslant 0
\end{aligned}
\right.
$$$$
\beta_l=\sigma_l\sqrt{\frac{\Gamma(1/\nu)}{\Gamma(3/\nu)}},\quad \beta_r=\sigma_r\sqrt{\frac{\Gamma(1/\nu)}{\Gamma(3/\nu)}}.
$$&lt;p&gt;
将原始图像缩小到原始大小的一半，并重复相同过程，便得到了$36\times1$的特征向量。&lt;/p&gt;
&lt;p&gt;将特征向量送入到机器学习算法中进行训练，即可使用模型对质量分数进行预测。&lt;/p&gt;
&lt;p&gt;用广义高斯分布来拟合MSCN的分布，GGD的形状参数α和分布方差sigma。接下来对MSCN系数进行二阶分析，在垂直、水平、主对角和次对角方向上进行非对称广义高斯分布的拟合，分别得到表征分布形状的四个参数。这样在原图像尺度上得到18个特征。图像和视频本质上是多尺度的，失真可以在不同的尺度上表现得不同。因此在降采样2倍的图像上再次提取18维度的特征，这样BRISQUE的特征集是36维。通过机器学习训练出能够从高维特征映射到低维MOS分数上的回归模型。&lt;/p&gt;
&lt;h4 id=&#34;bag-of-words&#34;&gt;Bag of words
&lt;/h4&gt;&lt;p&gt;不同于以上基于NSS特征提取模型，传统无参考图像质量评估的另一个方向是词袋 (Bag of Words, BOW) 模型。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CORNIA&lt;/li&gt;
&lt;li&gt;HOSA&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;cornia&#34;&gt;CORNIA
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Unsupervised Feature Learning Framework for No-reference Image Quality Assessment（IEEE Conf. Comput. Vis. Pattern Recognit.，2012）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;completely-blind&#34;&gt;Completely blind
&lt;/h4&gt;&lt;p&gt;Completely Blind方法不需要在数据集上进行训练来学习特征到MOS分数的映射，而是能够通过待测图像或者视频直接输出得到质量分数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NIQE&lt;/li&gt;
&lt;li&gt;IL-NIQE&lt;/li&gt;
&lt;li&gt;SLEEQ[V]&lt;/li&gt;
&lt;li&gt;STEM[V]&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;niqe&#34;&gt;NIQE
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Making a &amp;ldquo;Completely Blind&amp;rdquo; Image Quality Analyzer（IEEE Signal Process，2013）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NIQE（natural image quality evaluator）是Mittal等人提出的基于自然场景统计的盲图像质量评价模型。该方法仅利用从自然图像中观察到的统计规律进行失真偏差的度量，通过构建一系列质量相关的统计特征以实现对图像的质量预测。&lt;/p&gt;
&lt;p&gt;NIQE算法有以下几个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. Spatial Domain NSS：提取NSS特征&lt;/li&gt;
&lt;li&gt;Step2. Patch Selection：图像划分、选择&lt;/li&gt;
&lt;li&gt;Step3. Characterizing Image Patches：提取Patches的特征&lt;/li&gt;
&lt;li&gt;Step4. Multivatiate Gaussian Model（MVG）：拟合多元高斯分布&lt;/li&gt;
&lt;li&gt;Step5. NIQE Index：计算NIQE分数&lt;/li&gt;
&lt;/ul&gt;
$$
\hat{I}(i,j)=\frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}
$$&lt;p&gt;
其中，$i \in 1,2,\dots,M,j\in 1,2,\dots,N$，$M,N$是图像的高和宽，$C=1$是为了数值稳定的常数；$\omega={ \omega_{k,l}\vert k=-K,\dots,K,l=-L,\dots,L}$是高斯核。&lt;/p&gt;
$$
\delta(b)={\sum\sum}_{(i,j)\in patch_b}\sigma(i,j) \quad b=1,2,\dots,P\times P
$$&lt;p&gt;
设定阈值$T$，若$\delta(b)\gt T$，则认为$patch_b$是锐利的。将所有图像块的最大锐利程度的 $p$倍设为阈值，其中$p \in [0.6, 0.9]$，论文中取值为0.75。将大于阈值的图像块保留，小于阈值的图像块淘汰掉。&lt;/p&gt;
&lt;p&gt;选取一些patches后，类似于BRISQUE中的方法，在不同尺度下拟合GGD和AGGD得到36维特征。&lt;/p&gt;
$$
f_X(x_1,\dots,x_k)=\frac{1}{(2\pi)^{k/2}\vert\Sigma\vert^{1/2}}\exp(-\frac{1}{2}(x-\nu)^T\Sigma^{-1}(x-\nu))
$$&lt;p&gt;
注：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这个模型是由一组清晰图像得到的，用来计算低质量图像与它的距离；&lt;/li&gt;
&lt;li&gt;采用高斯分布来处理这些特征的基本前提是假设这里所涉及的特征在真实的图像中所反映的也是服从高斯分布的。&lt;/li&gt;
&lt;/ul&gt;
$$
D(\nu_1,\nu_2,\Sigma_1,\Sigma_2)=\sqrt{(\nu_1-\nu_2)^T(\frac{\Sigma_1+\Sigma_2}{2})^{-1}(\nu_1-\nu_2)}
$$&lt;p&gt;
由上式即可得出最终得分，$\nu_1,\Sigma_1$是一组清晰图像得到的均值向量和协方差矩阵，$\nu_2,\Sigma_2$是输入的图像得到的均值向量和协方差矩阵。&lt;/p&gt;
&lt;h4 id=&#34;handcraft&#34;&gt;Handcraft
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;TLVQM[V]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;deep-learning&#34;&gt;Deep learning
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;VSFA[V]&lt;/li&gt;
&lt;li&gt;VMEON[V]&lt;/li&gt;
&lt;li&gt;PVQ[V]&lt;/li&gt;
&lt;li&gt;RAPIQUE[V]&lt;/li&gt;
&lt;li&gt;CNNIQA&lt;/li&gt;
&lt;li&gt;PaQ-2-PIQ&lt;/li&gt;
&lt;li&gt;Hallucinated-IQA&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;nr-iqa中的deep-based方法&#34;&gt;NR-IQA中的Deep-based方法
&lt;/h1&gt;&lt;h2 id=&#34;score-based&#34;&gt;Score-based
&lt;/h2&gt;$$
\text{Score-based}\left\{
\begin{aligned}
&amp;\text{Patch-wise} \\
&amp;\text{Image-wise}
\end{aligned}
\right.
$$&lt;p&gt;Score-based方法可以分为Patch-wise和Image-wise。Image-wise直接将图片输入到CNN中；Patch-wise对图像进行分块并分别输入CNN，pooling后得到图像质量。&lt;/p&gt;
&lt;h3 id=&#34;iqa-cnnpatchcnn直接预测质量&#34;&gt;IQA-CNN（patch/CNN/直接预测质量）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;L. Kang, P. Ye, Y. Li, and D. Doermann, “Convolutional neural networks for no-reference image quality assessment,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IQA-CNN是首个将空间卷积神经网络模型在图像质量评价领域应用的工作，IQA-CNN以patch作为输入，由一层卷积、最大最小池化以及两层全连接组成，将特征学习和回归集成到一个优化过程中。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/4.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;dliqanssdbn预测分类置信度&#34;&gt;DLIQA（NSS/DBN/预测分类+置信度）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;W. Hou, X. Gao, D. Tao, and X. Li, “Blind image quality assessment via deep learning,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 6, pp. 1275–1286, Jun. 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;心理学表明人类更加偏向于定性的评价而不是定量的评价，一幅图片打分为70还是75实际上是很难抉择的问题，用excellent、good、bad这样的语言定性更加自然。&lt;/p&gt;
&lt;p&gt;DLIQA将BIQA（Bind Image Quality Assessment）作为一个五分类问题，输入图像用NSS特征表示，分类的依据是分类器得到的置信度，然后将分类和对应的置信度转为质量分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/5.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;作者认为IQA问题并不适合用切成patch的方式来做，但直接输入整张图像维度太高，所以使用了可以表征自然图像和畸变图像的差异的NSS特征作为输入。&lt;/p&gt;
&lt;p&gt;为了将分类结果和置信度转化为质量分数，首先有以下假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每张图片都有一个内在质量$Q$；&lt;/li&gt;
&lt;li&gt;每个训练有素的人在评估具有相同内在质量的图像时，都会给出相同的标签。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主观评分的似然分布$P(L=\text{Excellent|Q}),P(L=\text{Good}|Q),P(L=\text{Fair}|Q),P(L=\text{Poor}|Q),P(L=\text{Bad}|Q)$和先验分布$P(Q)$如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/6.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;然后通过三角形分布函数和平均分布来模拟各个似然函数和先验分布：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/11.png&#34; style=&#34;zoom:67%;&#34; /&gt;
$$
P(Q|L)\sim P(L|Q)P(Q)
$$$$
P(Q|X)=\int P(Q|L)P(L|X)dL
$$$$
\text{Quality}=\mathbb{E}[P(Q|X)]
$$&lt;h3 id=&#34;deepiqapatchcnn预测质量权重&#34;&gt;Deepiqa（patch/CNN/预测质量+权重）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;S. Bosse, D. Maniry, T. Wiegand, and W. Samek, “A deep neural network for image quality assessment,” IEEE International Conference on Image Processing, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deepiqa将无任何预处理的图像patch作为输入，通过池化获得最终的质量分数，网络的输出定义为2维，其中一维输出图像patch的质量分数的对应权重，通过该权重得到最终的质量分数。&lt;/p&gt;
&lt;h3 id=&#34;bieconpatchcnnfr中间图&#34;&gt;BIECON（patch/CNN/FR中间图）
&lt;/h3&gt;&lt;p&gt;使用部分全参考图像质量评价方法中的局部质量图作为中间结果对模型进行训练。&lt;/p&gt;
&lt;h3 id=&#34;diqam-nrwadiqam-nrpatchcnn预测质量权重&#34;&gt;DIQaM-NR/WaDIQaM-NR（patch/CNN/预测质量+权重）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;S. Bosse, D. Maniry, K.-R. M ̈uller, T. Wiegand, and W. Samek, &amp;ldquo;Deep neural networks for no-reference and full-reference image quality assessment,&amp;rdquo; IEEE Transactions on image processing, vol. 27, no. 1, pp. 206–219, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8063957&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;以上分别为FR和NR模型的结构，FR模型去掉孪生网络的分支就是NR模型。&lt;/p&gt;
&lt;p&gt;FR模型的两个输入（distorted和reference）通过一个共享参数的VGG-Style网络，得到两个特征向量$f_r,f_d$，然后将$\text{concat}(f_r,f_d,f_r-f_d)$用于回归，一个网络用于回归质量，另一个用于回归权重，pooling后得到最终的图像质量。&lt;/p&gt;
&lt;h3 id=&#34;nimaimagecnn预测主观分数分布&#34;&gt;NIMA（image/CNN/预测主观分数分布）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;H. Talebi, P. Milanfar, &amp;ldquo;NIMA: neural image assessment,&amp;rdquo; IEEE Trans. Image Process., vol. 27, no. 8, pp. 3998-4011, Aug. 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不同于常见的深度图像质量评价方法，NIMA对人类主观分数的分布$\hat{\mathbf{p}}$进行预测。&lt;/p&gt;
$$
\mathbf{p}=[p_{s_1},\dots, p_{s_N}]\quad\sum_{i=1}^Np_{s_i}=1,s_1\leqslant s_i \leqslant s_N
$$&lt;p&gt;
其中$s_i$表示第$i$个分值，$N$表示分值总数。&lt;/p&gt;
$$
\mu=\sum_{i=1}^Ns_i\cdot p_{s_i}
$$$$
\sigma=\sqrt{\sum_{i=1}^N(s_i-\mu)^2\cdotp_{s_i}}
$$&lt;p&gt;
NIMA中CNN后接的全连接为10维（AVA和TID数据集中均有$N=10$）。&lt;/p&gt;
$$
EMD(\mathbf{p},\hat{\mathbf{p}})=\left(\frac{1}{N}\sum_{k=1}^N|CDF_\mathbf{p}(k)-CDF_{\hat{\mathbf{p}}}(k)|\right)^\frac{1}{r}
$$&lt;p&gt;
其中累计分布函数$CDF_\mathbf{p}(k)=\sum_{i=1}^k p_{s_i}$。&lt;/p&gt;
&lt;h3 id=&#34;hallucinated-iqaimagegan生成幻觉图像感知差异&#34;&gt;Hallucinated-IQA（image/GAN/生成“幻觉”图像感知差异）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Lin, G. Wang, “Hallucinated-IQA: no-reference image quality assessment via adversarial learning” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IQA在失真图像的基础上产生一个“幻觉”参考图像，通过捕捉失真图像和“幻觉”图像之间的感知差异来预测图像质量。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Hallucinated-IQA模型包括生成网络$G$、IQA判别网络$D$和回归网络$R$三个部分，结构如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
$$
\hat{\theta}=\arg\min_\theta \frac{1}{N}\sum_{i=1}^N(l_p(G_\theta(I_d^i), I_r^i)+l_s(G_\theta(I_d^i),I_r^i))
$$$$
l_s(G_\theta(I_d^i),I_r^i)=\Vert\phi(G_\theta(I_d^i))-\phi(I_r^i)\Vert^2
$$$$
l_s(G_\theta(I_d^i),I_r^i)=\lambda_1l_v(G_\theta(I_d^i),I_r^i)+\lambda_2l_q(G_\theta(I_d^i),I_r^i)
$$$$
\begin{aligned}
l_v&amp;=\sum_{c_v=1}^{C_v}\frac{1}{W_jH_j}\sum_{x=1}^{W_j}\sum_{y=1}^{H_j}\Vert\phi_j(G_\theta(I_d^i))_{x,y}-\phi_j(I_r^i)_{x,y}\Vert^2 \\
l_q&amp;=\sum_{c_q=1}^{C_q}\frac{1}{W_kH_k}\sum_{x=1}^{W_k}\sum_{y=1}^{H_k}\Vert\pi_k(G_\theta(I_d^i))_{x,y}-\pi_k(I_r^i)_{x,y}\Vert^2
\end{aligned}
$$&lt;p&gt;
其中$C$表示某一层的特征图数量，$W,H$表示特征图的维度，$\phi_j(\cdot)$表示VGG-19在第$j$层的特征图，$\pi_k(\cdot)$表示$R$在第$k$层的特征图。&lt;/p&gt;
$$
\max_\omega\mathbb{E}[\log D_\omega(\mathbf{I_r})]+\mathbb{E}[\log(1-\vert D_\omega(G_\theta(\mathbf{I}_d))-\mathbf{d}_{fake}\vert)]
$$&lt;p&gt;
其中，$\mathbf{d}&lt;em&gt;{fake}$定义为：
$$
\mathbf{d}_{fake}^i=\left\{
\begin{aligned}
&amp;1\quad \text{if}\ \Vert R(I_d^i,I_{sh}^i)-s^i\Vert_F\lt \epsilon\\
&amp;0\quad \text{if}\ \Vert R(I_d^i,I_{sh}^i)-s^i\Vert_F\geqslant \epsilon
\end{aligned}
\right.
$$
该式所表达的是，当回归网络预测出来的分数与真实质量分数的差距大于阈值时，认为“幻觉”图像降低了回归网络的性能，$\mathbf{d}&lt;/em&gt;{fake}$为0，此时为了最大化损失函数，需要IQA判别网络将生成图像判别为0，真实图像判别为1；当回归网络预测出来得分数与真实质量分数得差距小于阈值时，幻觉图像提升了回归网络的性能，$\mathbf{d}_{fake}$为1，此时为了最大化损失函数，需要辨别器将生成图像和真实图像均辨别为1。、&lt;/p&gt;
$$
\mathcal{L}_{adv}=\mathbb{E}[\log(1-D_\omega(G_\theta(I_d)))]
$$$$
\mathcal{L}_G=\mu_1\mathcal{L}_p+\mu_2\mathcal{L}_s+\mu_3\mathcal{L}_{adv}
$$$$
\hat{\gamma}=\arg\min_r\frac{1}{N}\sum_{i=1}^Nl_r(\mathcal{R}(I_d^i,I_{map}^i),s^i)
$$&lt;p&gt;
其中$I_{map}=\vert I_d-G_{\hat{\theta}}(I_d)\vert$。&lt;/p&gt;
&lt;p&gt;$R$的精确度在很大程度上取决于“幻觉”场景的合格性。具体地说，合格的幻觉图像作为代理参照可以帮助$R$探索失真图像的感知差异，而不合格的“幻觉”图像则会引入对$R$的偏差。&lt;/p&gt;
$$
\mathcal{F}=f(\mathcal{H}_{5,2}(I_d))\otimes(\mathcal{R}_1(I_d,I_{map}))
$$$$
\mathcal{L_R}=\frac{1}{T}\sum_{t=1}^T\Vert\mathcal{R}_2(f(\mathcal{H}_{5,2}(I_d))\otimes (\mathcal{R}_1(I_d,I_{map})))-s^t\Vert_{\ell_1}
$$&lt;h3 id=&#34;ran4iqapatchgan&#34;&gt;RAN4IQA（patch/GAN）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;H. Ren, D. Chen, Y. Wang, “RAN4IQA: Restorative Adversarial Nets for No-reference Image Quality Assessment,” in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过生成式对抗网络对输入的失真图像进行修复，基于修复收益（gain of restoration, GoR）提取失真图像和修复图像的特征并进行比较以感知质量。&lt;/p&gt;
&lt;h3 id=&#34;sfapatch语义特征聚集&#34;&gt;SFA（patch/语义特征聚集）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;D. Li, T. Jiang, W. Lin, and M. Jiang, “Which has better visual quality: the clear blue sky or a blurry animal?,” IEEE Trans. Multimedia., vol. 21, no. 5, pp. 1221-1234, Nov. 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rank-based&#34;&gt;Rank-based
&lt;/h2&gt;&lt;h3 id=&#34;gao-et-al&#34;&gt;Gao et al
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;F. Gao, D. Tao, X. Gao, and X. Li, “Learning to rank for blind image quality assessment,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 10, pp. 2275–2290, Oct. 2015.&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1309.0213v2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1309.0213v2] Learning to Rank for Blind Image Quality Assessment (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Blind image quality assessment（BIQA）旨在预测图像质量分数，但图像质量分数的获取有一定的局限性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分数不精确；&lt;/li&gt;
&lt;li&gt;主观判断不准确；&lt;/li&gt;
&lt;li&gt;不同失真类别之间的质量尺度不一致；&lt;/li&gt;
&lt;li&gt;大规模数据难以获取。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者提出基于偏好图像对（preference image pair, PIPs），对于一个偏好图像对而言，其标签表示的是图像一的质量好于图像二的质量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/9.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;dipiq&#34;&gt;DipIQ
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Ma, W. Liu, T. Liu, Z. Wang and D. Tao, &amp;ldquo;dipIQ: blind image quality assessment by learning-to-rank discriminable image pairs,&amp;rdquo; IEEE Trans. Image Process., vol. 26, no. 8, pp. 3951-3963, Aug. 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rankiqa&#34;&gt;RankIQA
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;X. Liu, J. V. D. Weijer, and A. D. Bagdanov, “RankIQA: learning from rankings for no-reference image quality assessment,” in Proc. IEEE Int. Conf. Comput. Vis. 2017.&lt;/li&gt;
&lt;li&gt;X. Liu, J. V. D. Weijer, and A. D. Bagdanov, “Exploiting unlabeled data in cnns by self-supervised learning to rank,” IEEE Trans. Pattern. Anal. Mach. Intell., vol. 41, no. 8, pp. 1862-1878, Aug. 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lfma&#34;&gt;LFMA
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Ma, X. Liu, Y. Fang, and E. P. Simoncelli, “Blind image quality assessment by learning from multiple annotators,” IEEE International Conference on Image Processing, 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multi-task&#34;&gt;Multi-task
&lt;/h2&gt;&lt;h3 id=&#34;biqi&#34;&gt;BIQI
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;A. K. Moorthy, A. C. Bovik, “A two-step framework for constructing blind image quality indices,” IEEE Signal Process. Lett., vol. 17, no. 5, pp. 513-516, May. 2010.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iqa-cnn&#34;&gt;IQA-CNN++
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;L. Kang, P. Ye, Y. Li, and D. Doermann, “Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks,” in Proc. IEEE Int. Conf. Image Process., 2015, pp. 2791–2795.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mrliq&#34;&gt;MRLIQ
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;L. Xu, J. Li, W. Lin, Y. Zhang and L. Ma, Y. Fang, and Y. Yan, “Multi-task rank learning for image quality assessment,” IEEE Trans. Circuits Syst. Video Technol., vol. 27, no. 9, pp. 1833-1843, Sep. 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;meon&#34;&gt;MEON
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Ma, W. Liu, T. Liu, K. Zhang, Z. Duanmu, Z. Wang, and W. Zuo, &amp;ldquo;End-to-end blind image quality assessment using deep neural networks,&amp;rdquo; IEEE Trans. Image Process., vol. 27, no. 3, pp. 1202-1213, Mar. 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MEON是用于BIQA（Bind Image Quality Assessment）多任务端到端优化的深度神经网络，包括失真判别网络和质量预测网络两部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;MEON的训练过程分为两个阶段：原始图像经过共享参数层，两个子网络一个用于失真类型判别，另一个用于质量预测，并且其中质量预测阶段使用了失真类型判别子网络的输出。&lt;/p&gt;
$$
y_i(m,n)=\frac{x_i(m,n)}{\left(\beta_i+\sum_{j=1}^S\gamma_{ij}x_j(m,n)^2\right)^\frac{1}{2}}
$$&lt;p&gt;
$y_i$是根据$x_i$在空间位置$(m,n)$的激活响应，$\beta$和$\gamma$在训练过程中进行优化，GDN操作是一个可微的变换。&lt;/p&gt;
&lt;h2 id=&#34;more&#34;&gt;More
&lt;/h2&gt;&lt;h3 id=&#34;ser-fiq&#34;&gt;SER-FIQ
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.09373&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.09373] SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SER-FIQ用于人脸图像质量评估，作者认为人脸图像的质量来自于图像的embedding的鲁棒性，同一张图片经过人脸识别模型的不同的子网络得到的不同的embedding的方差反映了人脸图像的质量。&lt;/p&gt;
$$
X(I)=\{x_s\},s\in 1,2,\dots,m
$$$$
q(X(I))=2\sigma\left(-\frac{2}{m^2}\sum_{i\lt j} d(x_i,x_j)\right)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/10.png&#34; style=&#34;zoom:75%;&#34; /&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>YOLO v1 - v7</title>
        <link>https://demo.stack.jimmycai.com/p/yolo-v1-v7/</link>
        <pubDate>Thu, 14 Jul 2022 09:41:05 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/yolo-v1-v7/</guid>
        <description>&lt;p&gt;本文从Head、Backbone、Neck等方面介绍了YOLO的核心演变过程。&lt;/p&gt;
&lt;h1 id=&#34;yolov1---yolov7&#34;&gt;YOLOv1 - YOLOv7
&lt;/h1&gt;&lt;h2 id=&#34;yolov0&#34;&gt;YOLOv0
&lt;/h2&gt;&lt;p&gt;将目标检测任务当作是遍历性的分类任务，就可以使用分类器来完成检测。&lt;/p&gt;
&lt;p&gt;例如可以用一个框分别落在图片的各个区域，对框住的区域进行二分类，然后因为目标大小的不同，可能还需要使用不同大小的框来遍历图片，另外，为了提高检测精度，还可以使用滑动窗口的方法来遍历图像的所有位置&amp;hellip;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/2.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;R-CNN所采用的就是滑动窗口策略。&lt;/p&gt;
&lt;p&gt;而YOLO的思路是，将分类器的输出从一个One-hot向量换成$(c,x,y,w,h)$，直接输出这个框和置信度，将问题转化为一个回回归问题。要组织训练也比较简单，只需找很多的图片，标注框的位置，并将置信度$c$设为1，送入网络即可。&lt;/p&gt;
&lt;p&gt;以上便是YOLO最简单的版本。&lt;/p&gt;
&lt;h2 id=&#34;yolov1&#34;&gt;YOLOv1
&lt;/h2&gt;&lt;h3 id=&#34;head&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;在YOLOv0中，一张图片输出一个五元组$(c,x,y,w,h)$，只能输出一个目标。要输出多个目标，YOLOv1中采用的方式是将图片分为多个的网格（$S\times S$），每个网格单元对应一个五元组，就可以输出$S\times S$个框，如果目标的中心落在一个网格单元中，那么这个网格单元就会负责检测这个对象，如果网格单元中不存在目标，则置信度为0。&lt;/p&gt;
&lt;p&gt;例如当$S=4$时，输入一张图片：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;所返回的tensor为$(5,4,4)$，其中标签的置信度如下所示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;现在暂不考虑一个网格单元中存在多个目标的问题（上图3行3列），当得到$S\times S$个框后，YOLOv1中使用NMS（非极大值抑制）将目标所在的框保留下来而去除其它框。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 保留得分最高的框；&lt;/li&gt;
&lt;li&gt;Step2. 根据交并比，去掉与置信度最高的框重合度较高的框；&lt;/li&gt;
&lt;li&gt;Step3. 在剩下的框中继续选择得分最高的框，以此类推，直到没有剩下的框。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当要检测多个目标（$C$）时（例如既要检测葫芦娃的头，又要检测葫芦娃的葫芦），在返回的结果中多加一个二维的One-hot向量，此时输入一张图片后，返回的结果为：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;考虑到小目标检测效果不佳的问题，针对不同尺寸的目标，YOLOv1中为每个网格单元预测多个五元组（$B$），分别负责不同尺寸的目标的回归。&lt;/p&gt;
&lt;p&gt;以上，YOLO v1的预测结果为一个$S\times S\times(B*5+C)$的tensor，在YOLOv1中，$S=7$，$B=2$，$C=20$（PASCAL VOC数据集有20类），即每张图片的最终输出为$7\times 7\times 30$的tensor。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv1的整体结构如下（在ImageNet上训练时使用$224\times 224$的分辨率，检测时使用$448\times 448$。）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/7.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;YOLOv1对平方和误差（sum-squared error）进行优化，但它对定位误差和分类误差的权重相等，YOLOv1引入$\lambda_{coord}$和$\lambda_{noobj}$增加边界框坐标预测的损失，并减少不包含对象的框的置信度预测的损失，设$\lambda_{coord}=5,\lambda_{noobj}=.5$。此外，为了反映大检测框中的小偏差要比小检测框中的小偏差影响要小，YOLOv1对边界框宽度和高度的平方根进行预测，而不是直接预测宽度和高度。&lt;/p&gt;
&lt;h3 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
&amp;\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2] \\
+&amp;\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(\sqrt{w_i}-\sqrt{\hat{w}_i})^2+(\sqrt{h_i}-\sqrt{\hat{h}_i})^2] \\
+&amp;\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2 \\
+&amp;\lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2 \\
+&amp;\sum_{i=0}^{S^2}\mathbb{1}_i^{obj}\sum_{c\in classes}(p_i(c)-\hat{p}_i(c))^2
\end{aligned}
$$$$
\mathbb{1}_{ij}^{obj}=\left\{
\begin{aligned}
&amp;1,\quad第i个网格第j个\text{anchor box}负责预测这个物体 \\
&amp;0,\quad其他
\end{aligned}
\right.
$$&lt;h3 id=&#34;backbone&#34;&gt;Backbone
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/7.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;还是这张图，YOLOv1的backbone是参考GoogLeNet设计的，除最后两层全连接外，其它都大量使用了卷积层。下图是YOLOv4的论文中总结的检测类网络的结构：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/1.png&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;在YOLOv1中只有backbone，没有neck，属于Dense Prediction，一阶段检测器属于Dense Prediction，二阶段检测器既有Dense Prediction，又有Sparse Prediction。&lt;/p&gt;
&lt;h3 id=&#34;训练过程&#34;&gt;训练过程
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Step1. 在ImageNet上使用$224\times224$分辨率的图片训练分类器网络；&lt;/li&gt;
&lt;li&gt;Step2. 用$448\times448$分辨率端到端地训练目标检测网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yolov2--yolo9000&#34;&gt;YOLOv2 &amp;amp; YOLO9000
&lt;/h2&gt;&lt;h3 id=&#34;head-1&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;YOLOv1虽然检测速度快，但是定位不够准确，召回率较低，YOLOv2在YOLOv1的基础上提出了一些改进策略：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/1.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;h4 id=&#34;基于偏移量的边框回归&#34;&gt;基于偏移量的边框回归
&lt;/h4&gt;&lt;p&gt;同时代的R-CNN所预测的是偏移量，而YOLOv1直接预测框的坐标和大小，范围较大，YOLOv2中使用到了两种边框回归方式：基于anchor的偏移量和基于grid的偏移量。（anchor是R-CNN系列的一个概念，可以简单理解为一个预定义好的框，位置、宽高均已知，供预测时参考。）&lt;/p&gt;
&lt;p&gt;YOLOv2中的预测方式如下图所示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;其中，$b_x,b_y,b_w,b_h$是最终得到的检测结果，$t_x,t_y,t_w,t_h$是模型要预测的值，$c_x,c_y$是grid左上角的坐标，$p_w,p_h$是anchor的宽和高。YOLOv2基于anchor框的宽高和grid的先验位置预测的偏移量。&lt;/p&gt;
&lt;p&gt;例如对于下图，图片被分为9个grid，红色框为ground truth，紫色框为anchor：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\begin{aligned}
c_x&amp;=\frac{149}{149}=1\quad c_y=\frac{149}{149}=1 \\
b_x&amp;=\frac{230}{149}=1.543\quad b_y=\frac{218}{149}=1.463
\end{aligned}
$$$$
\begin{aligned}
b_x&amp;=\sigma(t_x)+c_x \\
b_y&amp;=\sigma(t_y)+c_y \\
b_w&amp;=p_we^{t_w} \\
b_h&amp;=p_he^{t_h}
\end{aligned}
$$$$
\begin{aligned}
t_x&amp;=\log\frac{1.543-1}{1-(1.543-1)}=0.172\\
t_y&amp;=\log\frac{1.463-1}{1-(1.463-1)}=-0.148 \\
t_w&amp;=\log\frac{224}{315}=-0.340 \\
t_h&amp;=\log\frac{202}{280}=-0.326
\end{aligned}
$$$$
\sigma(x)=\frac{1}{1+e^{-x}}\quad(\text{Sigmoid})
$$$$
125=5\times 5(c,x,y,w,h)+5\times 20\text{classes}
$$&lt;p&gt;
最后，关于每个网格中anchor个数的选择，YOLOv2的作者对VOC和COCO数据集的GT bounding box进行聚类，发现$k=5$时能够较好的平衡召回率和模型的复杂度。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;backbone-1&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;为了进一步提升性能，YOLOv2重新训练了一个DarkNet-19（下图双横线上方）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;VGGNet中得到一个结论，$7\times7$卷积可以用更小的卷积代替，且$3\times 3$卷积更加节约参数，使模型更小，且网络可以做得更深，因为能够引入更多的非线性的激活函数，能够更好地提取到特征。&lt;/p&gt;
&lt;p&gt;此外，还使用了bottleneck结构：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/3.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;使用$1\times1$的网络结构可以很方便的改变维度，灵活设计网络且减少计算量。&lt;/p&gt;
&lt;p&gt;同时在backbone中也没有FC层了，而是使用了GAP（Global Average Pooling）层，将$1000\times7\times7$映射为$1000\times1$，满足了不同尺度的输入图片的需求。并且针对较小的目标，只需把图片放大，就可以放大目标，提高检测精度。&lt;/p&gt;
&lt;p&gt;最后，由于backbone网络DarkNet-19是单独在ImageNet上训练的，所以最后加了softmax。&lt;/p&gt;
&lt;p&gt;完整的YOLOv2网络结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;reorg即passthrough layer，将$26\times26\times64$转为$13\times13\times256$，特征图大小减少4倍，通道数增加4倍，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/5.png&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;h3 id=&#34;训练过程-1&#34;&gt;训练过程
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Step1. 在ImageNet上训练DarkNet-19，输入为$224\times224$，训练160个epoch；&lt;/li&gt;
&lt;li&gt;Step2. 将网络输入调整为$448\times 448$（测试时使用$416\times416$），继续在ImageNet上finetune，训练10个epoch；（将$448\times448$改为$416\times416$，将创建奇数空间维度，对于一些大目标，其中心点往往落入图片的中心位置，使用特征图中心的1个网格单元去预测bounding box要更容易。）&lt;/li&gt;
&lt;li&gt;Step3. 修改DarkNet-19模型：移除最后的卷积层、GAP层和softmax层，新增3个$3\times3\times1024$的卷积层和1个passthrough层，最后用$1\times1$的卷积层输出预测结果，并在检测数据集上继续finetune网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yolov3&#34;&gt;YOLOv3
&lt;/h2&gt;&lt;h3 id=&#34;head-2&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;YOLOv2对于小目标的检测效果仍然不佳，在YOLOv3中，检测头分成了三部分：$13\times 13\times[3*(4+1+80)]$、$26\times 26\times[3*(4+1+80)]$和$52\times 52\times[3*(4+1+80)]$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv3/2.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;检测头的三个分支分别为32倍、16倍和8倍下采样，感受野由大变小，分别去预测大、中、小目标，对于每一个grid，共设置了9个先验框，大、中、小各3个，每个框预测一个五元组和80维的One-hot分类向量。三个特征图一共可以解码出$(52\times52+26\times26+13\times13)\times3=10647$个bounding box。&lt;/p&gt;
&lt;p&gt;YOLOv3仍然采用k-means聚类来确定先验框的个数，对于COCO数据集，9个聚类簇为$(10\times13),(16\times30),(33\times23),(30\times61),(62\times45),(59\times119),(116\times90),(156\times198),(373\times326)$。&lt;/p&gt;
&lt;p&gt;YOLOv3的网络结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv3/1.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;YOLOv3使用多标签分类，使用多个独立的logistic分类器代替softmax，因为部分数据集中存在许多重叠的标签（Woman and Person），多标签方法可以建立更优的模型。&lt;/p&gt;
&lt;h3 id=&#34;训练策略&#34;&gt;训练策略
&lt;/h3&gt;&lt;p&gt;YOLOv3的训练策略非常重要，论文中的表述如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following. We use the threshold of .5. Unlike our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;预测框分为正例、负例和忽略样例三种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正例：任取一个ground truth，与10647个框计算IoU，IoU最大的预测框即为正例，下一个ground truth在余下的10646个检测框中寻找IoU最大的检测框作为正例。
&lt;ul&gt;
&lt;li&gt;正例产生置信度loss、检测框loss和类别loss；&lt;/li&gt;
&lt;li&gt;置信度标签为1，对应的类别标签为1（其余为0），预测框为$(t_x,t_y,t_w,t_h)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;负例：正例除外，与全部的ground truth的IoU都小于阈值（论文中为0.5）则为负例。（注：与ground truth计算IoU最大的检测框，即使IoU小于阈值，仍然为正例。）
&lt;ul&gt;
&lt;li&gt;负例仅产生置信度loss；&lt;/li&gt;
&lt;li&gt;置信度标签为0。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;忽略样例：正例除外，与任意一个ground truth的IoU大于阈值则为忽略样例。
&lt;ul&gt;
&lt;li&gt;忽略样例不产生任何loss。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;损失函数-1&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
loss_{N_1}&amp;=\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(t_{xi}-\hat{t}_{xi})^2+(t_{yi}-\hat{t}_{yi})^2] \\
&amp;+\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(t_{wi}-\hat{t}_{wi})^2+(t_{hi}-\hat{t}_{hi})^2] \\
&amp;+\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2 \\
&amp;+\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2 \\
&amp;-\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}\sum_{c\in classes}[\hat{p}_i(c)\log(p_i(c))+(1-\hat{p}_i(c))\log(1-p_i(c))]
\end{aligned}
$$$$
Loss=loss_{N_1}+loss_{N_2}+loss_{N_3}
$$&lt;h3 id=&#34;backbone-2&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;YOLOv3使用的backbone为Darknet-53：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Darknet-53中依然采用了bottleneck结构，并添加了残差结构，但是没有池化层了，而是使用步长为2的卷积来替代池化层完成下采样的工作。&lt;/p&gt;
&lt;h3 id=&#34;neck&#34;&gt;Neck
&lt;/h3&gt;&lt;p&gt;相比YOLOv1和YOLOv2，YOLOv3增加了neck这一部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;YOLOv3的neck部分使用的是FPN（Feature Pyramid Networks），生成不同尺寸的图片，最后统计所有图片的预测结果。&lt;/p&gt;
&lt;p&gt;YOLOv3的backbone有$(13,13,1024),(26,26,512),(52,52,256)$三个输出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于$(13,13,1024)$的输出，经过5次DBL，输出$(13,13,512)$，然后一部分传到head中，另一部分经过DBL和上采样，得到$(26,26,256)$；&lt;/li&gt;
&lt;li&gt;对于$(26,26,512)$的输出，直接与上一步上采样后的结果concat，得到$(26,26,768)$，同时concat的结果经过5次DBL，得到$(26,26,256)$，一部分传入head中，另一部分经过DBL和上采样，得到$(52,52,128)$；&lt;/li&gt;
&lt;li&gt;对于$(52,52,256)$的输出，直接与上一步上采样后的结构concat，得到$(52,52,384)$，经过5次DBL后输入到head中。&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/9.jpg&#34; style=&#34;zoom:61%;&#34; /&gt;
&lt;h2 id=&#34;yolov4&#34;&gt;YOLOv4
&lt;/h2&gt;&lt;p&gt;YOLOv4在原有YOLO架构的基础上，采用了近年CNN领域中最优秀的优化策略，YOLOv4的文章如同一篇目标检测trick的综述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;head-3&#34;&gt;Head
&lt;/h3&gt;&lt;h4 id=&#34;iou-threshold&#34;&gt;IoU threshold
&lt;/h4&gt;&lt;p&gt;在YOLOv3中，1个anchor负责一个ground truth，而YOLOv4使用多个anchor负责一个ground truth，对于$GT_j$而言，只要$IoU(anchor_i,GT_j)\gt threshold$，就让$anchor_i$负责$GT_j$。&lt;/p&gt;
&lt;p&gt;该方法使得在anchor框数量不变的情况下，正样本比例增加，缓解了正负样本不均衡的问题。&lt;/p&gt;
&lt;h4 id=&#34;eliminate-grid-sensitivity&#34;&gt;Eliminate grid sensitivity
&lt;/h4&gt;$$
\begin{aligned}
b_x&amp;=1.1\cdot\sigma(t_x)+c_x \\
b_y&amp;=1.1\cdot\sigma(t_y)+c_y
\end{aligned}
$$$$
\begin{aligned}
b_x&amp;=scale_{xy}\cdot\sigma(t_x)-\frac{scale_{xy}-1}{2}+c_x \\
b_y&amp;=scale_{xy}\cdot\sigma(t_y)-\frac{scale_{xy}-1}{2}+c_y \\
\end{aligned}
$$&lt;p&gt;此处的1.1也可以替换成一个其它的略大于1的数。&lt;/p&gt;
&lt;h4 id=&#34;ciou-loss&#34;&gt;CIoU-loss
&lt;/h4&gt;$$
L_{IoU}=1-\frac{B\cap B_{gt}}{B\cup B_{gt}}
$$&lt;p&gt;
但这样带来的问题是，当ground truth和bounding box没有重合时，没有梯度回传，并且对于以下这种情况，三者IoU相等，但重合度不一样，左边较好，右边较差：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/1.png&#34; style=&#34;zoom:67%;&#34; /&gt;
$$
L_{GIoU}=1-IoU+\frac{|C-B\cup B_{gt}|}{|C|}
$$&lt;p&gt;
其中，$C$为同时包含预测框和真实框的最小框的面积。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/2.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;GIoU Loss解决了IoU Loss对距离不敏感的问题，但GIoU Loss存在训练过程中发散等问题，针对此，DIoU被提出，DIoU的作者提出了两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是否可以直接最小化anchor与目标框的归一化距离，使其更快速收敛？&lt;/li&gt;
&lt;li&gt;如何使回归在与目标框有重叠甚至包含时更准确、更快？&lt;/li&gt;
&lt;/ul&gt;
$$
L_{DIoU}=1-IoU+\frac{\rho^2(b,b^{gt})}{c^2}
$$&lt;p&gt;
其中$b$、$b^{gt}$分别代表预测框和真实框的中心点，$\rho$表示计算两个中心点间的欧氏距离，$c$代表能够同时包含预测框和真实框的最小闭包区域的对角线距离。&lt;/p&gt;
&lt;p&gt;DIoU Loss除了收敛速度比GIoU Loss要快不少外，对于下面这种情况：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/3.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;三者IoU Loss和GIoU Loss都一样，但DIoU Loss从左到右依次减小。&lt;/p&gt;
&lt;p&gt;但DIoU Loss仅缓解了bounding box全包含ground truth的问题，但没有彻底解决包含的问题，例如对于下面这种情况：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\mathcal{R}_{CIoU}=\frac{\rho^2(b,b^{gt})}{c^2}+\alpha v
$$$$
v=\frac{4}{\pi^2}\left(\arctan\frac{w^{gt}}{h^{gt}}-\arctan\frac{w}{h}\right)^2
$$$$
\mathcal{L}_{CIoU}=1-IoU+\frac{\rho^2(b,b^{gt})}{c^2}+\alpha v
$$&lt;p&gt;
CIoU Loss需要考虑$v$的梯度，长宽在$[0,1]$的情况下，$w^2+h^2$的值通常很小，会导致梯度爆炸，因此在实现时将$1/(w^2+h^2)$替换为1。&lt;/p&gt;
&lt;h3 id=&#34;损失函数-2&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
&amp;\lambda_{iou}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}L_{CIoU}\\
+&amp;\lambda_{cls}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}\lambda_c(C_i-\hat{C}_i)^2+\lambda_{cls}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{noobj}\lambda_c(C_i-\hat{C}_i)^2 \\
-&amp;\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}\sum_{c\in classes}\lambda_c[\hat{p}_i(c)\log(p_i(c))+(1-\hat{p}_i(c))\log(1-p_i(c))]
\end{aligned}
$$&lt;p&gt;
从上到下三行式子分别为定位损失、目标置信度损失和分类损失。&lt;/p&gt;
&lt;h3 id=&#34;输入端&#34;&gt;输入端
&lt;/h3&gt;&lt;p&gt;YOLOv4对训练的输入端进行改进，比如数据增强Mosaic、CmBN、SAT自对抗训练。&lt;/p&gt;
&lt;h4 id=&#34;mosaic数据增强&#34;&gt;Mosaic数据增强
&lt;/h4&gt;&lt;p&gt;YOLOv4中使用的Mosaic参考了2019年提出的CutMix数据增强方式，CutMix使用两张图片进行拼接，而Mosaic采用四张图片，使用随机缩放、随机裁剪、随机排布的方式进行拼接。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/17.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;小目标的定义是目标框长宽在$0\times0 \sim 32\times 32$的物体，数据集中小目标框占所有框达到41.4%，但仅有52.3%的图片有小目标，中目标和大目标的分布相对来说更加均匀一些。&lt;/p&gt;
&lt;p&gt;使用四张图片拼接的方法大大丰富了检测数据集，并且经过随机缩放后增加了很多小目标，增强网络鲁棒性，并且可以减少GPU，直接计算4张图片的数据，Mini-batch大小不需要很大。&lt;/p&gt;
&lt;h3 id=&#34;backbone-3&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;YOLOv4使用CSPDarknet-53作为backbone。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;csp结构&#34;&gt;CSP结构
&lt;/h4&gt;&lt;p&gt;CSPDarknet-53是在Darknet-53的基础上，借鉴2019年CSPNet的经验产生的结构，其中包含5个CSP模块，经过5次CSP模块得到的特征图大小变化为：$608\rightarrow304\rightarrow152\rightarrow76\rightarrow38\rightarrow19$。&lt;/p&gt;
&lt;p&gt;CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的，先将基础层的特征映射划分为两部分（$w&lt;em&gt;h&lt;/em&gt;i\rightarrow w&lt;em&gt;h&lt;/em&gt;i/2+w&lt;em&gt;h&lt;/em&gt;i/2$），然后通过跨阶段层次结构将其合并，减少计算量的同时保证准确率。&lt;/p&gt;
&lt;h4 id=&#34;mish激活函数&#34;&gt;Mish激活函数
&lt;/h4&gt;&lt;p&gt;YOLOv4的backbone中都使用了Mish激活函数（后面的网络仍为Leaky ReLU）。&lt;/p&gt;
&lt;p&gt;在介绍Mish前，需要先了解softplus和tanh。&lt;/p&gt;
$$
\zeta(x)=\log(1+e^x)
$$&lt;p&gt;
softplus与ReLU的曲线相似，但比ReLU更为平滑：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/Figure_1.png&#34; style=&#34;zoom:72%;&#34; /&gt;
$$
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$$$
Mish(x)=x\times\tanh(\zeta(x))
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/Figure_2.png&#34; style=&#34;zoom:72%;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;dropblock&#34;&gt;Dropblock
&lt;/h4&gt;&lt;p&gt;Dropblock于2018年提出，和Dropout功能类似，是一种缓解过拟合的一种正则化方式。&lt;/p&gt;
&lt;p&gt;Dropout会在神经网络的学习过程中，将部分隐含层节点的权重归零，但卷积层后的dropout层对网络的泛化能力影响不大，即使随机丢弃，卷积层仍然可以从相邻的激活单元学到相同的信息。&lt;/p&gt;
&lt;p&gt;而Dropblock在整个局部区域进行删减丢弃。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/12.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;neck-1&#34;&gt;Neck
&lt;/h3&gt;&lt;p&gt;YOLOv4的neck采用了SPP模块和FPN + PAN。&lt;/p&gt;
&lt;h4 id=&#34;spp模块&#34;&gt;SPP模块
&lt;/h4&gt;&lt;p&gt;SPP模块中采用$k={1\times1,5\times5,9\times9,13\times13}$的最大池化方式，再将不同尺度的特征图concat起来。（池化时采用padding操作，移动的步长为1，故池化后的特征图大小均为$13\times13$）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/13.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h4 id=&#34;fpn--pan&#34;&gt;FPN + PAN
&lt;/h4&gt;&lt;p&gt;PAN是借鉴2018年图像分割领域PANet的创新点，在YOLOv3中引入了FPN，将高层的特征信息通过上采样的方式进行传递融合，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/14.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv4在FPN层后面还添加了一个自底向上的特征金字塔，其中包含两个PAN结构，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/15.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;FPN层自顶向下传达强语义特征，特征金字塔自底向上传达强定位特征，将二者特征进行聚合。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/16.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;yolov5&#34;&gt;YOLOv5
&lt;/h2&gt;&lt;p&gt;YOLOv5结构和YOLOv5结构比较类似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/19.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;输入端-1&#34;&gt;输入端
&lt;/h3&gt;&lt;p&gt;YOLOv5的输入端采用了和YOLOv4相同的Mosaic数据增强。&lt;/p&gt;
&lt;h4 id=&#34;自适应锚框计算&#34;&gt;自适应锚框计算
&lt;/h4&gt;&lt;p&gt;在YOLOv3、YOLOv4中，初始锚框的值是通过单独的程序运行的，YOLOv5在每次训练时都自适应地计算不同训练集中的最佳锚框值。如果觉得计算锚框的效果不好，也可以在代码中将自动计算锚框的功能关闭。&lt;/p&gt;
&lt;h4 id=&#34;letterbox自适应图片缩放&#34;&gt;letterbox自适应图片缩放
&lt;/h4&gt;&lt;p&gt;常用目标检测算法中，对于不同长宽的图片，常用方式是将图片统一缩放到一个标准尺寸再送入检测网络，例如$416\times416$和$608\times608$，但如果简单的使用resize，可能会导致图片信息的丢失。&lt;/p&gt;
&lt;p&gt;letterbox的主要思想是尽可能的利用网络感受野的信息特征，YOLOv5的网络经过5次下采样，$2^5=32$，最后一层特征图上的每个点可以对应原图中$32\times$32的区域信息。&lt;/p&gt;
&lt;p&gt;以$800\times600$的图片为例，原始缩放尺寸为$416\times416$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;416\div800=\bf{0.52} \\
  &amp;416\div600=0.69
  \end{aligned}
  \right.
  $$&lt;/li&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;800\times0.52=416 \\
  &amp;600\times0.52=312
  \end{aligned}
  \right.
  $$&lt;/li&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;416-312=104 \\
  &amp;104\mod32=8 \\
  &amp;8\div2=4
  \end{aligned}
  \right.
  \quad\Rightarrow 416\times320\quad(312+8)
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backbone-4&#34;&gt;Backbone
&lt;/h3&gt;&lt;h4 id=&#34;focus结构&#34;&gt;Focus结构
&lt;/h4&gt;&lt;p&gt;Focus结构中比较关键的是切片操作，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/20.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;以YOLOv5s为例，原始$608\times608\times3$的图像输入到Focus结构中后，得到$304\times304\times12$的特征图，再经过卷积得到$304\times304\times32$的特征图。&lt;/p&gt;
&lt;h4 id=&#34;csp结构-1&#34;&gt;CSP结构
&lt;/h4&gt;&lt;p&gt;与YOLOv4不同点在于，YOLOv4只有主干网络使用了CSP结构，而YOLOv5中设计了两种CSP结构，分别应用在backbone和neck中。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/21.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;四种网络的深度和宽度&#34;&gt;四种网络的深度和宽度
&lt;/h3&gt;&lt;p&gt;YOLOv5s、YOLOv5m、YOLOv5l和YOLOv5x。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/18.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/22.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;yolov7&#34;&gt;YOLOv7
&lt;/h2&gt;&lt;p&gt;Alexey Bochkovskiy大佬的重磅Paper，YOLOv7相同体量下比YOLOv5精度更高，速度快120%（FPS），比 YOLOX 快180%（FPS），比 Dual-Swin-T 快1200%（FPS），比 ConvNext 快550%（FPS），比 SWIN-L快500%（FPS）&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture
&lt;/h3&gt;&lt;h4 id=&#34;extended-efficient-layer-aggregation-networks&#34;&gt;Extended efficient layer aggregation networks
&lt;/h4&gt;&lt;p&gt;除参数量、计算量和计算密度外，还可以从访存代价的角度来设计高效体系结构，ShuffleNet V2中分析了输入/输出通道比、结构的分支数量和Element-wise操作（ReLU、Add）对网络推理速度的影响。&lt;/p&gt;
&lt;p&gt;ShuffleNet V2中给出结论：在计算量和参数量固定的前提下，&lt;strong&gt;输入和输出的通道数相等&lt;/strong&gt;时MAC（Memory Access Cost）取下界，此时的设计是最高效的。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ShuffleNet V2的文章中提出了多个设计准则来帮助模型提速。&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1807.11164&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;vovnet&#34;&gt;VoVNet
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.09730&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;网络结构&#34;&gt;网络结构
&lt;/h6&gt;&lt;p&gt;VoVNet的网络结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;VoVNet-27的前两个stage如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于VoVNet-27：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第1个stage：包含3个$3\times3$卷积，通道数分别为64、64、128；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第2个stage：经过一个OSA模块后，输出特征变为原来的$1/4$，其中OSA模块包括5个通道数为64的$3\times3$卷积，然后将这5个卷积的输出concat在一起，通道数变为$64\times5=320$，再经过1个$1\times1$卷积。输出通道数为128；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后面3个stage均为OSA模块，只是通道数量不同。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;注：每个stage结束后都会有一次$3\times3$、stride=2的max-pooling层，用来减小特征的维度，且每个卷积层都有序列Conv-BN-ReLU。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;osaone-shot-aggregation模块&#34;&gt;OSA（One-Shot Aggregation）模块
&lt;/h6&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/12.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
X_l=H_l([X_0,X_1,...,X_{l-1}])
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/13.png&#34; style=&#34;zoom:42%;&#34; /&gt;
&lt;p&gt;DenseNet的输入与输出通道数不一致，且通道数较高，DenseNet采用了$1\times1$卷积压缩特征，下图第一行是DenseNet各个卷积层之间的相互关系的大小，$(s,l)$表示第$s$和第$l$层之间权重的归一化L1范数，可以看出浅层特征图对深层特征图的贡献很少。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/17.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;DenseNet带来了大量的特征冗余，OSA模块只在最后一层聚集前面所有的层，经过这一改动，每层输入和输出的通道数是固定的，就可以让输入和输出的通道数相等而取到最小的MAC，且不需要$1\times1$卷积来压缩特征（$1\times1$卷积会引入大量的激活函数计算）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/16.png&#34; style=&#34;zoom:56%;&#34; /&gt;
&lt;h5 id=&#34;cspvovnet&#34;&gt;CSPVoVNet
&lt;/h5&gt;&lt;p&gt;CSPVoVNet考虑了梯度路径，使不同层学到更多样化的特征。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/18.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;elan&#34;&gt;ELAN
&lt;/h5&gt;&lt;p&gt;//TODO&lt;/p&gt;
&lt;h5 id=&#34;e-elan&#34;&gt;E-ELAN
&lt;/h5&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/19.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h4 id=&#34;model-scaling-for-concatenation-based-models&#34;&gt;Model scaling for concatenation-based models
&lt;/h4&gt;&lt;p&gt;EfficientNet、Scaled-YOLOv4等方法的模型缩放主要用于PlainNet或ResNet等架构中，当这些架构执行扩容、缩容时，每一层的入度和出度都不会改变，因此可以独立分析每个伸缩因子对参数和计算量的影响，但如果将这些方法用于基于级联的体系结构，对深度执行放大或缩小时，紧接在基于级联的计算快之后的translation层的入度就会增加或减小。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/14.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;对于基于级联的模型，不能单独分析不同的比例因子，当缩放计算块的深度时，还必须计算该块的输出通道变化，然后在过渡层执行宽度因子缩放，提出的复合尺度方法既能保持模型在初始设计时的性质，又能保持最优结构。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/15.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;trainable-bag-of-freebies&#34;&gt;Trainable bag-of-freebies
&lt;/h3&gt;&lt;h4 id=&#34;repconv&#34;&gt;RepConv
&lt;/h4&gt;&lt;h5 id=&#34;repvgg&#34;&gt;RepVGG
&lt;/h5&gt;&lt;p&gt;RepVGG（CVPR-2021）使用“VGG式”单路极简架构，仅用$3\times3$卷积和ReLU，在速度和性能上都达到了SOTA水平。&lt;/p&gt;
&lt;p&gt;“VGG式”模型有以下几大优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$3\times3$卷积速度极快（现有加速库都对$3\times3$卷积有所优化）；&lt;/li&gt;
&lt;li&gt;单路架构速度快（减少分支可以加快推理速度）；&lt;/li&gt;
&lt;li&gt;单路架构省内存；&lt;/li&gt;
&lt;li&gt;单路架构灵活性更好，可灵活改变各层宽度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RepVGG最大的亮点是利用了结构重参数化，训练一个模型后，将多分支模型等价转换为单路模型，通过这种方式同时利用多分支模型训练性能高的优势和单路模型推理速度快的优势。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
Conv(x,W1)+Conv(x,W2)+Conv(x,W3)=Conv(x,(W1+W2+W3))
$$&lt;p&gt;
$1\times1$相当于一个特殊的$3\times3$卷积，而恒等映射相当于一个特殊的$1\times1$卷积，因此也是一个特殊的$3\times3$卷积，所以只需：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 将identity转换为$1\times1$卷积（单位矩阵，当前通道为1，其他通道为0）；&lt;/li&gt;
&lt;li&gt;Step2. 将$1\times1$卷积转换为$3\times3$卷积（用0填充）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是输入、输出通道均为2的情况，$3\times3$卷积的参数是4个$3\times3$矩阵，$1\times1$卷积的参数是1个$2\times2$矩阵。&lt;/p&gt;
&lt;p&gt;三个分支均有BN层，但这并不会妨碍转换的可行性。&lt;/p&gt;
$$
\begin{aligned}
\mu_\mathcal{B}&amp;\leftarrow\frac{1}{m}\sum_{i=1}^mx_i \\
\sigma_\mathcal{B}^2&amp;\leftarrow\frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_\mathcal{B})^2 \\
\hat{x}_i&amp;\leftarrow\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}} \\
y_i&amp;\leftarrow\gamma\hat{x}_i+\beta\equiv\textbf{BN}_{\gamma,\beta}(x_i)
\end{aligned}
$$$$
y_i=\gamma\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}+\beta=\frac{\gamma}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}x_i+\left(\beta-\frac{\gamma\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}\right)
$$$$
W&#39;_{i,:,:,:}=\frac{\gamma_i}{\sigma_i}W_{i,:,:,:}\ ,\quad b&#39;_i=\frac{\mu_i\gamma_i}{\sigma_i}+\beta_i
$$&lt;p&gt;
因此训练好的模型可以等价转换为只有$3\times3$卷积的单路模型。&lt;/p&gt;
&lt;h6 id=&#34;planned-repconv&#34;&gt;Planned RepConv
&lt;/h6&gt;&lt;p&gt;尽管RepConv在VGG上取得了优异的性能，但将其直接应用于ResNet和DenseNet等网络结构上时，其精度会显著降低，作者使用&lt;strong&gt;梯度流传播路径&lt;/strong&gt;来分析不同的重参化模块应该和哪些网络搭配使用。&lt;/p&gt;
&lt;p&gt;通过分析RepConv与不同架构的组合以及产生的性能，作者发现RepConv中的identity破坏了ResNet中的残差结构和DenseNet中的跨层连接，而残差结构和跨层连接为不同的特征图提供了梯度的多样性。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/3.png&#34; style=&#34;zoom:60%;&#34; /&gt;
&lt;p&gt;（RepConvN：RepConv without identity connection）&lt;/p&gt;
&lt;p&gt;得到的结论是：具有残差连接的层，其RepConv不应该具有identity连接。&lt;/p&gt;
&lt;h5 id=&#34;coarse-for-auxiliary-and-fine-for-lead-loss&#34;&gt;Coarse for auxiliary and fine for lead loss
&lt;/h5&gt;&lt;h6 id=&#34;deep-supervision&#34;&gt;Deep supervision
&lt;/h6&gt;&lt;p&gt;深度监督作为一个训练技巧于2014年在&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1409.5185&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Deeply-Supervised Nets&lt;/a&gt;提出来，深度监督又称中继监督，在深度神经网络的某些中间隐藏层加了一个辅助的分类器作为一种网络分支来对主干网络进行监督，目的是为了浅层能够得到更加充分的训练，解决梯度消失和收敛速度过慢的问题。&lt;/p&gt;
&lt;p&gt;如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/6.png&#34; style=&#34;zoom:70%;&#34; /&gt;
&lt;h5 id=&#34;label-assignment&#34;&gt;Label assignment
&lt;/h5&gt;&lt;p&gt;首先简单介绍一下Label assignment，在监督学习中，计算loss，需要有预测结果和标签，在目标检测中最后输出的是框，需要把一些有价值的输出和GT匹配，简单的做法是人工定义规则，例如当生成的矩形框和GT的矩形框之间的IoU大于多少，就认为是目标，小于多少就认为是背景。&lt;/p&gt;
&lt;p&gt;YOLOv7中将负责最终输出的head为lead head，用于辅助训练head称为auxiliary head。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;近年来，研究者经常利用网络预测输出的质量和分布，并结合GT考虑，使用一些计算和优化方法来生成可靠的软标签，例如目标检测中，YOLO使用边界框的回归预测和GT的IoU作为客观的软标签。YOLOv7中，将网络预测结果与GT一起考虑。将结合模型预测结果和GT来获取软标签的模块称为“label assigner”。&lt;/p&gt;
&lt;p&gt;新的问题是：如何将软标签分配给auxiliary head和lead head，目前常用的方法是将auxiliary head和lead head分开，然后使用各自的结果和GT执行标签分配。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/8.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv7中，通过lead head预测来引导auxiliary head和lead  head，使用lead head预测作为指导，生成由粗到细的层次标签，分别用于auxiliary head和lead head的学习。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/9.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;具体地，lead head的预测结果和GT为依据生成软标签，但生成两个不同集合的软标签，粗粒度和细粒度，粗粒度标签是通过放松约束条件来让更多网格当作正样本，专注于提高auxiliary head的召回能力，lead head的输出结果可以从高召回的结果中过滤高准确的结果作为最终输出。&lt;/p&gt;</description>
        </item>
        <item>
        <title>异常检测</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</link>
        <pubDate>Wed, 15 Jun 2022 09:34:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=85&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;anomaly-detection&#34;&gt;Anomaly Detection
&lt;/h1&gt;&lt;h2 id=&#34;problem-formulation&#34;&gt;Problem Formulation
&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Given a set of training data ${x^1,x^2,\cdots,x^N}$&lt;/li&gt;
&lt;li&gt;We want to find a function detecting input $x$ is &lt;em&gt;similar&lt;/em&gt; to training data or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;categories&#34;&gt;Categories
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;case-1-with-classifier&#34;&gt;Case 1: With Classifier
&lt;/h2&gt;&lt;h3 id=&#34;example-application&#34;&gt;Example Application
&lt;/h3&gt;&lt;p&gt;例：人物是否来自辛普森一家？&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\hat{y}^1=霸子\quad \hat{y}^2=麗莎\quad \hat{y}^3=荷馬\quad \hat{y}^2=美枝
$$
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Simpsons Characters Data | Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-the-classifier&#34;&gt;How to use the Classifier
&lt;/h3&gt;&lt;p&gt;分类器不知做分类这件事，除了输出它是辛普森一家的哪一个人物以外，还会输出信心分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Anomaly Detection：&lt;/strong&gt;
$$
f(x)=
\left{
\begin{aligned}
&amp;amp;normal,\quad &amp;amp;c(x)\gt\lambda \
&amp;amp;anomaly,\quad &amp;amp;c(x)\leqslant\lambda
\end{aligned}&lt;/p&gt;
&lt;p&gt;\right.
$$&lt;/p&gt;
&lt;h3 id=&#34;how-to-estimate-confidence&#34;&gt;How to estimate Confidence
&lt;/h3&gt;&lt;p&gt;Softmax Layer反映了Output的概率分布。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一般使用最高的分数当作信心分数，也可以使用负熵（negative entropy）。&lt;/p&gt;
&lt;h3 id=&#34;outlook-network-for-confidence-estimation&#34;&gt;Outlook: Network for Confidence Estimation
&lt;/h3&gt;&lt;p&gt;可以直接让一个模型去学如何输出信心分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Terrance DeVries, Graham W. Taylor, Learning Confidence for Out-of-Distribution  Detection in Neural Networks, arXiv, 2018&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation
&lt;/h3&gt;&lt;p&gt;正负样本比例往往比较悬殊，评估模型效果时正确率并不是很好的指标。&lt;/p&gt;
&lt;p&gt;Cost Table（给予假阴和假阳的权重）不同，得到的结果也不同。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/8.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一般使用Area under ROC curve，只考虑排序，不考虑阈值$\lambda$。&lt;/p&gt;
&lt;h3 id=&#34;possible-issues&#34;&gt;Possible Issues
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/9.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/10.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;h4 id=&#34;learn-a-classifier-giving-low-confidence-score-to-anomaly&#34;&gt;Learn a classifier giving low confidence score to anomaly
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin, Training Confidencecalibrated Classifiers for Detecting Out-of-Distribution Samples, ICLR 2018&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;how-can-you-obtain-anomaly&#34;&gt;How can you obtain anomaly?
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Mark Kliger, Shachar Fleishman, Novelty Detection with GAN, arXiv, 2018&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-2-without-labels&#34;&gt;Case 2: Without Labels
&lt;/h2&gt;&lt;h3 id=&#34;twitch-plays-pokémon&#34;&gt;Twitch Plays Pokémon
&lt;/h3&gt;&lt;p&gt;多人同时玩一款游戏，因为有“Troll”（网络小白）不会玩游戏、乱按或者出于恶意，导致游戏非常困难。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/11.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;假设多数玩家是想要通关的，从多数玩家的行为去侦测出异常的玩家。&lt;/p&gt;
&lt;h3 id=&#34;problem-formulation-1&#34;&gt;Problem Formulation
&lt;/h3&gt;&lt;p&gt;需要先把一个玩家$x$表示成一个向量$[x_1,x_2,\cdots]^T$，例如$x_1$表示说垃圾话（不能操控游戏的多余的发言），假设游戏中有民主状态（每隔20秒，系统选发言最多的指令）和无政府状态（系统不断地选看到的输入指令），$x_2$表示在无政府状态时的发言。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/12.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在没有分类器的情况下，可以建立一个模型，这个几率模型告诉我们某一种行为发生的概率$P(X)$有多大。&lt;/p&gt;
&lt;p&gt;如果$P(X)\geqslant \lambda$，则说他是正常的，否则是异常的。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/13.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;假设每一个玩家可以用二维向量$x=\begin{bmatrix}x_1 \ x_2\end{bmatrix}$表示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/14.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;maximum-likelihood&#34;&gt;Maximum Likelihood
&lt;/h3&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;假设数据点是从一个概率密度函数$f_\theta(x)$采样得到的
&lt;ul&gt;
&lt;li&gt;$\theta$ determines the shape of $f_\theta(x)$&lt;/li&gt;
&lt;li&gt;$\theta$ is unknown, to be found from data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
$$
L(\theta)=f_\theta(x^1)f_\theta(x^2)\cdots f_\theta(x^N)
$$$$
\theta^*=\arg\max_\theta L(\theta)
$$$$
f_{\mu,\Sigma}(x)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}
$$&lt;ul&gt;
&lt;li&gt;输入：vector $x$&lt;/li&gt;
&lt;li&gt;输出：probability density of sampling $x$&lt;/li&gt;
&lt;li&gt;均值$\mu$和协方差矩阵$\Sigma$是决定函数形态的$\theta$&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
L(\theta)&amp;=f_\theta(x^1)f_\theta(x^2)\cdots f_\theta(x^N) \\
&amp;\Downarrow \\
L(\mu,\Sigma)&amp;=f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)\cdots f_{\mu,\Sigma}(x^N) \\
\\
\theta^*&amp;=\arg\max_\theta L(\theta)\\
&amp;\Downarrow \\
\mu^*,\Sigma^*&amp;=\arg\max_{\mu,\Sigma}L(\mu,\Sigma)
\end{aligned}
$$$$
\mu^*=\frac{1}{N}\sum_{n=1}^Nx^n\quad \Sigma^*=\frac{1}{N}\sum_{n=1}^N(x-\mu^*)(x-\mu^*)^T
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/15.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;outlook-auto-encoder&#34;&gt;Outlook: Auto-encoder
&lt;/h3&gt;&lt;p&gt;用辛普森一家的图片去训练Auto-encoder，Auto-encoder就会特别还原辛普森一家的图片，但如果输入一张异常图片，重建的结果可能会差很多。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/16.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/17.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>自编码器</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</link>
        <pubDate>Sun, 05 Jun 2022 16:57:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=83&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——自编码器（Auto-encoder）&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;auto-encoder&#34;&gt;AUTO-ENCODER
&lt;/h1&gt;&lt;h2 id=&#34;reviewself-supervised-learning-framework&#34;&gt;Review：Self-supervised Learning Framework
&lt;/h2&gt;&lt;p&gt;AUTO-ENCODER可以算是Self-supervised Learning的一环。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在BERT、GPT之前有更古老的不需要标注资料的任务，就叫做Auto-Encoder，可以看作是Self-supervised Learning的一种预训练方法，跟填空、预测接下来的token是很相似的概念。&lt;/p&gt;
&lt;h2 id=&#34;basic-idea-of-auto-encoder&#34;&gt;Basic Idea of Auto-encoder
&lt;/h2&gt;&lt;h3 id=&#34;auto-encoder-1&#34;&gt;Auto-encoder
&lt;/h3&gt;&lt;p&gt;以影像为例，在Auto-encoder里面有两个网络，分别是Encoder和Decoder，Encoder（可能是很多层的CNN）读入一张图片，将其变成一个向量，向量作为Decoder的输入，Decoder（可能是GAN中的Generator）会产生一张图片。训练的目标是希望Encoder的输入和Decoder的输出越接近越好（Reconstruction，重建）。&lt;/p&gt;
&lt;p&gt;Auto-encoder的概念和Cycle GAN是一模一样的。&lt;/p&gt;
&lt;p&gt;Encoder的输出叫做Embedding，也叫做Representation、Code&amp;hellip;&lt;/p&gt;
&lt;p&gt;要把Auto-encoder用在下游任务中，常见的用法是，输入的图片这个向量可能会比较长，经过Auto-encoder的Encoder后得到一个比较短的向量，再用这个Embedding去做接下来想做的事情。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;more-dimension-reduction&#34;&gt;More Dimension Reduction
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;PCA&lt;/li&gt;
&lt;li&gt;t-SNE&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-auto-encoder&#34;&gt;Why Auto-encoder？
&lt;/h3&gt;&lt;p&gt;例如一张$3\times 3$的图片经过Encoder得到2维的Embedding，再经过Decoder得到$3\times 3$的图片，虽然9个数值才能描述这张图，但实际上图片的变化可能非常有限，一个低维的Embedding就可以描述。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;在神雕侠侣中，杨过三招之内把樊一翁的胡子剪掉，因为樊一翁的胡子是由他的头控制的，虽然胡子可以甩两丈长，但头能够做的变化非常有限，表面上胡子的鞭法很厉害，但只要打他的头，就能让他胡子的变化非常有限。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;de-noising-auto-encoder&#34;&gt;De-noising Auto-encoder
&lt;/h3&gt;&lt;p&gt;将原始的图片上加上一些噪声，再通过Auto-encoder，再让生成的图片和原始图片越接近越好，此时Encoder和Decoder又增加了一个任务：消除噪声。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Vincent, Pascal, et al. &amp;ldquo;Extracting and composing robust featureswith denoising autoencoders.&amp;rdquo; ICML, 2008.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在BERT中，我们加入的Mask其实就是噪声，BERT就是Encoder，BERT的输出是Embedding，输出经过的线性层是Decoder，结果是要做填空题，即和真实内容越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;feature-disentanglement&#34;&gt;Feature Disentanglement
&lt;/h2&gt;&lt;h3 id=&#34;representation-includes-information-of-different-aspects&#34;&gt;Representation includes information of different aspects
&lt;/h3&gt;&lt;p&gt;一张图片包含了色泽、纹理等多方面的内容，一段语音讯号包含了文字、语者等方面的内容&amp;hellip;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;feature-disentangle&#34;&gt;Feature Disentangle
&lt;/h3&gt;&lt;p&gt;Feature Disentagle想要做的事情是，有没有办法在训练一个Auto-encoder时，同时有办法知道这个Embedding的哪些维度代表了哪些资讯。举例来说，一个语音讯号经过Encoder，得到的100维的Embedding中，前50维是这句话的内容，后50维是这句话语者的特征。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.05742&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.05742] One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1804.02812&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1804.02812] Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.05879&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.05879] AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;application-voice-conversion&#34;&gt;Application: Voice Conversion
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/8.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;discrete-latent-representation&#34;&gt;Discrete Latent Representation
&lt;/h2&gt;&lt;p&gt;到目前位置，我们都假设Embedding是Real numbers（$[0.9\ 0.1\ 0.3\ 0.7]$）；此外它也可以是Binary（$[1\ 0\ 0\ 1]$），它代表了某种特性有或是没有，例如第一维的1代表是男性，0代表是女性；甚至也有可能是一个One-hot向量，例如做手写数字识别，代表着不同的数字。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/9.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;discrete-representation&#34;&gt;Discrete Representation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1711.00937&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1711.00937] Neural Discrete Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discrete Representation中，最知名的是VQVAE，VQVAE中，输入一张图片，Encoder输出一个连续的向量，接下来有一个Codebook（一排向量，学出来的），让Encoder的输出和每一个向量都计算相似度（类似Self-attention），再从Codebook中拿出相似度最高的向量，把这个向量输入到Decoder里面，输出一张图片，和输入越接近越好。&lt;/p&gt;
&lt;p&gt;VQVAE中，Decoder的输入只能是Codebook的其中一个，Embedding没有无穷无尽的可能，用这种方式在语音讯号学习时，可以学到最基本的发音单位（arxiv 1901.08810）。&lt;/p&gt;
&lt;h3 id=&#34;text-as-representation&#34;&gt;Text as Representation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.02851&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1810.02851] Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Representation也可以不是向量，而是一段文字，Encoder输出的文字可能是这篇文章的精华内容，此时Auto-encoder可以看作是两个Seq2seq的模型（&lt;strong&gt;seq2seq2seq auto-encoder&lt;/strong&gt;），但实际上这样做行不通，Encoder和Decoder间可能会学出它们之间的“暗号”，此时需要一个Discriminator，来判断生成的这个句子是不是人写的句子，Encoder要想办法产生一段句子不只透过Decoder产生原文，而且要想办法骗过Discriminator觉得是人写的句子。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/10.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;如何训练？：&lt;strong&gt;RL硬做&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;tree-as-embedding&#34;&gt;Tree as Embedding
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/11.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1806.07832&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1806.07832] StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.03746&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.03746] Unsupervised Recurrent Neural Network Grammars (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-applications&#34;&gt;More Applications
&lt;/h2&gt;&lt;h3 id=&#34;generator&#34;&gt;Generator
&lt;/h3&gt;&lt;p&gt;此前介绍的基本都是拿Encoder的输出做下游任务，实际上Decoder就相当于一个Generator，吃一个向量产生一个图片。&lt;/p&gt;
&lt;p&gt;除了GAN外，还有另外两种生成式模型，variational auto-encoder（VAE）就是其中一个，它就是把Auto-encoder的Decoder拿出来当Generator来用。&lt;/p&gt;
&lt;h3 id=&#34;compression&#34;&gt;Compression
&lt;/h3&gt;&lt;p&gt;可以把Encoder的输出当作是图片压缩后的结果，Decoder的输出当作是解压后的结果，只是经过压缩和解压缩后，图片会有一定的失真。&lt;/p&gt;
&lt;h3 id=&#34;anomaly-detection&#34;&gt;Anomaly Detection
&lt;/h3&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Given a set of training data ${x^1,x^2\cdots,x^N}$&lt;/li&gt;
&lt;li&gt;Detecting input $x$ is &lt;em&gt;similar&lt;/em&gt; to training data or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/12.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一个对象是否正常，取决于训练资料中的内容。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/13.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;fraud-detection&#34;&gt;Fraud Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Traning data: credit card transactions, $x$: fraud or not
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Credit Card Fraud Detection | Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;network-intrusion-detection&#34;&gt;Network Intrusion Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Traning data: connection, $x$: attack or not
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;KDD Cup 1999 Data (uci.edu)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cancer-detection&#34;&gt;Cancer Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Traning data: normal cells, $x$: cancer or not?
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Breast Cancer Wisconsin (Diagnostic) Data Set | Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;异常检测中往往是有一大堆正常的样本，而只有少量或者没有负样本，所以它不是一个一般的分类问题（One Class分类问题）。&lt;/p&gt;
&lt;h4 id=&#34;approach-auto-encoder&#34;&gt;Approach: Auto-encoder
&lt;/h4&gt;&lt;p&gt;例：真实人脸检测&lt;/p&gt;
&lt;p&gt;用真人的人脸去训练一个Auto-encoder，对于一张异常的图片，它的reconstruction loss会比较大。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/14.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>语音与影像上的自监督式学习模型</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%AF%AD%E9%9F%B3%E4%B8%8E%E5%BD%B1%E5%83%8F%E4%B8%8A%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</link>
        <pubDate>Fri, 20 May 2022 21:39:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%AF%AD%E9%9F%B3%E4%B8%8E%E5%BD%B1%E5%83%8F%E4%B8%8A%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=76&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——语音与影像上的神奇自督导式学习模型&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-supervised-learning-for-speech-and-image&#34;&gt;Self-supervised Learning for Speech and Image
&lt;/h1&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;h3 id=&#34;review-self-supervised-learning-for-text&#34;&gt;Review: Self-supervised Learning for &lt;strong&gt;Text&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;使用大量无标注的资料来训练BERT，然后在BERT的输出后接下游模型，并提供少量有标注的资料来完成下游任务的训练，下游模型往往都是比较简单的模型。这些有标注的训练资料也可以同时拿来微调BERT。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/review.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;self-supervised-learning-for-speech&#34;&gt;Self-supervised Learning for &lt;strong&gt;Speech&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;使用未标注的语音讯号来训练语音版的BERT，BERT会将声音讯号变为一排向量，将少量的标注资料提供给下游模型。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/speech%20bert.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://superbbenchmark.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;S&lt;/strong&gt;peech processing &lt;strong&gt;U&lt;/strong&gt;niversal &lt;strong&gt;PER&lt;/strong&gt;formance &lt;strong&gt;B&lt;/strong&gt;enchmark（SUPERB）&lt;/a&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/superb.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;self-supervised-learning-for-image&#34;&gt;Self-supervised Learning for &lt;strong&gt;Image&lt;/strong&gt;
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/for%20image.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2011.13377&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2011.13377] How Well Do Self-Supervised Models Transfer? (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.01235&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.01235] Scaling and Benchmarking Self-Supervised Visual Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不仅是在语言上，自监督式学习在语音和图像上也有着非常好的表现，下面将介绍在影像和语音上使用自监督学习的五大方法。&lt;/p&gt;
&lt;h2 id=&#34;generative-approaches&#34;&gt;Generative Approaches
&lt;/h2&gt;&lt;p&gt;将文字上已经非常成功的方法（BERT、GPT）拿来语音和图像上使用。&lt;/p&gt;
&lt;h3 id=&#34;maskingbert-series&#34;&gt;Masking（BERT series）
&lt;/h3&gt;&lt;p&gt;将某些声音讯号盖起来，让模型去还原被盖起来的部分。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.12638&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1910.12638] Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/mockingjay.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Smoothness of acoustic features（Mockingjay）&lt;/p&gt;
&lt;p&gt;语音和文字有所不同，声音讯号表示成一排向量，但相邻的向量和向量之间非常接近，把某个向量盖起来后，只需拿两边的向量做内插，就可以大致还原出被盖住的内容，故每次Mask时，Mask一长排的数据。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/masking%20consecutive%20features.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Masking strategies for speech&lt;/p&gt;
&lt;p&gt;在语音上可以做新的尝试，不再时间的方向上做Masking，而是Mask这些向量的某几个dimension。这种方法可以学到更多语者方面的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/masking%20specific%20dimensions.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;predicting-futuregpt-series&#34;&gt;Predicting Future（GPT series）
&lt;/h3&gt;&lt;p&gt;在文字上，GPT预测下一个会出现的token（$n=1$），而在语音上，由于相邻向量很接近，所以通常会预测接下来&lt;strong&gt;某一段时间之后&lt;/strong&gt;的向量（$n\gt 3$）。&lt;/p&gt;
&lt;p&gt;代表性的模型是APC（Autoregressive Predictive Coding）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/predicting%20future.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;how-about-image&#34;&gt;How about &lt;strong&gt;image&lt;/strong&gt;?
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://openai.com/blog/image-gpt/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Image GPT (openai.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将一张图片的各个pixel拉直，训练模型，再将训练后的模型用在下游任务中。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;影像版的GPT：给一段pixel，预测下一个pixel。&lt;/li&gt;
&lt;li&gt;影像版的BERT：将一段pixel的一部分盖起来，让机器预测被盖起来的部分。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相较由token来表示的文字，语音和影像中包含了更多的信息，让机器产生复杂的影像和声音讯号往往不是一件容易的事情。除了让机器还原影像和声音讯号外，能否还原、预测其他内容，从而达到和Self-supervised learning一样的效果？&lt;/p&gt;
&lt;h2 id=&#34;predictive-approach&#34;&gt;Predictive Approach
&lt;/h2&gt;&lt;h3 id=&#34;image---predicting-rotation&#34;&gt;Image - Predicting Rotation
&lt;/h3&gt;&lt;p&gt;让机器去学习一张图片有没有被旋转过。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1803.07728&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1803.07728] Unsupervised Representation Learning by Predicting Image Rotations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/predicting%20rotation.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image---context-prediction&#34;&gt;Image - Context Prediction
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1505.05192&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1505.05192] Unsupervised Visual Representation Learning by Context Prediction (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;给一张比较大的图片，将图片中的两个小块切出来，让模型去回答第一块在第二块的哪个方向。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/context%20prediction.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;similar-idea-on-speech&#34;&gt;Similar idea on &lt;strong&gt;Speech&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/9060816&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Pre-Training Audio Representations With Self-Supervision | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从一句话中切两段出来，让机器去判断两段声音讯号相距多长时间。&lt;/p&gt;
&lt;h3 id=&#34;predict-simplified-objects&#34;&gt;Predict Simplified Objects
&lt;/h3&gt;&lt;p&gt;把原来要生成的复杂的内容简化。&lt;/p&gt;
&lt;p&gt;以语音讯号为例，对声音讯号中的向量做clustering（K-means, etc.），将其离散化，每个向量变成一个token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/simplified%20object.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speech&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HuBERT：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2106.07447&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2106.07447] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEST-RQ：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2202.01855&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2202.01855] Self-supervised Learning with Random-projection Quantizer for Speech Recognition (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DeepCluster：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1807.05520&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1807.05520] Deep Clustering for Unsupervised Learning of Visual Features (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;模型是否在不产生内容的情况下学习？&lt;/p&gt;
&lt;h2 id=&#34;contrastive-learning&#34;&gt;Contrastive Learning
&lt;/h2&gt;&lt;h3 id=&#34;basic-idea-of-contrastive-learning&#34;&gt;Basic Idea of Contrastive Learning
&lt;/h3&gt;&lt;p&gt;一对positive（属于同一类）的样本，扔到Encoder中，他们的输出越接近越好，而对于一对negative（属于不同类）的样本，他们的产生的向量越远越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/basic%20idea%20of%20contrastive%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;没有让机器去预测或产生任何内容，而是找出positive的pair，让其向量越近越好，找出negative的pair，其向量越远越好。但目前并没有任何标签，不知道哪些图片是同样类别而哪些图片是不同类别。&lt;/p&gt;
&lt;h3 id=&#34;simclr&#34;&gt;SimCLR
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.05709&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.05709] A Simple Framework for Contrastive Learning of Visual Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将同一张图片做数据增强（随机裁切、颜色变型、高斯噪声等），同一张图片做数据增强得到的图片为正样本，而和另一张图片的增强结果为负样本。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/simclr.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;文章中探讨了各种数据增强的组合，比较一致的发现是，通常随机裁切是不可或缺的方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speech&lt;/strong&gt; SimCLR：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.13991&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2010.13991] Speech SIMCLR: Combining Contrastive and Reconstruction Objective for Self-supervised Speech Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;moco&#34;&gt;MoCo
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1911.05722&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1911.05722] Momentum Contrast for Unsupervised Visual Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/moco.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MoCo v2&lt;/strong&gt;（吸收SimCLR的优点后产生）：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.04297&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.04297] Improved Baselines with Momentum Contrastive Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contrastive-learning-for-speech&#34;&gt;Contrastive Learning for &lt;strong&gt;Speech&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;CPC：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1807.03748&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1807.03748] Representation Learning with Contrastive Predictive Coding (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wav2vec：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.05862&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.05862] wav2vec: Unsupervised Pre-training for Speech Recognition (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两段声音讯号，经过Encoder后各产生一排向量，将Encoder的输出再通过一个Predicter产生新的输出，新的输出和与它相邻的向量是positive的pair，而与另一个句子中的向量是negative的向量。新的输出向量通过不同的线性变换以后，和下一个时间点的输出越接近越好，和其他的句子的向量越远越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/contrastive%20learning%20for%20speech.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可以拿Encoder或Encoder和Predicter用在下游任务中。&lt;/p&gt;
&lt;p&gt;CPC和Wav2vec的主要差别在Predicter上，后来又出现了VQ-wav2vec。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VQ-wav2vec &lt;em&gt;+ BERT&lt;/em&gt;：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.05453&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1910.05453] vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/vq-wav2vec.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;VQ-wav2vec中，Encoder的输出不是向量，而是离散的token。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How to train with quantization：&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=JZvEzb5PV3U&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【機器學習2021】自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用 - YouTube&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在原论文中，VQ-wav2vec的提出是为了在后面接上一个类似文字BERT的Encoder，VQ-wav2vec的输出正好可以当成BERT架构Encoder的输入。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/vq-wav2vec_2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;这样做的另一个好处是模型可以比较容易的抽出和声音的内容有关的资讯。&lt;/p&gt;
&lt;p&gt;Discrete BERT中说明了VQ-wav2vec + BERT的结构具有较好的表现。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discrete BERT：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1911.03912&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1911.03912] Effectiveness of self-supervised pre-training for speech recognition (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在VQ-wav2vec + BERT架构的结构中，VQ-wav2vec和BERT架构是分开训练的，在Wav2vec 2.0中，将二者合并起来一起训练。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.11477&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.11477] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但合起来训练时，由于第一个Encoder的输出是离散token，不方便训练，所以将离散token改为向量，再将向量输入到第二个Encoder中，第二个Encoder会对输入的向量做masking，再用被盖住位置的输出向量去预测这个位置的离散token，同时希望这个向量产生其他向量的可能性越小越好。（&lt;em&gt;在原论文中，离散token都经过一个变换转为了embedding，希望输出的向量和对应位置的embedding越接近越好，实际上这就等价于把第二个Encoder的输出向量做线性变换后当作分类问题，希望分类为当前token的概率越高越好，分类为其他token的概率越小越好。&lt;/em&gt;）&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/wav2vec2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;不直接将离散token扔到第二个Encoder中，而是将连续向量扔进去，直观的解释是离散token包含的信息较少，而连续向量包含的信息相对要多。若第二个Encoder中输入的是离散token，其表现会掉一大截。&lt;/p&gt;
&lt;p&gt;另外，没有将其当作一个分类问题来做的原因可能是，语音中的token数量太大了，大概是十万量级，而常见语言模型的token数量也只有两万左右。&lt;/p&gt;
&lt;h3 id=&#34;alterative-way-to-understand-wav2vec-20&#34;&gt;Alterative way to understand Wav2vec 2.0
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;文字是否可以做Contrastive learning？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;事实上，BERT也可以看作是一种Contrastive learning的方式。&lt;/p&gt;
&lt;p&gt;BERT可以看作是在完成分类任务，但分类任务也可以看作Contrastive learning，例如“深 [MASK] 学 习”经过BERT得到embedding，通过线性变换和Softmax，希望得到“度”的概率越大越好，其他词的概率越小越好，即BERT输出的embedding和“度”的embedding距离越近越好，和其他词的embedding距离越远越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/bert%20contrastive%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在语音上，不像文字中negative样本是可以穷举的，语音讯号通常会经过一个网络后，将其离散化再进行处理。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/wav2vec2%20speech.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;classification-vs-contrastive&#34;&gt;Classification vs. Contrastive
&lt;/h4&gt;&lt;h5 id=&#34;classification&#34;&gt;Classification
&lt;/h5&gt;&lt;p&gt;对于分类问题，模型最后一层的输出向量通过一个线性变换（乘一个矩阵，和矩阵里的行做dot-product），希望和对应的行向量做dot-product后值越大越好，和其他行向量计算后结果越小越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/classification.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;contrastive&#34;&gt;Contrastive
&lt;/h5&gt;&lt;p&gt;希望模型最后一层的输出向量和对应分类的embedding越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/contrastive.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;selecting-negative-examples-is-not-trivial&#34;&gt;Selecting Negative Examples is not trivial
&lt;/h3&gt;&lt;p&gt;negative样本的选取应该足够困难，但同时也不应该过难。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/selecting%20negative%20examples.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如何避开负样本的选择？&lt;/p&gt;
&lt;h2 id=&#34;bootstrapping-approaches&#34;&gt;Bootstrapping Approaches
&lt;/h2&gt;&lt;p&gt;如果训练时不使用正样本，仅用正样本来训练，最后模型对于所有图像都会判断他们很接近。&lt;/p&gt;
&lt;p&gt;解决的办法是，在其中一个Encoder后接一个Predictor（可能是只有几层的前馈神经网络），训练的目标是两个向量越接近越好。但只计算Predictor所在的这一边的梯度，更新这一边的参数，再将这一边Encoder更新后的参数复制到另一边。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/bootstrapping%20approaches.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;这看起来像是某种“妖术”，但大量实验表明这种方法的确有效&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;alterative-way-to-understand-bootstrapping&#34;&gt;Alterative way to understand Bootstrapping
&lt;/h3&gt;&lt;h4 id=&#34;typical-knowledge-distillation知识蒸馏&#34;&gt;Typical Knowledge Distillation（知识蒸馏）
&lt;/h4&gt;&lt;p&gt;Teacher的参数固定，更新Student的参数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/knowledge%20distillation.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image&#34;&gt;Image
&lt;/h3&gt;&lt;h4 id=&#34;bootstrap-your-own-latentbyol&#34;&gt;Bootstrap your own latent（BYOL）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.07733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.07733] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
\theta\leftarrow\lambda\theta+(1-\lambda)\theta&#39;
$$&lt;p&gt;Teacher Encoder的参数不会马上变得和Student Encoder一样，用一个渐进的方法影响Teacher Encoder。&lt;/p&gt;
&lt;h4 id=&#34;simple-siamesesimsiam&#34;&gt;Simple Siamese（SimSiam）
&lt;/h4&gt;&lt;p&gt;Moving Average不是必要的，直接复制参数同样可行。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.07733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.07733] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;speech&#34;&gt;Speech
&lt;/h3&gt;&lt;h4 id=&#34;data2vec&#34;&gt;Data2vec
&lt;/h4&gt;&lt;p&gt;在文字、影像、语音上都有不错的结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2202.03555&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2202.03555] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;simply-extra-regularization&#34;&gt;Simply Extra Regularization
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.03230&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2103.03230] Barlow Twins: Self-Supervised Learning via Redundancy Reduction (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.04906&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.04906] VICReg: &lt;strong&gt;Variance-Invariance-Covariance Regularization&lt;/strong&gt; for Self-Supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;variance-invariance-convariance-regularization&#34;&gt;&lt;strong&gt;Variance&lt;/strong&gt;-&lt;strong&gt;Invariance&lt;/strong&gt;-&lt;strong&gt;Convariance&lt;/strong&gt; Regularization
&lt;/h3&gt;&lt;h4 id=&#34;invariance&#34;&gt;Invariance
&lt;/h4&gt;&lt;p&gt;用正样本来训练，输出向量越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/invariance.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;variance&#34;&gt;Variance
&lt;/h4&gt;&lt;p&gt;给Encoder一个batch的图片，得到一个batch数量的向量，这些向量的每一个维度的方差要大于某一个阈值。&lt;/p&gt;
&lt;p&gt;通过这种方法来避免Encoder看到什么图片都输出同一个向量。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/variance.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;covariance&#34;&gt;Covariance
&lt;/h4&gt;&lt;p&gt;把一个batch里的向量拿出来计算协方差，协方差矩阵中非对角线的元素接近0。&lt;/p&gt;
&lt;p&gt;有了Covariance后，让所有维度都充分利用到，散布更平均。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/covariance.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>自监督式学习</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Tue, 17 May 2022 14:32:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=71&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-supervised-learning&#34;&gt;Self-Supervised Learning
&lt;/h1&gt;&lt;h2 id=&#34;芝麻街与进击的巨人&#34;&gt;芝麻街与进击的巨人
&lt;/h2&gt;&lt;h3 id=&#34;芝麻街&#34;&gt;芝麻街
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-1/stavreal.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;进击的巨人bertolt-hoover&#34;&gt;进击的巨人：Bertolt Hoover
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-1/%E8%B4%9D%E7%89%B9%E9%9C%8D%E5%B0%94%E5%BE%B7%E8%83%A1%E4%BD%9B.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;主流模型参数量&#34;&gt;主流模型参数量
&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Model&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Parameters&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ELMO&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;94M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;BERT&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;340M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;GPT-2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1542M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Megatron&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;8B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;T5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;11B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Turing NLG&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;17B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;GPT-3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;175B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Switch Transformer&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1.6T&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;bert&#34;&gt;BERT
&lt;/h2&gt;&lt;h3 id=&#34;self-supervised-learning-1&#34;&gt;Self-supervised Learning
&lt;/h3&gt;&lt;p&gt;对于数据$x$，监督学习需要知道数据的标签$\hat{y}$，来让模型输出我们想要的$y$。而自监督学习没有标注，将$x$分为两部分，一部分$x&amp;rsquo;$输入到模型得到$y$，另一部分$x&amp;rsquo;&amp;rsquo;$作为标签，然后让$y$和$x&amp;rsquo;&amp;rsquo;$越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/self-supervised%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;自监督学习可以看作是一种无监督学习的方法，无监督学习的范围很大，里面有很多不同的方法，为了明确说明现在说做的工作，就称为自监督学习。&lt;/p&gt;
&lt;h3 id=&#34;masked-token-prediction&#34;&gt;Masked token prediction
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.04805&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BERT的架构和Transfromer Encoder相同，输入一排向量，输出另一排向量，一般用在文字处理上。&lt;/p&gt;
&lt;p&gt;输入一串token（token是处理一段文字的单位，在中文里一般把一个方块字当作一个token。），随机盖住一些token，盖住token有两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;变为某个特殊的token&lt;/li&gt;
&lt;li&gt;随机换为另一个token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对BERT的输出序列分别做线性变换（乘矩阵），再做Softmax就得到了一个分布。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/masking%20input.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;BERT不知道被盖住的部分是什么内容，但我们知道这部分内容，BERT学习的目标是输出和盖住的部分越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/masking%20input2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;next-sentence-prediction&#34;&gt;Next Sentence Prediction
&lt;/h3&gt;&lt;p&gt;从资料库中拿出两个句子，两个句子之间加入特殊的分隔符号[SEP]，再整个序号的最前面加[CLS]符号，整个序列输入BERT，看[CLS]对应的输出，[CLS]经过BERT的输出再经过线性变换后输出为Yes/No，代表这两个句子是不是相接的。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;例：[CLS] I like cat. [SEP] He likes dog&lt;/em&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/next%20sentence%20prediction.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Next Sentence Prediction对于BERT接下来要做的事情可能无用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1907.11692&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1907.11692] RoBERTa: A Robustly Optimized BERT Pretraining Approach (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next Sentence Prediction这个任务可能比较简单，BERT可能学习不到太多有用的东西。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SOP&lt;/strong&gt;：Sentence order prediction Used in ALBERT&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1909.11942&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1909.11942] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两个句子本来就连在一起，人为拆分开，本来放在前面的句子作为Sentence 1，本来放在后面的句子作为Sentence 2，或本来放在前面的句子作为Sentence 2，本来放在后面的句子作为Sentence 1。然后让BERT去回答是哪一种顺序。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;downstream-tasks&#34;&gt;Downstream Tasks
&lt;/h3&gt;&lt;p&gt;在训练BERT时，给了BERT两个任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Masked token prediction&lt;/li&gt;
&lt;li&gt;Next sentence prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在训练BERT时，似乎仅仅在教BERT如何去做“填空题”，但BERT可以用在其他地方，BERT真正在下游任务（Downstream Tasks）中被使用，但需要少量有标注的数据。BERT经过微调（Fine-tune）可以去完成各种其他的任务。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/Fine-tune.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;glue&#34;&gt;GLUE
&lt;/h3&gt;&lt;p&gt;任务集GLUE（General Language Understanding Evaluation）共有9个任务，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corpus of Linguistic Acceptability（CoLA）&lt;/li&gt;
&lt;li&gt;Stanford Sentiment Treebank（SST-2）&lt;/li&gt;
&lt;li&gt;Microsoft Research Paraphrase Corpus（MRPC）&lt;/li&gt;
&lt;li&gt;Quora Question Pairs（QQP）&lt;/li&gt;
&lt;li&gt;Semantic Textual Similarity Benchmark（STS-B）&lt;/li&gt;
&lt;li&gt;Multi-Genre Natural Language Inference（MNLI）&lt;/li&gt;
&lt;li&gt;Question-answering NLI（QNLI）&lt;/li&gt;
&lt;li&gt;Recognizing Textual Entailment（RTE）&lt;/li&gt;
&lt;li&gt;Winograd NLI（WNLI）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以在这9个任务上分别微调模型得到9个模型，通过结果数值来判断模型的好坏。&lt;/p&gt;
&lt;p&gt;中文版本的GLUE：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cluebenchmarks.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CLUE中文语言理解基准测评 (cluebenchmarks.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-bert&#34;&gt;How to use BERT
&lt;/h3&gt;&lt;h4 id=&#34;case-1&#34;&gt;Case 1
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Input：sequence&lt;/li&gt;
&lt;li&gt;Output：class&lt;/li&gt;
&lt;li&gt;Example：Sentiment analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;给BERT输入一个句子，前面放[CLS] token，对[CLS]输出的向量做线性变换，Softmax后输出class。需要提供大量的已标注的训练资料。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Linear部分是随机初始化，而BERT部分将学会了做“填空题”的BERT模型的参数拿来初始化。&lt;/p&gt;
&lt;h4 id=&#34;case-2&#34;&gt;Case 2
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Input：sequence&lt;/li&gt;
&lt;li&gt;Output：sequence&lt;/li&gt;
&lt;li&gt;Example：POS tagging（词性标注）&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;给BERT输入一个句子，前面放[CLS] token，对句子里面每一个token输出的向量做线性变换，Softmax后输出每一个token的类别。需要提供已标注的训练资料。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;case-3&#34;&gt;Case 3
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Input：two sequences&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output：a class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example：Natural Language Inference（NLI）&lt;/p&gt;
&lt;p&gt;前提：一个人骑马越过了一架坏掉的飞机，假设：这个人在一个小餐馆里面，输出：矛盾。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case3_eg.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;输入两个句子，两个句子之间放[SEP] token，第一个句子前放[CLS] token，整串内容输入BERT，对[CLS]输出的向量做线性变换，Softmax后输出class。需要提供已标注的训练资料。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;case-4&#34;&gt;Case 4
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Extraction-based Question Answering&lt;/strong&gt;（有限制的QA，答案一定能在文章中找到。）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Input：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Document：$D={d_1,d_2,\cdots,d_N}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Query：$Q={q_1,q_2,\cdots,q_M}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;对于中文，$d_i$和$q_i$都是汉字。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output：two integers$(s,e)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Answer：$A={d_s,\cdots,d_e}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;输出两个正整数，代表答案的范围。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_eg.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/blockquote&gt;
&lt;p&gt;输入问题和文章，问题和文章之间放[SEP] token，问题前放[CLS] token，整串内容输入BERT。&lt;/p&gt;
&lt;p&gt;文章的各个token输出的向量先和一个向量（橙）做内积，再对结果做Softmax，得到答案起始的位置。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;文章的各个token输出的向量再和一个向量（蓝）做内积，再对结果做Softmax，得到答案结束的位置。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;以上只有和BERT输出做内积的两个向量是随机初始化的，即这两个向量是重头开始学习的。&lt;/p&gt;
&lt;h3 id=&#34;bert-embryology胚胎学&#34;&gt;BERT Embryology（胚胎学）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.02480&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2010.02480] Pretrained Language Model Embryology: The Birth of ALBERT (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BERT的训练需要耗费大量的资源，BERT的训练资料大概是30亿个词，是哈利波特全集的3000倍。有没有什么方法去节省计算资源？&lt;/p&gt;
&lt;p&gt;从观察BERT的训练过程开始，BERT在什么时候学会填什么样的词汇？他的填空能力是怎么增进的？&lt;/p&gt;
&lt;h3 id=&#34;pre-training-a-seq2seq-model&#34;&gt;Pre-training a seq2seq model
&lt;/h3&gt;&lt;p&gt;BERT只有预训练的Encoder。&lt;/p&gt;
&lt;p&gt;Encoder和Decoder间通过Cross Atention连接起来，在Encoder的输入中故意加一些扰动，希望Decoder输出的句子和弄坏前的句子是一样的。&lt;/p&gt;
&lt;h4 id=&#34;mass--bart&#34;&gt;MASS / BART
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.02450&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.02450] MASS: Masked Sequence to Sequence Pre-training for Language Generation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.13461&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1910.13461] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对Encoder的输入加一些扰动来弄坏原本的内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;盖住一些词&lt;/li&gt;
&lt;li&gt;删掉一些词&lt;/li&gt;
&lt;li&gt;打乱词顺序&lt;/li&gt;
&lt;li&gt;词顺序旋转&lt;/li&gt;
&lt;li&gt;混合&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/MASS_BART.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;t5---comparison&#34;&gt;T5 - Comparison
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Transfer Text-to-Text Transformer（T5）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;T5在Colossal Clean Crawled Corpus（C4）进行训练，对比了多种弄坏的方法。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/T5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;why-does-bert-work&#34;&gt;Why does BERT work?
&lt;/h3&gt;&lt;p&gt;将文字输入到BERT中，得到的输出向量称为&lt;strong&gt;embedding&lt;/strong&gt;，代表了各个token的意思，有相似意思的token有着非常相似的embeddng。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/embedding.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在语言中常常有一词多义的情况，例如“吃苹果”的“果”和“苹果手机”的“果”的含义可能相差较大。通过Cosine Similarity计算“吃苹果”和“苹果手机”的Embedding的相似度，可以发现它们之间的相似度较低。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;You shall know a word by the company it keeps.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一个词的意思可以从上下文看出来，BERT在做“填空题”的过程中所学习的内容也许就是根据上下文来预测当前被盖住的词汇。事实上，BERT之前已经有这样的方法：word embedding，word embedding中的CBOW就是把中间挖空然后预测内容。BERT所抽取出来的向量也叫做&lt;strong&gt;Contextualized word embedding&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;现在尝试将BERT拿来做蛋白质分类、DNA分类、音乐分类。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.07162&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2103.07162] Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models&amp;rsquo; Transferability (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以DNA分类为例：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/DNA.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;将DNA中的序列替换成文字，输入到BERT中，输出分类，当作是文章分类的任务来处理。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/DNA_2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;类似地，对于蛋白质，随意给各个氨基酸映射到词汇上，对于音乐，将各个音符映射到词汇上，得到了下面的结果：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/Protein_DNA_music_res.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;BERT的表现是比较好的，就算给BERT乱七八糟的句子，它可能也能把任务完成的比较好，这也说明BERT的表现可能并不完全来自于它“看得懂”文章这件事，关于BERT到底为什么好的问题可能还有很大的研究空间。&lt;/p&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=1_gRK9EIQpc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DLHLP 2020] BERT and its family - Introduction and Fine-tune - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=Bywo7m6ySlk&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DLHLP 2020] BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-lingual-bert&#34;&gt;Multi-lingual BERT
&lt;/h3&gt;&lt;p&gt;在训练的时候，会拿各种各样的语言来给BERT做“填空题”，Multi-BERT使用了104种语言来训练。拿英文的QA资料去训练，Multi-BERT就会做中文的QA问题。&lt;/p&gt;
&lt;h4 id=&#34;zero-shot-reading-comprehension&#34;&gt;Zero-shot Reading Comprehension
&lt;/h4&gt;&lt;p&gt;在英文数据集SQuAD和中文数据集DRCD上：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/zero-shot.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1909.09587&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1909.09587] Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cross-lingual-alignment&#34;&gt;Cross-lingual Alignment
&lt;/h4&gt;&lt;p&gt;也许对Multi-lingual BERT来说，不同语言间没有什么差别。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/cross-lingual.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;mean-reciprocal-rankmrr&#34;&gt;Mean Reciprocal Rank（MRR）
&lt;/h4&gt;&lt;p&gt;MRR值越高，两个不同语言align的越好（同样意思但不同语言的词汇的向量比较接近）。&lt;/p&gt;
&lt;p&gt;在1000k资料量下，相比200k资料量下的效果显著提升：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/mrr1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;一个神奇的实验&#34;&gt;一个神奇的实验
&lt;/h3&gt;&lt;p&gt;BERT可以让同样意思但不同语言的词汇的向量很接近，但在训练Multi-lingual BERT时，还是给BERT喂中文，它能够做中文填空，喂英文能够做英文填空，不会混在一起，给他喂英文他并没有填中文进去。说明来自不同语言的符号终究还是不一样，并没有完全抹掉语言的资讯。&lt;/p&gt;
&lt;p&gt;把所有英文的embedding平均起来，再把所有中文的embedding平均起来，两者相减得到的向量就是中文和英文之间的差距。给Multi-lingual BERT一句英文，得到一串embedding，将embedding加上相减得到的向量，这些向量对于Multi-lingual BERT就变成了中文的句子，再让BERT去做“填空题”，就填出了中文的答案。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/wired.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;语言的资讯还是藏在Multi-lingual BERT中：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/unsupervised%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;gpt&#34;&gt;GPT
&lt;/h2&gt;&lt;h3 id=&#34;predict-next-token&#34;&gt;Predict Next Token
&lt;/h3&gt;&lt;p&gt;GPT修改了BERT中模型的任务，GPT的任务是预测接下来的句子是什么。&lt;/p&gt;
&lt;p&gt;对于训练资料“台湾大学”，在最前面加上[BOS] token，对于[BOS] token，GPT输出一个embedding，接下来用这个embedding预测下一个应该出现的“台”这个token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/predict%20next%20sentence.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;GPT与拿掉Cross attention后的Transformer Decoder结构类似。&lt;/p&gt;
&lt;p&gt;GPT要预测下一个token，有生成的能力，GPT最知名的例子就是用GPT写了一篇关于独角兽的假新闻。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://app.inferkit.com/demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Demo – InferKit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-gpt&#34;&gt;How to use GPT?
&lt;/h3&gt;&lt;p&gt;GPT有一个更“狂“的使用方式，和人类更接近。&lt;/p&gt;
&lt;h4 id=&#34;few-shot-learning&#34;&gt;“Few-shot” Learning
&lt;/h4&gt;&lt;p&gt;例如在进行外语考试时，首先会看题目的说明（&lt;em&gt;“&amp;hellip;从A、B、C、D四个选项中选出最佳选项&amp;hellip;”&lt;/em&gt;），再会看一个例子（&lt;em&gt;“&amp;hellip;衬衫的价格是9镑15便士，所以你选择&amp;hellip;”&lt;/em&gt;）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/few-shot%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;“Few-shot” Learning中完全没有Gradient Descent，GPT文献中将这种训练称为“In-context Learning”。&lt;/p&gt;
&lt;p&gt;类似地，还有“One-shot” Learning、甚至“Zero-shot” Learning。&lt;/p&gt;
&lt;h4 id=&#34;one-shot-learning&#34;&gt;“One-shot” Learning
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/one-shot%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;zero-shot-learning&#34;&gt;“Zero-shot” Learning
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/zero-shot%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;第三代GPT测试了42个任务：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/GPT_benchmarks.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more-1&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=DOG1L9lvsDY&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DLHLP 2020] 來自獵人暗黑大陸的模型 GPT-3 - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;beyond-text&#34;&gt;Beyond Text
&lt;/h2&gt;&lt;p&gt;不止NLP，在语音、图像上都可以用Self-Supervised Learning的技术。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/beyond%20text.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image---simclr&#34;&gt;Image - SimCLR
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.05709&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.05709] A Simple Framework for Contrastive Learning of Visual Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/google-research/simclr&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;google-research/simclr: SimCLRv2 - Big Self-Supervised Models are Strong Semi-Supervised Learners (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/SimCLR.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image---byol&#34;&gt;Image - BYOL
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.07733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.07733] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/BYOL.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;speech&#34;&gt;Speech
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/speech.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;speech-glue---superb&#34;&gt;Speech GLUE - SUPERB
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;S&lt;/strong&gt;peech processing &lt;strong&gt;U&lt;/strong&gt;niversal &lt;strong&gt;PER&lt;/strong&gt;formance &lt;strong&gt;B&lt;/strong&gt;enchmark，包含了十多个下游任务，包含内容、说话的人、情感、语义等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Toolkit：&lt;a class=&#34;link&#34; href=&#34;https://github.com/s3prl/s3prl/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;s3prl/s3prl: Self-Supervised Speech Pre-training and Representation Learning Toolkit. (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/toolkit.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>生成式对抗网络</title>
        <link>https://demo.stack.jimmycai.com/p/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Fri, 13 May 2022 17:45:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=58&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——生成式对抗网络&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;generative-adversarial-networks&#34;&gt;Generative Adversarial Networks
&lt;/h1&gt;&lt;h2 id=&#34;generator&#34;&gt;Generator
&lt;/h2&gt;&lt;h3 id=&#34;network-as-generator&#34;&gt;Network as Generator
&lt;/h3&gt;&lt;p&gt;输入$x$和一个简单的分布$z$（不固定，从一个分布中采样得到，每次使用网络时都会随机生成。），经过网络输出一个复杂的分布$y$。这样的网络称为&lt;strong&gt;Generator&lt;/strong&gt;。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/generator.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;why-distribution&#34;&gt;Why distribution？
&lt;/h3&gt;&lt;p&gt;当任务需要一些“创造力”时（同样的输入有多种可能的输出），需要预测分布。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;eg1. &lt;em&gt;Video Prediction&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;输入：吃豆人游戏的历史帧序列&lt;/p&gt;
&lt;p&gt;输出：吃豆人游戏新一帧的内容（吃豆人可能向不同的方向移动）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eg2. &lt;em&gt;Drawing&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;输入：“Character with red eyes”&lt;/p&gt;
&lt;p&gt;输出1：酷拉皮卡&lt;/p&gt;
&lt;p&gt;输出2：辉夜&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eg3. &lt;em&gt;Chatbot&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;输入：“你知道辉夜是谁吗？”&lt;/p&gt;
&lt;p&gt;输出1：“她是秀知院学生会&amp;hellip;”&lt;/p&gt;
&lt;p&gt;输出2：“她开创了忍者时代&amp;hellip;”&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generative-adversarial-networkgan&#34;&gt;Generative Adversarial Network（GAN）
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/hindupuravinash/the-gan-zoo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;hindupuravinash/the-gan-zoo: A list of all named GANs! (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anime-face-generation&#34;&gt;Anime Face Generation
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Unconditional generation&lt;/strong&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/unconditional.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/24767059&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GAN学习指南：从原理入门到制作生成Demo - 知乎 (zhihu.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discriminator&#34;&gt;Discriminator
&lt;/h3&gt;&lt;p&gt;Discriminator本身也是一个网络，Discriminator拿一张图片作为输入，输出一个数值。数字越大，表示图片越接近真实的图片。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/discriminator.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;basic-idea-of-gan&#34;&gt;Basic Idea of GAN
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;写作敌人，念做朋友。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;二者关系好比Generator造假钞，Discriminator是抓造假钞的警察，Generator越来越像，Discriminator的辨别能力越来越强。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/basic%20idea.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm
&lt;/h3&gt;&lt;p&gt;首先初始化generator $G$和discriminator $D$，在每一次训练中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step 1：定住$G$，更新$D$&lt;/p&gt;
&lt;p&gt;$D$学习去给真实二次元人物赋予更高的分数，而为生成的二次元人物赋予更低的分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/step1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;$D$分辨真正的二次元人物和生成的二次元人物之间的差异，可以当作一个分类或回归任务处理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 2：定住$D$，更新$G$&lt;/p&gt;
&lt;p&gt;$G$学习去“骗过”$D$，经过调整后，使得生成的图片能在$D$中产生更高的分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/step2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;theory-behind-gan&#34;&gt;Theory behind GAN
&lt;/h2&gt;&lt;h3 id=&#34;objective&#34;&gt;Objective
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/objective.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
G^\ast=\arg\min_G Div(P_G,P_{data})
$$&lt;p&gt;其中$Div(P_G,P_{data})$是$P_G$与$P_{data}$之间的散度（Divergence），可以看作是两个分布之间某种距离，散度越大，代表两个分布越不像，散度越小，代表两个分布越相近。c.f. $w^\ast,b^\ast=\arg\min_{w,b}L$&lt;/p&gt;
&lt;h3 id=&#34;sampling&#34;&gt;Sampling
&lt;/h3&gt;&lt;p&gt;虽然不清楚$P_G$和$P_{data}$的分布，但是可以从其中采样来计算散度。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/sampling.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;discriminator-1&#34;&gt;Discriminator
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2661&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1406.2661] Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discriminator的训练目标是看到真实数据就给出一个高的分数，看到生成数据就给出一个低的分数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;：$D^\ast=\arg\max_DV(D,G)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective Function&lt;/strong&gt; for $D$：$V(G,D)=E_{y\sim P_{data}}[\log D(y)]+E_{y\sim P_G}[\log(1-D(y))]$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$V(D,G)$是交叉熵的相反数，Discriminator可以等同于一个分类器，最小化交叉熵。而正好$\max_DV(D,G)$就和&lt;strong&gt;JS散度&lt;/strong&gt;有关。&lt;/p&gt;
&lt;p&gt;最开始，$P_G$和$P_{data}$混在一起，散度很小，Discriminator难以分辨哪些数据是生成数据而哪些数据是真实数据，即Discriminator难以区分小的$max_D{V(D,G)}$。&lt;/p&gt;
&lt;p&gt;若$P_G$和$P_{data}$散度很大，$max_DV(D,G)$比较大，DIscriminator则很容易区分生成数据和真实数据。&lt;/p&gt;
$$
G^\ast=\arg\min_G\max_DV(G,D)
$$&lt;h3 id=&#34;why-js-divergence&#34;&gt;Why JS Divergence？
&lt;/h3&gt;&lt;p&gt;除了JS散度，也可以使用例如KL散度等其他散度。如何设计目标函数，得到不同的散度，在f-GAN论文中有详细的证明。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1606.00709&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1606.00709] f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tips-for-gan&#34;&gt;Tips for GAN
&lt;/h2&gt;&lt;h3 id=&#34;js散度的问题&#34;&gt;JS散度的问题
&lt;/h3&gt;&lt;p&gt;在大多数情况下，$P_G$和$P_{data}$重复的部分非常少。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据本身的特性：数据是高维空间中的低维流形，重叠的部分可以忽略。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;流形学习的观点认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。例如单位圆上有无穷多个点，无法用二唯坐标系上的点来表示圆上所有的点，而若使用极坐标，圆心在原点的圆只需一个参数——半径，就可以确定。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/manifold.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;采样：即使$P_G$和$P_{data}$有重叠，若采样的点不够多，对Discriminator来说也是没有重叠的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而以上的问题会导致JS散度出现问题。&lt;/p&gt;
&lt;p&gt;在两个分布完全不重叠时，无论两个分布的中心距离有多近，其JS散度都是一个常数$\log2$，无法判断哪个case更好，从而无法更新参数。&lt;/p&gt;
&lt;p&gt;参考证明：&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/MorStar/p/14882813.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;JS散度(Jensen–Shannon divergence) - MorStar - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/js%20div%20problem.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;直观来看，如果两个分布不重叠，二分类的准确率几乎可以达到100%。&lt;/p&gt;
&lt;h3 id=&#34;wasserstein-distance&#34;&gt;Wasserstein distance
&lt;/h3&gt;&lt;p&gt;考虑两个分布$P$和$Q$，想象一台推土机，$P$是一堆土，$Q$是要堆放的目的地，把$P$挪动到$Q$的平均距离就是Wasserstein distance。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;考虑更复杂的情况，移动的方案可能有无穷多种。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;穷举所有的移动方法，看哪一个移动方法可以让平均的距离最小，最小的值就是Wasserstein distance。但计算方法似乎比较复杂，要计算距离还需要求解这样一个最优化问题。&lt;/p&gt;
&lt;p&gt;假设现在我们已经可以计算Wasserstein distance，就可以解决JS散度带来的问题。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;wgan&#34;&gt;WGAN
&lt;/h3&gt;&lt;p&gt;WGAN使用Wasserstein distance取代JS散度。&lt;/p&gt;
$$
\max_{D\in 1-Lipschitz}\{E_{y\sim P_{data}}[D(y)]-E_{y\sim P_G}[D(y)]\}
$$&lt;p&gt;
$D\in 1-Lipschitz$：$D$需要是一个足够平滑的函数，不能是变动很剧烈的函数，若没有这个限制，单纯让生成数据越小越好，真实数据越大越好，在生成数据和真实数据没有重叠的情况下，会给真实数据无穷大的正值而给生成数据无穷小的负值。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;din-1-lipschitz&#34;&gt;$D\in 1-Lipschitz$
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Original WGAN：Weight&lt;/p&gt;
&lt;p&gt;强制参数$w$在$c$和$-c$之间，参数更新后若$w\gt c$，则$w=c$，若$w\lt -c$，则$w=-c$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Improved WGAN：Gradient Penalty&lt;/p&gt;
&lt;p&gt;在真实数据和生成数据中各取一个样本，两点连线中再取一个样本，要求这个点的梯度接近1。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/gradient%20penalty.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.00028&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.00028] Improved Training of Wasserstein GANs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spectral Normalization（SNGAN）：让梯度模长在任何地方都小于1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1802.05957&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1802.05957] Spectral Normalization for Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;more-tips&#34;&gt;More Tips
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Tops from Soumith
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/soumith/ganhacks&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;soumith/ganhacks: starter from &amp;ldquo;How to Train a GAN?&amp;rdquo; at NIPS2016 (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tips in DCGAN：Guideline for network architecture design for image generation
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1511.06434&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1511.06434] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Improved techniques for training GANs
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1606.03498&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1606.03498] Improved Techniques for Training GANs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tips from BigGAN
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1809.11096&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1809.11096] Large Scale GAN Training for High Fidelity Natural Image Synthesis (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GAN的训练需要Generator和Discriminator共同配合，若有其中一方不再进步，另一方也会停下来，&lt;em&gt;Generator和Discriminator需要棋逢敌手&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;gan-for-sequence-generation&#34;&gt;GAN for Sequence Generation
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/GAN%20seq.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Decoder参数改变后，经过max输出的文字可能不会发生改变，就无法完成参数更新。可以用RL来训练。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.09922&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.09922] Training language GANs from Scratch (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/scratch.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;more-generative-models&#34;&gt;More Generative Models
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/playlist?list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GAN（Full version）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/8zomhgKrsmQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Variational Autoencoder（VAE）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/uXY18nzdSsM&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;FLOW-based Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation-of-generation&#34;&gt;Evaluation of Generation
&lt;/h2&gt;&lt;h3 id=&#34;quality-of-image&#34;&gt;Quality of Image
&lt;/h3&gt;&lt;p&gt;评价图像质量最直接的做法是找人来看，在Generator研究初期，有人会选几张图说“看，这个结果应该比目前的结果都要好，应该是SOTA。”，这显然不够客观，如何自动地评价生成图像的质量？&lt;/p&gt;
&lt;p&gt;一个方法是使用图像分类器，输入一张图片$y$，输出图片属于各个类的概率分布$p(c|y)$，分布越集中，产生的图片可能就越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/quality%20of%20img.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;diversity---mode-collapse&#34;&gt;Diversity - Mode Collapse
&lt;/h3&gt;&lt;p&gt;仅使用以上这种方法评估图像质量时可能会遇到Mode Collapse（模式坍塌）的问题。&lt;/p&gt;
&lt;p&gt;训练GAN的过程可能会遇到以下问题：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20collapse.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Generator生成出来的图片可能来来去去都是那几张：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20collapse2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;直觉上看，这样的点是Discriminator的“盲点”，Discriminator没办法看出这样的图片是假的，当Generator学会产生这种图片后，就永远都可以骗过Discriminator。&lt;/p&gt;
&lt;h3 id=&#34;diversity---mode-dropping&#34;&gt;Diversity - Mode Dropping
&lt;/h3&gt;&lt;p&gt;Mode Dropping可能比Mode Collapse更难侦测出来，产生出来的数据可能只能贴近已有真实数据的分布，但真实的数据分布的多样性其实是更大的。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20dropping.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一个人脸生成的例子：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20dropping2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;diversity&#34;&gt;Diversity
&lt;/h3&gt;&lt;p&gt;过去判定多样性的做法是将一批图片输入到图片分类器中，计算所有图片的概率分布的均值，若平均的分布非常集中，则代表多样性不够。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/diversity.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;若输入这批图片产生的分布都非常不同，平均后的结果非常平坦，则代表多样性是足够的。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/diversity2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Diversity和Quality的评估方式相反，Diversity看的是一批图片的平均，而Quality看的是一张图片。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inception Score（IS）&lt;/strong&gt;：质量越高，多样性越大，则IS越高。&lt;/p&gt;
&lt;h3 id=&#34;fréchet-inception-distancefid&#34;&gt;Fréchet Inception Distance（FID）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.08500&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1706.08500] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;取Softmax前的输出向量，假设真实图像和生成图像都是高斯分布，计算这两个高斯分布之间的Fréchet distance，这个距离越小说明真实图像与生成图像越接近，生成图像品质越高。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/FID.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可能需要大量的图片样本才能做到。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1711.10337&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1711.10337] Are GANs Created Equal? A Large-Scale Study (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;we-dont-want-memory-gan&#34;&gt;We don&amp;rsquo;t want memory GAN
&lt;/h3&gt;&lt;p&gt;生成出来的图片有可能和训练集一模一样，这种情况下FID非常小，也有可能仅仅把图片翻转，这样很难侦测出来。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1511.01844&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1511.01844] A note on the evaluation of generative models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;more-about-evaluation&#34;&gt;More about evaluation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1802.03446&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1802.03446] Pros and Cons of GAN Evaluation Measures (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conditional-generation&#34;&gt;Conditional Generation
&lt;/h2&gt;&lt;p&gt;前面所提到的Unconditional GAN的输入都是一个随机的分布。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/conditional%20generation.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;例如要做文本转图片，需要给模型一个文本输入$x$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/text-to-img.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;conditional-gan&#34;&gt;Conditional GAN
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/conditional%20GAN.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在Unconditional GAN中，Discriminator接受一个图片$y$作为输入，输出一个数值，代表图片是真实的或是生成的，但这样的方法无法解Conditional GAN的问题，Generator可以产生非常接近真实的图片，但是忽略了输入的条件。&lt;/p&gt;
&lt;p&gt;Conditional GAN中，需要成对的训练数据。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/conditional%20GAN2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Conditional GAN也可以用图像来生成图像，例如图像去雾，黑白转彩色，白天转黑夜，素描转实物。也叫&lt;strong&gt;Image translation&lt;/strong&gt;或&lt;strong&gt;pix2pix&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;通常可以将GAN和有监督学习结合，得到更好的结果。&lt;/p&gt;
&lt;h3 id=&#34;其他应用&#34;&gt;其他应用
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Sound-to-image&lt;/li&gt;
&lt;li&gt;Talking Head Generation
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.08233&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.08233] Few-Shot Adversarial Learning of Realistic Neural Talking Head Models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learning-from-unpaired-data&#34;&gt;Learning from Unpaired Data
&lt;/h2&gt;&lt;p&gt;有一堆$x$和一堆$y$，但$x$和$y$不成对（未标注）。pseudo labeling（伪标签）和back translation（反向翻译）都需要一些成对的数据。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/unpaired%20data.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;例如在影像风格转换，将定义域$\mathcal{X}$真人头像转为定义域$\mathcal{Y}$二次元头像：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/image%20style%20transfer.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;cycle-gan&#34;&gt;Cycle GAN
&lt;/h3&gt;&lt;p&gt;输入一个Domain $\mathcal{X}$，输出Domain $\mathcal{Y}$。但如果仍然按照之前的方法学习，GAN无法判断生成的二次元图像是否与输入的真人图像是相似的，可能会将输入当作为高斯噪声，忽略输入的内容。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/cycle%20gan.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Cycle GAN中会训练两个Generator，第一个Generator $G_{\mathcal{X}\rightarrow \mathcal{Y}}$将$\mathcal{X}$ domain的图变成$\mathcal{Y}$ domain的图，第二个Generator $G_{\mathcal{Y}\rightarrow \mathcal{X}}$将$\mathcal{Y}$ domain的图还原为$\mathcal{X}$ domain的图。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/cycle%20gan2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Cycle GAN能保证真实图片和生成图片有一些关系，但如何保证这种关系是我们想要的呢？例如输入一个戴眼镜的人，$G_{\mathcal{X}\rightarrow \mathcal{Y}}$将眼镜转成痣，但$G_{\mathcal{Y}\rightarrow \mathcal{X}}$又会把痣转成眼镜。理论上可能会出现这样的情况，不过在实际中这种情况往往不会出现。&lt;/p&gt;
&lt;p&gt;类似地还有Disco GAN和Dual GAN，思想与Cycle GAN基本相同。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1703.05192&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1703.05192] Learning to Discover Cross-Domain Relations with Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.02510&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.02510] DualGAN: Unsupervised Dual Learning for Image-to-Image Translation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外还有能够在多种风格之间转换的Star GAN：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.02510&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.02510] DualGAN: Unsupervised Dual Learning for Image-to-Image Translation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/starGAN.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;selfie2anime&#34;&gt;SELFIE2ANIME
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://selfie2anime.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Selfie2Anime&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1907.10830&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1907.10830] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;text-style-transfer&#34;&gt;Text Style Transfer
&lt;/h3&gt;&lt;p&gt;文字风格转换，例如将负面的句子转为正面的句子。和Cycle GAN的做法类似。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/text%20style%20transfer.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Unsupervised Abstractive Summarization
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.02851&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1810.02851] Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised Translation
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1710.04087&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1710.04087] Word Translation Without Parallel Data (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1710.11041&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1710.11041] Unsupervised Neural Machine Translation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised ASR
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1804.00316&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1804.00316] Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1812.09323&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1812.09323] Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.04100&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.04100] Completely Unsupervised Speech Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>各式各样的自注意力机制</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
        <pubDate>Sat, 30 Apr 2022 19:29:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=51&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——各式各样的神奇的自注意力&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-attention变型&#34;&gt;Self-attention变型
&lt;/h1&gt;&lt;p&gt;Sequence length=$N$，产生的$N$个key向量和$N$个query向量两两之间做Dot-product，共$N^2$平方次计算，得到一个$N\times N$的矩阵Attention Matrix，根据该矩阵对value向量加权求和。Self-attention往往是模型里面的一个小模块，当$N$很大时，模型的主要计算量都集中在Self-attention上，对于计算速度的优化往往都是用在影像上。&lt;/p&gt;
&lt;h2 id=&#34;human-knowledge&#34;&gt;Human knowledge
&lt;/h2&gt;&lt;h3 id=&#34;local-attention--truncated-attention&#34;&gt;Local Attention / Truncated Attention
&lt;/h3&gt;&lt;p&gt;某些问题不用看完整的序列，只用看左右邻居的信息即可，将其他位置的信息设为0。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/local%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可以加快运算速度，但每次做Attention只能看到某个小范围的信息，和CNN的差别就不大了。&lt;/p&gt;
&lt;h3 id=&#34;stride-attention&#34;&gt;Stride Attention
&lt;/h3&gt;&lt;p&gt;与Local Attention类似，看更远的邻居，例如看三个位置之前和三个位置之后的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/stride%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;global-attention&#34;&gt;Global Attention
&lt;/h3&gt;&lt;p&gt;在原始序列中加入一些特殊的token，代表该位置要做Global Attention，Global Attention会从序列中的每一个token去收集信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attend to every token -&amp;gt; 收集所有的信息&lt;/li&gt;
&lt;li&gt;Attended by every token -&amp;gt; 能获取全局的信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Global Attention有两种做法，可以从原始序列中选择一些已有的字符（例如BERT中的[CLS]标志、句号等）作为token，或外加额外的token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/global%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;papers&#34;&gt;Papers
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2004.05150&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2004.05150] Longformer: The Long-Document Transformer (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2007.14062&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2007.14062] Big Bird: Transformers for Longer Sequences (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;clustering&#34;&gt;Clustering
&lt;/h2&gt;&lt;p&gt;在Attention矩阵，可能有些值很大，有些值特别小，可以直接把较小的权值置0，问题在于如何快速估计哪些地方的Attention值较高，而哪些地方的Attention值较低。&lt;/p&gt;
&lt;h3 id=&#34;reformer&#34;&gt;Reformer
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=rkgNKkHtvB&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Reformer: The Efficient Transformer | OpenReview&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.05997&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.05997] Efficient Content-Based Sparse Attention with Routing Transformers (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;步骤&#34;&gt;步骤
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step 1：对query和key做聚类&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/clustering1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;聚类有很多可以加速的方法，对query和key做聚类时，会采取精度相对较低但速度很快的方法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 2 对同一Cluster的query和key计算Attention分数&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/clustering2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;不属于同一类的直接将Attention值设为0。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learnable-patterns&#34;&gt;Learnable Patterns
&lt;/h2&gt;&lt;h3 id=&#34;sinkhorn-sorting-network&#34;&gt;Sinkhorn Sorting Network
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.11296&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.11296] Sparse Sinkhorn Attention (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;让机器去学习两个位置的向量要不要做Attention。&lt;/p&gt;
&lt;p&gt;Sinkhorn Sorting Network里面有一个额外需要学习的矩阵，来决定哪些地方需要计算Attention。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/sinkhorn%20sorting%20network.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;多个向量会共享一个矩阵以加快计算速度。（例如对于长度为100的输入，会分成10组，每组都是同一个矩阵。）&lt;/p&gt;
&lt;h2 id=&#34;representative-key&#34;&gt;Representative key
&lt;/h2&gt;&lt;p&gt;Attention矩阵中有很多冗余列，往往无需$N\times N$的Attention矩阵。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;linformer&#34;&gt;Linformer
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.04768&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.04768] Linformer: Self-Attention with Linear Complexity (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从$N$个key中选出最有代表性的$K$个key，只需算$N\times K$的Attention矩阵。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;reduce-number-of-keys&#34;&gt;Reduce Number of Keys
&lt;/h4&gt;$$
M_{d\times N}\times M_{N\times K}=M_{d\times K}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer3.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;Compressed Attention&lt;/strong&gt;中的处理方式是对输入的较长序列用CNN去处理，得到一个较长的序列。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/compressed%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1801.10198&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1801.10198] Generating Wikipedia by Summarizing Long Sequences (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kq-first-rightarrow-vk-first&#34;&gt;$k,q$ first $\rightarrow$ $v,k$ first
&lt;/h2&gt;&lt;h3 id=&#34;忽略softmax的情况&#34;&gt;忽略Softmax的情况
&lt;/h3&gt;$$
O\approx VK^TQ
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
$$
O\approx V[K^TQ]\rightarrow O\approx [VK^T]Q
$$&lt;ul&gt;
&lt;li&gt;对于计算方法$O\approx V[K^TQ]$：
&lt;ul&gt;
&lt;li&gt;$A=K^TQ$：$N\times d\times N$&lt;/li&gt;
&lt;li&gt;$O=VA$：$d&amp;rsquo;\times N\times N$&lt;/li&gt;
&lt;li&gt;求和：$(d+d&amp;rsquo;)N^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;而对于计算方法$O\approx [VK^T]Q$：
&lt;ul&gt;
&lt;li&gt;$M_1=VK^T$：$d&amp;rsquo;\times N\times d$&lt;/li&gt;
&lt;li&gt;$M_2=M_1Q$：$d&amp;rsquo;\times d\times N$&lt;/li&gt;
&lt;li&gt;求和：$2d&amp;rsquo;dN$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;加回softmax&#34;&gt;加回Softmax
&lt;/h3&gt;&lt;p&gt;已知存在一个$\phi$，使得有&lt;a class=&#34;link&#34; href=&#34;#%e5%ae%9e%e7%8e%b0&#34; &gt;以下式子&lt;/a&gt;成立：&lt;/p&gt;
&lt;blockquote&gt;
$$
&gt; \exp(q\cdot k)\approx \phi(q)\cdot\phi(k)
&gt; $$&lt;/blockquote&gt;
$$
\begin{aligned}
b^1=\sum_{i=1}^Na&#39;_{1,i}v^i&amp;=\sum_{i=1}^N\frac{\exp{(q^1\cdot k^i)}}{\sum_{j=1}^{N}\exp{(q^1\cdot k^j)}}v^i \\
&amp;=\sum_{i=1}^N\frac{\phi(q^1)\cdot\phi(k^i)}{\sum_{j=1}^{N}\phi(q^1)\cdot\phi(k^j)}v^i \\
&amp;=\frac{\sum_{i=1}^{N}[\phi(q^1)\cdot\phi(k^i)]v^i}{\sum_{j=1}^{N}\phi(q^1)\cdot\phi(k^j)}
\end{aligned}
$$$$
\sum_{j=1}^{N}\phi(q^1)\cdot\phi(k^j)=\phi(q^1)\cdot\sum_{j=1}^{N}\phi(k^j)
$$$$
\phi(q^1)=
\begin{bmatrix}
q_1^1 \\
q_2^1 \\
\vdots
\end{bmatrix}
\quad
\phi(k^1)=
\begin{bmatrix}
k_1^1 \\
k_2^1 \\
\vdots
\end{bmatrix}
$$&lt;p&gt;
则有：
$$
\begin{aligned}
&amp;amp;\quad\sum_{i=1}^N[\phi(q^1)\cdot \phi(k^i)]v^i \
&amp;amp;=[\phi(q^1)\cdot\phi(k^1)]v^1+[\phi(q^1)\cdot\phi(k^2)]v^2+\cdots \
&amp;amp;=(q_1^1k_1^1+q_2^1k_2^1+\cdots)v^1+(q_1^1k_1^2+q_2^1k_2^2+\cdots)v^2+\cdots \
&amp;amp;=(q_1^1k_1^1v^1+q_2^1k_2^1v^1+\cdots)+(q_1^1k_1^2v^2+q_2^1k_2^2v^2+\cdots)+\cdots \
&amp;amp;=q_1^1(k_1^1v^1+k_1^2v^2+\cdots)+q_2^1(k_2^1v^1+k_2^2v^2+\cdots)+\cdots \&lt;/p&gt;
&lt;p&gt;\end{aligned}
$$
设$\phi(q^1)$的维度为$M$，则：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;即在分子中的$M$个向量中，每一个向量都是通过，拿出$\phi(k^1)$、$\phi(k^2)$、&amp;hellip;、$\phi(k^N)$中的第$i$个分量，对$v^1$、$v^2$、&amp;hellip;、$v^N$做加权和。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可以看出，每次计算$b^i$时，除了$\phi(q^i)$以外，其他部分没有发生变化，这部分内容无需&lt;strong&gt;再重复计算&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;self-attention中的qkv&#34;&gt;Self-attention中的$q$、$k$、$v$
&lt;/h3&gt;&lt;p&gt;计算$b^1$：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;产生的$M$个向量以及$\sum_{j=1}^{N}\phi(k^j)$在后面$b^2$、$b^3$、$b^4$的计算中无需再进行计算。&lt;/p&gt;
&lt;h3 id=&#34;实现&#34;&gt;实现
&lt;/h3&gt;&lt;p&gt;关于$\phi$的实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1812.01243&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1812.01243] Efficient Attention: Attention with Linear Complexities (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://linear-transformers.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linear Transformers (linear-transformers.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.02143&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2103.02143] Random Feature Attention (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2009.14794&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2009.14794] Rethinking Attention with Performers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;new-framework&#34;&gt;New framework
&lt;/h2&gt;&lt;h3 id=&#34;无需qk产生attentionsynthesizer&#34;&gt;无需$q,k$产生Attention——Synthesizer
&lt;/h3&gt;$$
\begin{bmatrix}
\alpha_{1,1} &amp; \alpha_{1,2} &amp; \alpha_{1,3} &amp; \alpha_{1,4} \\
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{2,3} &amp; \alpha_{2,4} \\
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp; \alpha_{3,4} \\
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp; \alpha_{4,4} \\
\end{bmatrix}
$$&lt;h2 id=&#34;attention-free&#34;&gt;Attention-free
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.03824&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.03824] FNet: Mixing Tokens with Fourier Transforms (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.08050&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.08050] Pay Attention to MLPs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.01601&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.01601] MLP-Mixer: An all-MLP Architecture for Vision (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/summary.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>Attention Is All You Need</title>
        <link>https://demo.stack.jimmycai.com/p/attention-is-all-you-need/</link>
        <pubDate>Mon, 25 Apr 2022 23:01:32 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/attention-is-all-you-need/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=49&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——Transformer&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;transformer&#34;&gt;Transformer
&lt;/h1&gt;&lt;p&gt;Transformer是一个Sequence-to-sequence（Seq2seq）的模型，输出的长度由模型自己来决定。&lt;/p&gt;
&lt;h2 id=&#34;sequence-to-sequence&#34;&gt;Sequence-to-sequence
&lt;/h2&gt;&lt;h3 id=&#34;应用&#34;&gt;应用
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Maching Translation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speech Translation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Text-to-Speech（TTS）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;questions-answeringqa&#34;&gt;Questions Answering（QA）
&lt;/h4&gt;$$
\text{question}, \text{context} \stackrel{Seq2seq}{\longrightarrow} \text{answer}
$$&lt;h4 id=&#34;multi-label-classification&#34;&gt;Multi-label Classification
&lt;/h4&gt;$$
\text{data} \stackrel{Seq2seq}{\longrightarrow} \text{class 7, class 9, class 13}
$$&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1909.03434&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1909.03434] Order-free Learning Alleviating Exposure Bias in Multi-label Classification (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1707.05495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1707.05495] Order-Free RNN with Visual Attention for Multi-Label Classification (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;object-detection&#34;&gt;Object Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.12872&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2005.12872] End-to-End Object Detection with Transformers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;结构&#34;&gt;结构
&lt;/h3&gt;$$
\text{Encoder}\longrightarrow \text{Decoder}
$$&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1409.3215&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1409.3215] Sequence to Sequence Learning with Neural Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.03762&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1706.03762] Attention Is All You Need (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;encoder&#34;&gt;Encoder
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;输入一排向量：${x^1,x^2,x^3,x^4}$&lt;/li&gt;
&lt;li&gt;输出一排向量：${h^1,h^2,h^3,h^4}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Self-attention、RNN、CNN&amp;hellip;均可用来作为Encoder。&lt;/p&gt;
&lt;h3 id=&#34;transformer-encoder&#34;&gt;Transformer Encoder
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/seq2seq.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在Transformer Encoder中，加入了Residual Connection，经过Self-attention输出的向量加上原输入的向量后当作新的输出向量。&lt;/p&gt;
&lt;p&gt;得到Residual的结果以后，进行Normalization，但此处使用的是Layer Normalization而非Batch Normalization。对于输入的向量，Layer Norm会计算它的均值$m$和标准差$\sigma$，与Batch Norm不同点在于，Batch Norm是对不同特征、样本的同一个维度计算均值和标准差，而Layer Norm是对同一个特征、样本的不同维度去计算均值和标准差。&lt;/p&gt;
&lt;p&gt;Layer Norm的结果将作为FC的输入，经过FC Network得到新的向量，在FC层也同样地加入了Residual Connection，得到的结果再做一次Layer Norm，则得到了此Block的输出。&lt;/p&gt;
&lt;p&gt;即一个Block的结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/encoder.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;此Block会重复N次，组成Transformer的Encoder，BERT与Transformer Encoder采用了相同的结构。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/encoder2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.04745&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.04745] On Layer Normalization in the Transformer Architecture (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.07845&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.07845] PowerNorm: Rethinking Batch Normalization in Transformers (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;decoderautoregressiveat&#34;&gt;Decoder——Autoregressive（AT）
&lt;/h2&gt;&lt;h3 id=&#34;decoder的运作方式&#34;&gt;Decoder的运作方式
&lt;/h3&gt;&lt;p&gt;除了Encoder产生的输出以外，Decoder中还会加入一个BOS（Begin of Sequence）符号（token），用来表示开始，BOS token是一个One-hot表示的向量。&lt;/p&gt;
&lt;p&gt;输出一个向量，向量的长度应该和Vocabulary Size相等来表示所有的汉字（对于中文，Vocabulary Size就是所有汉字的数量），每一个中文对应向量中的一个数值，这个向量是经过Soft-max得到的，取最大的作为输出的文字，Decoder会将这个输出的文字的One-hot向量作为新的输入。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/decoder.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;但是按照这样的运作方式，后面会产生无穷尽的文字，像文字接龙一样一直不能停下来，所以，还应该加一个EOS（End of Sequence） token来表示文字的结束，一般情况下，EOS token和BOS token都用一个相同的向量来表示，故向量的长度应该为Vocabulary Size + 1。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/decoder2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在这种运作方式下，某步的错误预测可能影响后面的预测（“一步错，步步错。”），具体参考最后一节&lt;a class=&#34;link&#34; href=&#34;#Scheduled-Sampling&#34; &gt;Scheduled Sampling&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;transformer-decoder&#34;&gt;Transformer Decoder
&lt;/h3&gt;&lt;p&gt;Transformer中的Decoder和Encoder结构类似，除中间的Multi-Head Attention和Add &amp;amp; Norm结构外，在第一次Multi-Head Attention计算中加入了Mask。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/encodervsdecoder.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;masked-self-attention&#34;&gt;Masked Self-attention
&lt;/h4&gt;&lt;p&gt;在产生$b^1$的时候，只能考虑$a^1$的信息，而不能考虑$a^2$、$a^3$、$a^4$的信息；在产生$b^2$的时候，只能考虑$a^1$、$a^2$的信息，而不能考虑$a^3$、$a^4$的信息。&lt;/p&gt;
&lt;p&gt;具体来说，在产生$b^2$时，只会拿第二个位置的query去跟第一个位置的key和第二个位置的key来计算Attention，而不管第三、四个位置的key。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/mask.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;对于Decoder而言，先有$a^1$才有$a^2$，才有接下来的$a^3$、$a^4$，计算$b^2$的时候无法考虑$a^3$、$a^4$。&lt;/p&gt;
&lt;h2 id=&#34;decodernon-autoregressivenat&#34;&gt;Decoder——Non-autoregressive（NAT）
&lt;/h2&gt;&lt;h3 id=&#34;at-vs-nat&#34;&gt;AT v.s. NAT
&lt;/h3&gt;&lt;p&gt;AT分别输入BOS token、$w_1$、$w_2$、$w_3$、EOS token，而NAT一次输入一整排BOS token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/ATvsNAT.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;并行&lt;/li&gt;
&lt;li&gt;输出长度可控&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;问题：模型如何知道输出的长度，从而确定输入的BOS token的数量？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用一个模型来预测输出长度；&lt;/li&gt;
&lt;li&gt;输出一个很长的句子，忽略EOS token以后的内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NAT的表现往往比AT要差（Multi-modality）。&lt;/p&gt;
&lt;h2 id=&#34;encoder-decoder&#34;&gt;Encoder-Decoder
&lt;/h2&gt;&lt;p&gt;Transformer中由&lt;strong&gt;Cross Attention&lt;/strong&gt;模块来连接Encoder和Decoder。该模块接收Encoder的两个输出和Decoder的一个输出作为输入。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/cross%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;将BOS token输入到Decoder的Masked Self-attention模块后，将输出的向量进行线性变换得到query $q$，再将$q$与Encoder中的key $k^1$、$k^2$、$k^3$计算Attention分数，与value $v^1$、$v^2$、$v^3$相乘加权求和得到$v$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/cross%20attention2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Cross Attention比Self-Attention出现要更早。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/7472621&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Listen, attend and spell: A neural network for large vocabulary conversational speech recognition | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transformer中，Decoder中每一层都与Encoder的最后一层做Cross Attention，也有论文的工作中尝试了与其他层的不同的连接方式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.08081&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2005.08081] Layer-Wise Multi-View Decoding for Improved Natural Language Generation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training&#34;&gt;Training
&lt;/h2&gt;&lt;p&gt;以下以一段标签为“机器学习”的语音数据为例。&lt;/p&gt;
&lt;p&gt;在Decoder中输入BOS token后，输出的向量应该和“机”对应的向量越接近越好，即通过计算两个向量的交叉熵，交叉熵越小越好，这个过程和分类非常相似。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/training.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;包括最后一个EOS token，模型希望最后一个字所输出的内容与EOS token的One-hot向量是接近的。&lt;/p&gt;
&lt;p&gt;在训练过程中，Decoder的输入是真实标签，训练过程中会给Decoder看正确答案，即给Decoder输入BOS token和“机”以后，希望模型的输出是“器”，给Decoder输入BOS token、“机”和“器”之后，希望模型输出的是“学”。这种方法叫做&lt;strong&gt;Teacher Forcing&lt;/strong&gt;，将正确答案作为输入。&lt;/p&gt;
&lt;p&gt;训练时Decoder可以看到完全正确的信息，而测试的时候Decoder可能会看到一些错误的信息，可能会导致“一步错，步步错。”，训练与测试不一致的现象叫做&lt;strong&gt;exposure bias&lt;/strong&gt;，方法是在学习时，给Decoder的输入加入一些错误的信息，具体参考最后一节&lt;a class=&#34;link&#34; href=&#34;#Scheduled-Sampling&#34; &gt;Scheduled Sampling&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;tips&#34;&gt;Tips
&lt;/h2&gt;&lt;h3 id=&#34;copy-mechanism&#34;&gt;Copy Mechanism
&lt;/h3&gt;&lt;p&gt;某些信息并不需要机器来学习，可能是从输入信息中复制出来，例如聊天机器人中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;eg1&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;User：你好，我是&lt;em&gt;库洛洛&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;Machine：&lt;em&gt;库洛洛&lt;/em&gt;你好，很高兴认识你。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eg2&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;User：小杰&lt;em&gt;不能使用念能力了&lt;/em&gt;！&lt;/p&gt;
&lt;p&gt;Machine：你所谓的*「不能使用念能力」*是什么意思？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;又例如从文章中提取摘要这一任务，从文章中复制一些信息是很模型很关键的能力。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.04368&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.04368] Get To The Point: Summarization with Pointer-Generator Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1603.06393&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1603.06393] Incorporating Copying Mechanism in Sequence-to-Sequence Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;guided-attention&#34;&gt;Guided Attention
&lt;/h3&gt;&lt;p&gt;在一些任务中（例如语音辨识、TTS等），对于输入的每一个内容都要看到，不能漏掉某些信息。&lt;/p&gt;
&lt;p&gt;Guided Attention要求机器以特定的方式完成Attention的计算，应该由左向右分别产生输出。例如在TTS中，应该先看最左边的文字产生输出，最后看最右的文字产生输出。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monotonic Attention Location-aware attention&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;beam-search&#34;&gt;Beam Search
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/beam%20search.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;要找到最优解，暴力搜索难以计算。通过Beam Search找一个不是完全精准的解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.09751&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.09751] The Curious Case of Neural Text Degeneration (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设一个任务的答案非常明确，Beam Search会比较有帮助，但对于一些答案不唯一的任务（例如文本补全），分数最高的路径可能结果并不是很好，往往需要在Decoder中加入随机性（noise）。&lt;/p&gt;
&lt;h2 id=&#34;optimizing-evaluation-metrics&#34;&gt;Optimizing Evaluation Metrics
&lt;/h2&gt;&lt;p&gt;训练时使用交叉熵，在评估时使用BLEU。BLEU不可微分，无法作为Loss。不过对于无法优化的Loss，可以将其当作Reinforcement Learning（RL）的reward，Decoder作为Agent，将其看作是RL问题来解决。&lt;/p&gt;
&lt;h2 id=&#34;scheduled-sampling&#34;&gt;Scheduled Sampling
&lt;/h2&gt;&lt;p&gt;Schedule可能会影响计算的并行化，对于Transformer的Scheduled Sampling另有方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1506.03099&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1506.03099] Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.07651&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1906.07651] Scheduled Sampling for Transformers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.04331&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1906.04331] Parallel Scheduled Sampling (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>自注意力机制</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
        <pubDate>Sat, 23 Apr 2022 17:45:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=38&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——自注意力机制（Self-attention）&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-attention&#34;&gt;Self-attention
&lt;/h1&gt;&lt;p&gt;考虑两种不同的输入：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入是一个向量&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入是一排向量&lt;/strong&gt;（输入的向量个数可能会改变）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;将一排向量作为输入&#34;&gt;将一排向量作为输入
&lt;/h2&gt;&lt;h3 id=&#34;方法&#34;&gt;方法
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;One-hot Encoding&lt;/li&gt;
&lt;li&gt;Word Embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;输入&#34;&gt;输入
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;一段语音窗口&lt;/li&gt;
&lt;li&gt;一张图&lt;/li&gt;
&lt;li&gt;分子结构&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;输出&#34;&gt;输出
&lt;/h3&gt;&lt;h4 id=&#34;n个向量对应n个标签&#34;&gt;N个向量对应N个标签
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;句子中每个词的词性：I saw a saw -&amp;gt; N V DET N&lt;/li&gt;
&lt;li&gt;社交网络中每个人的购买意向：甲 -&amp;gt; buy;乙 -&amp;gt; not&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;n个向量对应一个标签&#34;&gt;N个向量对应一个标签
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;情感分析：this is good -&amp;gt; positive&lt;/li&gt;
&lt;li&gt;分子属性分析：一个分子图 -&amp;gt; 亲水性&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;n个向量对应多个标签seq2seq&#34;&gt;N个向量对应多个标签（seq2seq）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;机器翻译&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fully-connected-network&#34;&gt;Fully-connected Network
&lt;/h2&gt;&lt;p&gt;使用全连接网络，设置窗口大小，每次输入邻近的多个词。但限制于窗口大小，无法考虑整个句子的影响，且窗口覆盖整个句子比较困难。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/fc.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;self-attention-1&#34;&gt;Self-attention
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：一排向量${a^1,a^2,a^3,a^4}$&lt;/li&gt;
&lt;li&gt;输出：一排向量${b^1,b^2,b^3,b^4}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;$b^1$、$b^2$、$b^3$、$b^4$分别都是考虑了$a^1$、$a^2$、$a^3$、$a^4$而产生的。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;如何生成b1&#34;&gt;如何生成$b^1$？
&lt;/h3&gt;&lt;h4 id=&#34;a1与其他向量的相关性alpha的计算方法&#34;&gt;$a^1$与其他向量的相关性$\alpha$的计算方法
&lt;/h4&gt;&lt;h5 id=&#34;dot-product&#34;&gt;&lt;strong&gt;Dot-product&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;输入两个向量，分别与矩阵$W^q$和$W^k$相乘，再将得到的向量$q$和$k$做点乘。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/dot-product.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\begin{aligned}
q&amp;=a^1\times W^q \\
k&amp;=a^2\times W^k \\
\alpha &amp;= q\cdot k
\end{aligned}
$$
&lt;h5 id=&#34;additive&#34;&gt;Additive
&lt;/h5&gt;&lt;p&gt;输入两个向量，分别与矩阵$W^q$和$W^k$相乘，将得到的向量$q$和$k$串起来并通过激活函数，最后通过一个变换得到$alpha$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/additive.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;b1的计算&#34;&gt;$b^1$的计算
&lt;/h4&gt;&lt;p&gt;本节中的例子中，对于$b^1$，计算步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step1.计算$q^1$&lt;/li&gt;
&lt;/ul&gt;
$$
q^1=W^qa^1
$$&lt;ul&gt;
&lt;li&gt;step2.计算$k^1$、$k^2$、$k^3$、$k^4$&lt;/li&gt;
&lt;/ul&gt;
$$
k^i=W^ka^i
$$&lt;ul&gt;
&lt;li&gt;step3.计算$\alpha_{1,1}$、$\alpha_{1,2}$、$\alpha_{1,3}$、$\alpha_{1,4}$&lt;/li&gt;
&lt;/ul&gt;
$$
\alpha_{1,i}=q^1\cdot k^i \\
$$&lt;ul&gt;
&lt;li&gt;step4.通过Soft-max（也可使用ReLU等）计算$\alpha_{1,1}&amp;rsquo;$、$\alpha_{1,2}&amp;rsquo;$、$\alpha_{1,3}&amp;rsquo;$、$\alpha_{1,4}&#39;$&lt;/li&gt;
&lt;/ul&gt;
$$
\alpha_{1,i}&#39;=\frac{\exp(\alpha_{1,i})}{\sum_j\exp(\alpha_{1,j})}
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  v^i=W^va^i
  $$&lt;/li&gt;
&lt;li&gt;
$$
  b^1=\sum_j\alpha_{1,j}&#39;v^j
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;输出向量组b1b2b3b4的完整计算过程&#34;&gt;输出向量组${b^1,b^2,b^3,b^4}$的完整计算过程
&lt;/h3&gt;&lt;p&gt;整理以上过程，$Q$、$K$、$V$和Attention分数的计算过程如下：&lt;/p&gt;
&lt;h4 id=&#34;qkv的计算&#34;&gt;$Q$、$K$、$V$的计算
&lt;/h4&gt;&lt;p&gt;由：&lt;/p&gt;
$$
\begin{aligned}
q^i &amp;=W^qa^i \\
k^i &amp;=W^ka^i \\
v^i &amp;=W^va^i 
\end{aligned}
$$$$
\begin{aligned}
\begin{bmatrix}
q^1 &amp; q^2 &amp; q^3 &amp; q^4
\end{bmatrix}
&amp;=W^q
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\\
\begin{bmatrix}
k^1 &amp; k^2 &amp; k^3 &amp; k^4
\end{bmatrix}
&amp;=W^k
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\\
\begin{bmatrix}
v^1 &amp; v^2 &amp; v^3 &amp; v^4
\end{bmatrix}
&amp;=W^v
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\end{aligned}
$$&lt;p&gt;即：&lt;/p&gt;
$$
\begin{aligned}
Q&amp;=W^qI \\
K&amp;=W^kI \\
V&amp;=W^vI
\end{aligned}
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;attention分数的计算&#34;&gt;Attention分数的计算
&lt;/h4&gt;&lt;p&gt;由：&lt;/p&gt;
$$
\alpha_{1,i}=k^i\cdot q^1
$$&lt;p&gt;有：&lt;/p&gt;
$$
\begin{bmatrix}
\alpha_{1,1} \\
\alpha_{1,2} \\
\alpha_{1,3} \\
\alpha_{1,4} \\
\end{bmatrix}=
\begin{bmatrix}
k^1 \\
k^2 \\
k^3 \\
k^4 
\end{bmatrix}
\cdot
q^1
$$&lt;p&gt;进一步有：&lt;/p&gt;
$$
\begin{bmatrix}
\alpha_{1,1} &amp; \alpha_{2,1} &amp; \alpha_{3,1} &amp;\alpha_{4,1} \\
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{3,2} &amp;\alpha_{4,2} \\
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp;\alpha_{4,3} \\
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp;\alpha_{4,4} \\
\end{bmatrix}=
\begin{bmatrix}
k^1 \\
k^2 \\
k^3 \\
k^4 
\end{bmatrix}
\cdot
\begin{bmatrix}
q^1 &amp; q^2 &amp; q^3 &amp;q^4
\end{bmatrix}
$$&lt;p&gt;即：&lt;/p&gt;
$$
A=K^TQ
$$&lt;p&gt;对$A$进行Soft-max得到$A&amp;rsquo;$：&lt;/p&gt;
$$
A&#39;=
\begin{bmatrix}
\alpha_{1,1}&#39; &amp; \alpha_{2,1}&#39; &amp; \alpha_{3,1}&#39; &amp;\alpha_{4,1}&#39; \\
\alpha_{1,2}&#39; &amp; \alpha_{2,2}&#39; &amp; \alpha_{3,2}&#39; &amp;\alpha_{4,2}&#39; \\
\alpha_{1,3}&#39; &amp; \alpha_{2,3}&#39; &amp; \alpha_{3,3}&#39; &amp;\alpha_{4,3}&#39; \\
\alpha_{1,4}&#39; &amp; \alpha_{2,4}&#39; &amp; \alpha_{3,4}&#39; &amp;\alpha_{4,4}&#39; \\
\end{bmatrix}
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\begin{bmatrix}
b^1 &amp; b^2 &amp; b^3 &amp; b^4
\end{bmatrix}=
\begin{bmatrix}
v^1 &amp; v^2 &amp; v^3 &amp; v^4
\end{bmatrix}
\cdot
\begin{bmatrix}
\alpha_{1,1}&#39; &amp; \alpha_{2,1}&#39; &amp; \alpha_{3,1}&#39; &amp;\alpha_{4,1}&#39; \\
\alpha_{1,2}&#39; &amp; \alpha_{2,2}&#39; &amp; \alpha_{3,2}&#39; &amp;\alpha_{4,2}&#39; \\
\alpha_{1,3}&#39; &amp; \alpha_{2,3}&#39; &amp; \alpha_{3,3}&#39; &amp;\alpha_{4,3}&#39; \\
\alpha_{1,4}&#39; &amp; \alpha_{2,4}&#39; &amp; \alpha_{3,4}&#39; &amp;\alpha_{4,4}&#39; \\
\end{bmatrix}
$$$$
O=VA&#39;
$$&lt;h4 id=&#34;总结&#34;&gt;总结
&lt;/h4&gt;$$
\begin{aligned}
&amp;Q=W^qI \\
&amp;K=W^kI \\
&amp;V=W^vI \\
&amp;A=K^TQ \\
&amp;A \rightarrow A&#39; \\
&amp;O=VA&#39;
\end{aligned}
$$&lt;p&gt;$W^q$、$W^k$、$W^v$是需要学习的参数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;multi-head-self-attention&#34;&gt;Multi-head Self-attention
&lt;/h2&gt;&lt;p&gt;多个$q$，对应不同种类的相关性。&lt;/p&gt;
$$
\begin{aligned}
&amp;q^{i,1}=W^{q,1}q^i \\
&amp;q^{i,2}=W^{q,2}q^i
\end{aligned}
$$&lt;p&gt;
类似的，$a^j$对应的$q^j$、$k^j$、$v^j$具体有$q^{j,1}$、$k^{j,1}$、$v^{j,1}$：&lt;/p&gt;
$$
b^i=W^O
\begin{bmatrix}
b^{i,1} \\
b^{i,2}
\end{bmatrix}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/multi-head%20self-attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;位置编码&#34;&gt;位置编码
&lt;/h2&gt;&lt;p&gt;Self-attention中，对输入的几个向量所进行的操作是相同的，与位置无关，可能丢失了位置信息。&lt;/p&gt;
$$
e^i+a^i\rightarrow q^i,k^i,v^i
$$&lt;p&gt;
向量$e^i$可以通过一个规则设定（人工）或从训练数据中学习出来。&lt;/p&gt;
&lt;p&gt;有各种不同的方法产生位置编码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sinusoidal&lt;/li&gt;
&lt;li&gt;Position embedding&lt;/li&gt;
&lt;li&gt;FLOATER&lt;/li&gt;
&lt;li&gt;RNN&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention用于语音&#34;&gt;Self-attention用于语音
&lt;/h2&gt;&lt;p&gt;把声音讯号表示为一排向量，一般一个向量只有10ms的长度，会导致向量个数过多，$A&amp;rsquo;$计算的复杂度是$O(L^2)$，一般使用&lt;strong&gt;Truncated Self-attention&lt;/strong&gt;，不看一整句话，只看一小部分。&lt;/p&gt;
&lt;h2 id=&#34;self-attention用于图像&#34;&gt;Self-attention用于图像
&lt;/h2&gt;&lt;p&gt;一张图片可以看成是一排向量。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20for%20img.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Self-Attention GAN&lt;/li&gt;
&lt;li&gt;Detection Transformer(DETR)&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20for%20img2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;self-attention-vs-cnn&#34;&gt;Self-attention v.s. CNN
&lt;/h2&gt;&lt;p&gt;Self-attention可以看作复杂化的CNN，Self-attention考虑全局。CNN是Self-attention的特例，Self-attention可以通过设定合适的参数，达到和CNN同样的效果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1911.03584&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1911.03584] On the Relationship between Self-Attention and Convolutional Layers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention-vs-rnn&#34;&gt;Self-attention v.s. RNN
&lt;/h2&gt;&lt;p&gt;RNN（Recurrent Neural Network）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20vs%20RNN.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;RNN的缺点很明显，很难去考虑到输入位置较远的向量，并且无法并行计算。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.16236&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.16236] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention用于图&#34;&gt;Self-attention用于图
&lt;/h2&gt;&lt;p&gt;图中每个结点可以表示为一个向量，边可以用来考虑结点之间的关联性，计算Attention分数时，只需计算相连的结点。&lt;/p&gt;
&lt;p&gt;Self-attention用在图上面，是某一种类型的GNN（Graph Neural Network）。&lt;/p&gt;
&lt;h2 id=&#34;more&#34;&gt;More
&lt;/h2&gt;&lt;p&gt;Self-Attention的计算量较大，优化效率是一个研究方向。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2011.04006&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2011.04006] Long Range Arena: A Benchmark for Efficient Transformers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2009.06732&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2009.06732] Efficient Transformers: A Survey (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>2021 中国高校计算机大赛 - 微信大数据挑战赛</title>
        <link>https://demo.stack.jimmycai.com/p/2021-%E4%B8%AD%E5%9B%BD%E9%AB%98%E6%A0%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E8%B5%9B-%E5%BE%AE%E4%BF%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%91%E6%88%98%E8%B5%9B/</link>
        <pubDate>Tue, 29 Jun 2021 21:15:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/2021-%E4%B8%AD%E5%9B%BD%E9%AB%98%E6%A0%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E8%B5%9B-%E5%BE%AE%E4%BF%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%91%E6%88%98%E8%B5%9B/</guid>
        <description>&lt;p&gt;多任务推荐系统赛题，初赛Rank 68/6768，复赛Rank 70。&lt;/p&gt;
&lt;h1 id=&#34;2021中国高校计算机大赛---微信大数据挑战赛&#34;&gt;2021中国高校计算机大赛 - 微信大数据挑战赛
&lt;/h1&gt;&lt;h2 id=&#34;赛题描述&#34;&gt;赛题描述
&lt;/h2&gt;&lt;p&gt;　　本次比赛基于脱敏和采样后的数据信息，对于给定的一定数量到访过微信视频号“热门推荐”的用户， 根据这些用户在视频号内的历史n天的行为数据，通过算法在测试集上预测出这些用户对于不同视频内容的互动行为（包括点赞、点击头像、收藏、转发等）的发生概率。 本次比赛以多个行为预测结果的加权uAUC值进行评分。&lt;br&gt;
　　比赛提供训练集用于训练模型，测试集用于评估模型效果，提交结果demo文件用于展示提交结果的格式。 所有数据文件格式都是带表头的.csv格式，不同字段列之间用英文逗号分隔。初赛与复赛的数据分布一致，数据规模不同。 初赛提供百万级训练数据，复赛提供千万级训练数据。&lt;/p&gt;
&lt;h2 id=&#34;baseline&#34;&gt;Baseline
&lt;/h2&gt;&lt;p&gt;　　主办方为本次比赛提供了一份基线:&lt;a class=&#34;link&#34; href=&#34;https://github.com/WeChat-Big-Data-Challenge-2021/WeChat_Big_Data_Challenge&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Wechat_Big_Data_Challenge_Baseline&lt;/a&gt;，该基线基于Wide &amp;amp; Deep模型实现，除6列原始id特征和feed时长特征外，在id特征的基础上构造了一些统计特征。Weight_uAUC线下0.657003，线上0.607908。&lt;br&gt;
　　官方提供的这份基线分为数据集生成、离线模型训练、离线模型评估、在线模型训练、生成线上提交结果几个步骤，流程比较复杂，且线上线下gap较大，达到了5个百分点（经验证是统计特征涉及时间穿越，删去此部分特征可以提高2-3个百，群里也有人在基线基础上调参也能得到线上0.65+的分数），故我并没有过多参考此份基线。&lt;br&gt;
　　我所使用的是讨论区深度匹配树大佬所开源的基于MMoE的多任务学习模型，由于MMoE的多任务训练机制，训练速度相比四个任务逐个建模大大提升，可以迅速验证一些特征的有效性。线上分数约为0.635。&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/wechat_algo_stage1/MMoE.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;MMoE&#34;
	
	
&gt;
　　TensorFlow版：&lt;a class=&#34;link&#34; href=&#34;https://github.com/zanshuxun/WeChat_Big_Data_Challenge_DeepCTR_baseline&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;mmoe_tf&lt;/a&gt;，Pytorch版：&lt;a class=&#34;link&#34; href=&#34;https://github.com/dpoqb/wechat_big_data_baseline_pytorch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;mmoe_torch&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;统计特征&#34;&gt;统计特征
&lt;/h2&gt;&lt;p&gt;　　基线中仅使用了6列id类特征，第一想法是在这六列id的基础上构建统计特征，由于数据带有时间序列的性质，提取特征时要注意时间穿越问题。我根据user、feed等多侧的历史行为，构建了点击、曝光、CTR等相关特征，这些特征在CTR类的比赛中非常常见，Kaggle、Github上也有非常多优秀的开源代码，故这部分特征的具体提取不再赘述。&lt;br&gt;
　　但将此部分统计特征喂给nn时，nn几乎不收敛，loss波动极大，归一化后线上成绩也非常低，于是开始思考什么样的特征适合喂给nn，开始下一阶段的特征构建。&lt;/p&gt;
&lt;h2 id=&#34;512维多模态向量&#34;&gt;512维多模态向量
&lt;/h2&gt;&lt;p&gt;　　这部分特征的正确使用能够获得较大幅度的提升，我尝试了两种方法，第一种是直接merge到训练集上，不过直接merge极容易OOM（除非内存足够），第二种是给feedid的嵌入赋值权重，不过后者经过实验效果不佳，也可能是我使用的方式不对。我租用的服务器配置为2*P40 + 112G RAM，将512维多模态向量经过PCA降维到48维后并在训练集上送入nn进行训练，线上线下均有大幅度提升，仅加入多模态这部分embedding特征后，线上成绩可以直接突破0.65。&lt;/p&gt;
&lt;h2 id=&#34;tagkeyword多值离散特征&#34;&gt;tag、keyword：多值离散特征
&lt;/h2&gt;&lt;p&gt;　　这部分特征的处理方式有很多，例如作为序列提取通过word2vec提取embedding，将每个离散取值当成词，整个tag/keyword列表当作句子，获取每个词的词向量后做pooling操作得到该部分的embedding，此处可参考&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==&amp;amp;mid=2247496721&amp;amp;idx=1&amp;amp;sn=c7fab106254f555cbea64e8464c84074&amp;amp;chksm=c32aed9ef45d64887f23606031d052c3fdce868b975644ca62db52bfd70185d4ddd212f0364f&amp;amp;mpshare=1&amp;amp;scene=23&amp;amp;srcid=0701JaeEB4Z1IDrtYDj3zaDK&amp;amp;sharer_sharetime=1625107770080&amp;amp;sharer_shareid=8c3bd21461ee94c3d6cc62dff5de10ac#rd&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;序列问题必备特征工程——基于Word2Vec的文本向量&lt;/a&gt;，或者通过embedding_lookup，此处可参考&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/149014347&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;推荐算法-4.多值离散特征的embedding解决方案&lt;/a&gt;。&lt;br&gt;
　　该部分特征对分数的提升也能达到7-8个千，我的线上分数也达到了0.659。&lt;/p&gt;
&lt;h2 id=&#34;多种子融合&#34;&gt;多种子融合
&lt;/h2&gt;&lt;p&gt;　　由于tensorflow的内部机制，导致其无法完全固定随机种子，同特征同参数下训练结果有一定幅度波动，波动幅度大概有2-3个千，通过多次训练，取平均可以稳定结果，线上成绩有约2个千的小幅度提升。&lt;/p&gt;
&lt;h2 id=&#34;树模型&#34;&gt;树模型
&lt;/h2&gt;&lt;p&gt;　　我在0.664附近卡了近两周的时间，比赛中后期的时候，天才儿童在6.14的周周星分享中开源了一份线上成绩0.645的&lt;a class=&#34;link&#34; href=&#34;https://developers.weixin.qq.com/community/minihome/article/doc/0006467d05427892b94c341aa56813&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;梯度提升决策树模型基线&lt;/a&gt;，当时队伍中仅我自己一人，仅靠单模进入复赛不太稳定，而且初赛由于数据采样的原因，树模型会比nn更有优势，故决定转手再做一个树模型。&lt;br&gt;
　　树模型中主要构造了一些统计类特征，和前文提到的类似，主要包括曝光、转化、视频观看等情况的滑窗统计特征，以及包括曝光、偏好等&lt;strong&gt;全局信息统计特征&lt;/strong&gt;（全局统计也能上分..Orz）。&lt;br&gt;
　　我在此份基线的基础上添加了nn中使用的几个embedding特征，树模型单模单折分数做到0.655。和我此前0.664的nn仅5/5平均后，就能够有3个k的提升。&lt;/p&gt;
&lt;h2 id=&#34;基于embedding的衍生特征&#34;&gt;基于embedding的衍生特征
&lt;/h2&gt;&lt;p&gt;　　这时距离比赛结束还有一周的时间，我在群里找到了一个做nn的和一个做树模型的队有，单模分数都在0.664上下，我和另一个队友的sub简单融合后分数达到了0.670，提升较大，然后继续优化nn单模。&lt;br&gt;
　　基于前面提取的多个embedding特征，我在此基础上又提取了一些衍生特征，方式包括滑窗pooling等，收益较大，简单衍生后就能提升3个千。&lt;/p&gt;
&lt;h2 id=&#34;初赛b榜&#34;&gt;初赛B榜
&lt;/h2&gt;&lt;p&gt;　　B榜数据和A榜的user无重叠，分布一致，排行榜上普遍有3个k到6个k的下降，我们的B榜最终分数为0.67093。&lt;/p&gt;
&lt;h2 id=&#34;待解决的问题&#34;&gt;待解决的问题
&lt;/h2&gt;&lt;h3 id=&#34;部分feed冷启动&#34;&gt;部分feed冷启动
&lt;/h3&gt;&lt;p&gt;　　简单观察数据可发现，第15天仍有2607个feed冷启动（未在user_action中出现），样本量为72758。&lt;br&gt;
　　对于冷启动问题，目前的基本思路是做矩阵SVD分解，构建用户和商品的交互矩阵，对稀疏矩阵进行SVD分解，得到用户和商品的向量，将用户向量和商品向量作为特征拼接到用户和商品侧。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/wechat_algo_stage1/feed%E5%86%B7%E5%90%AF%E5%8A%A8&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;每日冷启动feed数量&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;模型结构&#34;&gt;模型结构
&lt;/h3&gt;&lt;p&gt;　　比赛初期GDY郭大使用Transformer输入原始的id类特征轻松上到了0.65+，从后面周周星分享的一些思路也可以看出，模型结构的修改可以带来比较大的收益，例如参考DIN对用户的长短期兴趣进行表征等。&lt;/p&gt;
&lt;h2 id=&#34;参考代码&#34;&gt;参考代码
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/meurice996/WBDC2021_Solution&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/meurice996/WBDC2021_Solution&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>2021 招商银行 FinTech 数据赛道</title>
        <link>https://demo.stack.jimmycai.com/p/2021-%E6%8B%9B%E5%95%86%E9%93%B6%E8%A1%8C-fintech-%E6%95%B0%E6%8D%AE%E8%B5%9B%E9%81%93/</link>
        <pubDate>Tue, 18 May 2021 00:48:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/2021-%E6%8B%9B%E5%95%86%E9%93%B6%E8%A1%8C-fintech-%E6%95%B0%E6%8D%AE%E8%B5%9B%E9%81%93/</guid>
        <description>&lt;p&gt;时间序列回归赛题，一个简单的方案分享+指标MAPE翻车记录，B榜Rank53。magic number yyds~&lt;/p&gt;
&lt;h1 id=&#34;2021招商银行-fintech-精英训练营---数据赛道&#34;&gt;2021招商银行 FinTech 精英训练营 - 数据赛道
&lt;/h1&gt;&lt;h2 id=&#34;赛题任务&#34;&gt;赛题任务
&lt;/h2&gt;&lt;p&gt;　　本次竞赛给出的数据包含日期、节假日信息、时间段、岗位（含2种岗位A、B）、业务类型和业务量数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;任务1：预测未来31天各岗位每天的业务量总量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;任务2：预测未来31天各岗位每天每半小时粒度的业务总量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A榜：提供2018年1月1日到2020年10月31日的训练数据（train_v1），选手提交2020年11月1日到2020年11月30日的预测结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;B榜：提供2018年1月1日到2020年11月30日的训练数据（train_v2），选手提交2020年12月1日到2020年12月31日的预测结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
MAPE = \frac{1}{N} \sum_{i=1}^{N} \left| \frac{Y_i - \hat{Y_i}}{Y_i + 1} \right| \times 100\%\\
\end{aligned}
$$&lt;h2 id=&#34;解决方案&#34;&gt;解决方案
&lt;/h2&gt;&lt;h3 id=&#34;任务1&#34;&gt;任务1
&lt;/h3&gt;&lt;p&gt;　　简单观察数据可发现，A、B岗位业务量差距较大，故A、B岗位分开建模。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/fintech2021/A%E5%B2%97%E4%BD%8D%E6%97%A5%E4%B8%9A%E5%8A%A1%E9%87%8F.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;A岗位日业务量&#34;
	
	
&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/fintech2021/B%E5%B2%97%E4%BD%8D%E6%97%A5%E4%B8%9A%E5%8A%A1%E9%87%8F.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;B岗位日业务量&#34;
	
	
&gt;
　　对于日业务量，我均采用&lt;strong&gt;LightGBM回归模型&lt;/strong&gt;进行预测。&lt;br&gt;
　　考虑到特殊时期对业务量的影响（特别是2020年上半年的Covid-19疫情），仅采用&lt;strong&gt;2018年3月1日至2018年11月30日&lt;/strong&gt;的数据作为训练集，采用&lt;strong&gt;2018年12月1日至2018年12月30日&lt;/strong&gt;的数据作为验证集（B榜，A榜划分类似，向前推一个月即可）。&lt;br&gt;
　　特征工程方面，所做的工作并不太多，特征包括&lt;strong&gt;日期特征&lt;/strong&gt;（day_of_week、day_of_month&amp;hellip;）、&lt;strong&gt;节假日特征&lt;/strong&gt;（节假日类型、距离下个工作日的天数、距离下个节假日的天数&amp;hellip;）等。&lt;br&gt;
　　&lt;strong&gt;后处理&lt;/strong&gt;是本题上分的一个关键点。观察数据，可以大致推断出接近年底的业务量是一个逐渐升高的趋势，将模型所预测的结果拼接到原数据集后再对整体走势做可视化分析，可以对模型预测结果的合理性做出大致判断。最开始，我尝试对预测的所有结果都乘一个系数（大约1.25左右，具体根据训练集和使用的特征等的不同会有一定的差异），获得了比较大的收益，随后我又对这个系数更加细化，月上旬、中旬、下旬分别乘不同的系数（逐渐递增），也获得了一定的提升，继续细化这个粒度（按周、按日）应该还会有提升，不过我并没有做更多这方面的尝试。&lt;br&gt;
　　&lt;strong&gt;PS&lt;/strong&gt;：关于后处理还有一些比较不寻常的方法，比如老肥将每一条预测结果都加上大小为666的偏移量，同样取得了较好的线上分数&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;任务2&#34;&gt;任务2
&lt;/h3&gt;&lt;p&gt;　　任务2同样采用LightGBM回归模型，训练数据与任务1相同，特征方面主要就增加了periods。由于与任务1相比，任务2的误差较大，所以我将任务2的训练从直接预测业务量改为了&lt;strong&gt;预测当前时间段的业务量占当天全部业务量的比例&lt;/strong&gt;，利用到了任务1的预测结果来调整任务2，这样做还一个好处是，任务1、2基本是同增同减的状态。&lt;/p&gt;
&lt;h2 id=&#34;关于指标mape&#34;&gt;关于指标MAPE
&lt;/h2&gt;&lt;p&gt;　　本次比赛中，我一开始就错误的将MAPE当作Lgb的metric，后面一直都忽略了这点，导致任务2的预测结果问题很大，特别是业务量较少（接近0）时，对指标的影响非常大。当我任务1优化到0.059时，任务2的MAPE仍为0.20，最终B榜也仅位于第53名，将metric调整为MSE即可。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当实际值为零时，MAPE会采用未定义的值，例如在需求预测中可能会发生这种情况。此外，当实际值非常接近零时，它将采用极值。&lt;/li&gt;
&lt;li&gt;MAPE是不对称的，它对负误差（当预测值高于实际值时）要比对正误差施加更大的罚款。解释如下：对于过低的预测，百分比误差不能超过100％。虽然没有太高的预测上限。因此，MAPE将偏向于预测不足而不是过度预测的模型。&lt;/li&gt;
&lt;li&gt;MAPE假定变量的度量单位具有有意义的零值。因此，尽管预测需求并使用MAPE是有意义的，但当预测温度以摄氏度（不仅是那个）表示时，却没有意义，因为温度具有任意零点。&lt;/li&gt;
&lt;li&gt;MAPE并非到处都是可微的，在将其用作优化标准时可能会导致问题。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
