<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>目标检测 on zn.yan</title>
        <link>https://demo.stack.jimmycai.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</link>
        <description>Recent content in 目标检测 on zn.yan</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 14 Jul 2022 09:41:05 +0000</lastBuildDate><atom:link href="https://demo.stack.jimmycai.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>YOLO v1 - v7</title>
        <link>https://demo.stack.jimmycai.com/p/yolo-v1-v7/</link>
        <pubDate>Thu, 14 Jul 2022 09:41:05 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/yolo-v1-v7/</guid>
        <description>&lt;p&gt;本文从Head、Backbone、Neck等方面介绍了YOLO的核心演变过程。&lt;/p&gt;
&lt;h1 id=&#34;yolov1---yolov7&#34;&gt;YOLOv1 - YOLOv7
&lt;/h1&gt;&lt;h2 id=&#34;yolov0&#34;&gt;YOLOv0
&lt;/h2&gt;&lt;p&gt;将目标检测任务当作是遍历性的分类任务，就可以使用分类器来完成检测。&lt;/p&gt;
&lt;p&gt;例如可以用一个框分别落在图片的各个区域，对框住的区域进行二分类，然后因为目标大小的不同，可能还需要使用不同大小的框来遍历图片，另外，为了提高检测精度，还可以使用滑动窗口的方法来遍历图像的所有位置&amp;hellip;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/2.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;R-CNN所采用的就是滑动窗口策略。&lt;/p&gt;
&lt;p&gt;而YOLO的思路是，将分类器的输出从一个One-hot向量换成$(c,x,y,w,h)$，直接输出这个框和置信度，将问题转化为一个回回归问题。要组织训练也比较简单，只需找很多的图片，标注框的位置，并将置信度$c$设为1，送入网络即可。&lt;/p&gt;
&lt;p&gt;以上便是YOLO最简单的版本。&lt;/p&gt;
&lt;h2 id=&#34;yolov1&#34;&gt;YOLOv1
&lt;/h2&gt;&lt;h3 id=&#34;head&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;在YOLOv0中，一张图片输出一个五元组$(c,x,y,w,h)$，只能输出一个目标。要输出多个目标，YOLOv1中采用的方式是将图片分为多个的网格（$S\times S$），每个网格单元对应一个五元组，就可以输出$S\times S$个框，如果目标的中心落在一个网格单元中，那么这个网格单元就会负责检测这个对象，如果网格单元中不存在目标，则置信度为0。&lt;/p&gt;
&lt;p&gt;例如当$S=4$时，输入一张图片：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;所返回的tensor为$(5,4,4)$，其中标签的置信度如下所示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;现在暂不考虑一个网格单元中存在多个目标的问题（上图3行3列），当得到$S\times S$个框后，YOLOv1中使用NMS（非极大值抑制）将目标所在的框保留下来而去除其它框。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 保留得分最高的框；&lt;/li&gt;
&lt;li&gt;Step2. 根据交并比，去掉与置信度最高的框重合度较高的框；&lt;/li&gt;
&lt;li&gt;Step3. 在剩下的框中继续选择得分最高的框，以此类推，直到没有剩下的框。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当要检测多个目标（$C$）时（例如既要检测葫芦娃的头，又要检测葫芦娃的葫芦），在返回的结果中多加一个二维的One-hot向量，此时输入一张图片后，返回的结果为：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;考虑到小目标检测效果不佳的问题，针对不同尺寸的目标，YOLOv1中为每个网格单元预测多个五元组（$B$），分别负责不同尺寸的目标的回归。&lt;/p&gt;
&lt;p&gt;以上，YOLO v1的预测结果为一个$S\times S\times(B*5+C)$的tensor，在YOLOv1中，$S=7$，$B=2$，$C=20$（PASCAL VOC数据集有20类），即每张图片的最终输出为$7\times 7\times 30$的tensor。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv1的整体结构如下（在ImageNet上训练时使用$224\times 224$的分辨率，检测时使用$448\times 448$。）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/7.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;YOLOv1对平方和误差（sum-squared error）进行优化，但它对定位误差和分类误差的权重相等，YOLOv1引入$\lambda_{coord}$和$\lambda_{noobj}$增加边界框坐标预测的损失，并减少不包含对象的框的置信度预测的损失，设$\lambda_{coord}=5,\lambda_{noobj}=.5$。此外，为了反映大检测框中的小偏差要比小检测框中的小偏差影响要小，YOLOv1对边界框宽度和高度的平方根进行预测，而不是直接预测宽度和高度。&lt;/p&gt;
&lt;h3 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
&amp;\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2] \\
+&amp;\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(\sqrt{w_i}-\sqrt{\hat{w}_i})^2+(\sqrt{h_i}-\sqrt{\hat{h}_i})^2] \\
+&amp;\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2 \\
+&amp;\lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2 \\
+&amp;\sum_{i=0}^{S^2}\mathbb{1}_i^{obj}\sum_{c\in classes}(p_i(c)-\hat{p}_i(c))^2
\end{aligned}
$$$$
\mathbb{1}_{ij}^{obj}=\left\{
\begin{aligned}
&amp;1,\quad第i个网格第j个\text{anchor box}负责预测这个物体 \\
&amp;0,\quad其他
\end{aligned}
\right.
$$&lt;h3 id=&#34;backbone&#34;&gt;Backbone
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/7.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;还是这张图，YOLOv1的backbone是参考GoogLeNet设计的，除最后两层全连接外，其它都大量使用了卷积层。下图是YOLOv4的论文中总结的检测类网络的结构：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/1.png&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;在YOLOv1中只有backbone，没有neck，属于Dense Prediction，一阶段检测器属于Dense Prediction，二阶段检测器既有Dense Prediction，又有Sparse Prediction。&lt;/p&gt;
&lt;h3 id=&#34;训练过程&#34;&gt;训练过程
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Step1. 在ImageNet上使用$224\times224$分辨率的图片训练分类器网络；&lt;/li&gt;
&lt;li&gt;Step2. 用$448\times448$分辨率端到端地训练目标检测网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yolov2--yolo9000&#34;&gt;YOLOv2 &amp;amp; YOLO9000
&lt;/h2&gt;&lt;h3 id=&#34;head-1&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;YOLOv1虽然检测速度快，但是定位不够准确，召回率较低，YOLOv2在YOLOv1的基础上提出了一些改进策略：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/1.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;h4 id=&#34;基于偏移量的边框回归&#34;&gt;基于偏移量的边框回归
&lt;/h4&gt;&lt;p&gt;同时代的R-CNN所预测的是偏移量，而YOLOv1直接预测框的坐标和大小，范围较大，YOLOv2中使用到了两种边框回归方式：基于anchor的偏移量和基于grid的偏移量。（anchor是R-CNN系列的一个概念，可以简单理解为一个预定义好的框，位置、宽高均已知，供预测时参考。）&lt;/p&gt;
&lt;p&gt;YOLOv2中的预测方式如下图所示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;其中，$b_x,b_y,b_w,b_h$是最终得到的检测结果，$t_x,t_y,t_w,t_h$是模型要预测的值，$c_x,c_y$是grid左上角的坐标，$p_w,p_h$是anchor的宽和高。YOLOv2基于anchor框的宽高和grid的先验位置预测的偏移量。&lt;/p&gt;
&lt;p&gt;例如对于下图，图片被分为9个grid，红色框为ground truth，紫色框为anchor：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\begin{aligned}
c_x&amp;=\frac{149}{149}=1\quad c_y=\frac{149}{149}=1 \\
b_x&amp;=\frac{230}{149}=1.543\quad b_y=\frac{218}{149}=1.463
\end{aligned}
$$$$
\begin{aligned}
b_x&amp;=\sigma(t_x)+c_x \\
b_y&amp;=\sigma(t_y)+c_y \\
b_w&amp;=p_we^{t_w} \\
b_h&amp;=p_he^{t_h}
\end{aligned}
$$$$
\begin{aligned}
t_x&amp;=\log\frac{1.543-1}{1-(1.543-1)}=0.172\\
t_y&amp;=\log\frac{1.463-1}{1-(1.463-1)}=-0.148 \\
t_w&amp;=\log\frac{224}{315}=-0.340 \\
t_h&amp;=\log\frac{202}{280}=-0.326
\end{aligned}
$$$$
\sigma(x)=\frac{1}{1+e^{-x}}\quad(\text{Sigmoid})
$$$$
125=5\times 5(c,x,y,w,h)+5\times 20\text{classes}
$$&lt;p&gt;
最后，关于每个网格中anchor个数的选择，YOLOv2的作者对VOC和COCO数据集的GT bounding box进行聚类，发现$k=5$时能够较好的平衡召回率和模型的复杂度。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;backbone-1&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;为了进一步提升性能，YOLOv2重新训练了一个DarkNet-19（下图双横线上方）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;VGGNet中得到一个结论，$7\times7$卷积可以用更小的卷积代替，且$3\times 3$卷积更加节约参数，使模型更小，且网络可以做得更深，因为能够引入更多的非线性的激活函数，能够更好地提取到特征。&lt;/p&gt;
&lt;p&gt;此外，还使用了bottleneck结构：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/3.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;使用$1\times1$的网络结构可以很方便的改变维度，灵活设计网络且减少计算量。&lt;/p&gt;
&lt;p&gt;同时在backbone中也没有FC层了，而是使用了GAP（Global Average Pooling）层，将$1000\times7\times7$映射为$1000\times1$，满足了不同尺度的输入图片的需求。并且针对较小的目标，只需把图片放大，就可以放大目标，提高检测精度。&lt;/p&gt;
&lt;p&gt;最后，由于backbone网络DarkNet-19是单独在ImageNet上训练的，所以最后加了softmax。&lt;/p&gt;
&lt;p&gt;完整的YOLOv2网络结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;reorg即passthrough layer，将$26\times26\times64$转为$13\times13\times256$，特征图大小减少4倍，通道数增加4倍，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/5.png&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;h3 id=&#34;训练过程-1&#34;&gt;训练过程
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Step1. 在ImageNet上训练DarkNet-19，输入为$224\times224$，训练160个epoch；&lt;/li&gt;
&lt;li&gt;Step2. 将网络输入调整为$448\times 448$（测试时使用$416\times416$），继续在ImageNet上finetune，训练10个epoch；（将$448\times448$改为$416\times416$，将创建奇数空间维度，对于一些大目标，其中心点往往落入图片的中心位置，使用特征图中心的1个网格单元去预测bounding box要更容易。）&lt;/li&gt;
&lt;li&gt;Step3. 修改DarkNet-19模型：移除最后的卷积层、GAP层和softmax层，新增3个$3\times3\times1024$的卷积层和1个passthrough层，最后用$1\times1$的卷积层输出预测结果，并在检测数据集上继续finetune网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yolov3&#34;&gt;YOLOv3
&lt;/h2&gt;&lt;h3 id=&#34;head-2&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;YOLOv2对于小目标的检测效果仍然不佳，在YOLOv3中，检测头分成了三部分：$13\times 13\times[3*(4+1+80)]$、$26\times 26\times[3*(4+1+80)]$和$52\times 52\times[3*(4+1+80)]$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv3/2.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;检测头的三个分支分别为32倍、16倍和8倍下采样，感受野由大变小，分别去预测大、中、小目标，对于每一个grid，共设置了9个先验框，大、中、小各3个，每个框预测一个五元组和80维的One-hot分类向量。三个特征图一共可以解码出$(52\times52+26\times26+13\times13)\times3=10647$个bounding box。&lt;/p&gt;
&lt;p&gt;YOLOv3仍然采用k-means聚类来确定先验框的个数，对于COCO数据集，9个聚类簇为$(10\times13),(16\times30),(33\times23),(30\times61),(62\times45),(59\times119),(116\times90),(156\times198),(373\times326)$。&lt;/p&gt;
&lt;p&gt;YOLOv3的网络结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv3/1.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;YOLOv3使用多标签分类，使用多个独立的logistic分类器代替softmax，因为部分数据集中存在许多重叠的标签（Woman and Person），多标签方法可以建立更优的模型。&lt;/p&gt;
&lt;h3 id=&#34;训练策略&#34;&gt;训练策略
&lt;/h3&gt;&lt;p&gt;YOLOv3的训练策略非常重要，论文中的表述如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following. We use the threshold of .5. Unlike our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;预测框分为正例、负例和忽略样例三种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正例：任取一个ground truth，与10647个框计算IoU，IoU最大的预测框即为正例，下一个ground truth在余下的10646个检测框中寻找IoU最大的检测框作为正例。
&lt;ul&gt;
&lt;li&gt;正例产生置信度loss、检测框loss和类别loss；&lt;/li&gt;
&lt;li&gt;置信度标签为1，对应的类别标签为1（其余为0），预测框为$(t_x,t_y,t_w,t_h)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;负例：正例除外，与全部的ground truth的IoU都小于阈值（论文中为0.5）则为负例。（注：与ground truth计算IoU最大的检测框，即使IoU小于阈值，仍然为正例。）
&lt;ul&gt;
&lt;li&gt;负例仅产生置信度loss；&lt;/li&gt;
&lt;li&gt;置信度标签为0。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;忽略样例：正例除外，与任意一个ground truth的IoU大于阈值则为忽略样例。
&lt;ul&gt;
&lt;li&gt;忽略样例不产生任何loss。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;损失函数-1&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
loss_{N_1}&amp;=\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(t_{xi}-\hat{t}_{xi})^2+(t_{yi}-\hat{t}_{yi})^2] \\
&amp;+\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(t_{wi}-\hat{t}_{wi})^2+(t_{hi}-\hat{t}_{hi})^2] \\
&amp;+\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2 \\
&amp;+\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2 \\
&amp;-\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}\sum_{c\in classes}[\hat{p}_i(c)\log(p_i(c))+(1-\hat{p}_i(c))\log(1-p_i(c))]
\end{aligned}
$$$$
Loss=loss_{N_1}+loss_{N_2}+loss_{N_3}
$$&lt;h3 id=&#34;backbone-2&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;YOLOv3使用的backbone为Darknet-53：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Darknet-53中依然采用了bottleneck结构，并添加了残差结构，但是没有池化层了，而是使用步长为2的卷积来替代池化层完成下采样的工作。&lt;/p&gt;
&lt;h3 id=&#34;neck&#34;&gt;Neck
&lt;/h3&gt;&lt;p&gt;相比YOLOv1和YOLOv2，YOLOv3增加了neck这一部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;YOLOv3的neck部分使用的是FPN（Feature Pyramid Networks），生成不同尺寸的图片，最后统计所有图片的预测结果。&lt;/p&gt;
&lt;p&gt;YOLOv3的backbone有$(13,13,1024),(26,26,512),(52,52,256)$三个输出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于$(13,13,1024)$的输出，经过5次DBL，输出$(13,13,512)$，然后一部分传到head中，另一部分经过DBL和上采样，得到$(26,26,256)$；&lt;/li&gt;
&lt;li&gt;对于$(26,26,512)$的输出，直接与上一步上采样后的结果concat，得到$(26,26,768)$，同时concat的结果经过5次DBL，得到$(26,26,256)$，一部分传入head中，另一部分经过DBL和上采样，得到$(52,52,128)$；&lt;/li&gt;
&lt;li&gt;对于$(52,52,256)$的输出，直接与上一步上采样后的结构concat，得到$(52,52,384)$，经过5次DBL后输入到head中。&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/9.jpg&#34; style=&#34;zoom:61%;&#34; /&gt;
&lt;h2 id=&#34;yolov4&#34;&gt;YOLOv4
&lt;/h2&gt;&lt;p&gt;YOLOv4在原有YOLO架构的基础上，采用了近年CNN领域中最优秀的优化策略，YOLOv4的文章如同一篇目标检测trick的综述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;head-3&#34;&gt;Head
&lt;/h3&gt;&lt;h4 id=&#34;iou-threshold&#34;&gt;IoU threshold
&lt;/h4&gt;&lt;p&gt;在YOLOv3中，1个anchor负责一个ground truth，而YOLOv4使用多个anchor负责一个ground truth，对于$GT_j$而言，只要$IoU(anchor_i,GT_j)\gt threshold$，就让$anchor_i$负责$GT_j$。&lt;/p&gt;
&lt;p&gt;该方法使得在anchor框数量不变的情况下，正样本比例增加，缓解了正负样本不均衡的问题。&lt;/p&gt;
&lt;h4 id=&#34;eliminate-grid-sensitivity&#34;&gt;Eliminate grid sensitivity
&lt;/h4&gt;$$
\begin{aligned}
b_x&amp;=1.1\cdot\sigma(t_x)+c_x \\
b_y&amp;=1.1\cdot\sigma(t_y)+c_y
\end{aligned}
$$$$
\begin{aligned}
b_x&amp;=scale_{xy}\cdot\sigma(t_x)-\frac{scale_{xy}-1}{2}+c_x \\
b_y&amp;=scale_{xy}\cdot\sigma(t_y)-\frac{scale_{xy}-1}{2}+c_y \\
\end{aligned}
$$&lt;p&gt;此处的1.1也可以替换成一个其它的略大于1的数。&lt;/p&gt;
&lt;h4 id=&#34;ciou-loss&#34;&gt;CIoU-loss
&lt;/h4&gt;$$
L_{IoU}=1-\frac{B\cap B_{gt}}{B\cup B_{gt}}
$$&lt;p&gt;
但这样带来的问题是，当ground truth和bounding box没有重合时，没有梯度回传，并且对于以下这种情况，三者IoU相等，但重合度不一样，左边较好，右边较差：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/1.png&#34; style=&#34;zoom:67%;&#34; /&gt;
$$
L_{GIoU}=1-IoU+\frac{|C-B\cup B_{gt}|}{|C|}
$$&lt;p&gt;
其中，$C$为同时包含预测框和真实框的最小框的面积。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/2.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;GIoU Loss解决了IoU Loss对距离不敏感的问题，但GIoU Loss存在训练过程中发散等问题，针对此，DIoU被提出，DIoU的作者提出了两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是否可以直接最小化anchor与目标框的归一化距离，使其更快速收敛？&lt;/li&gt;
&lt;li&gt;如何使回归在与目标框有重叠甚至包含时更准确、更快？&lt;/li&gt;
&lt;/ul&gt;
$$
L_{DIoU}=1-IoU+\frac{\rho^2(b,b^{gt})}{c^2}
$$&lt;p&gt;
其中$b$、$b^{gt}$分别代表预测框和真实框的中心点，$\rho$表示计算两个中心点间的欧氏距离，$c$代表能够同时包含预测框和真实框的最小闭包区域的对角线距离。&lt;/p&gt;
&lt;p&gt;DIoU Loss除了收敛速度比GIoU Loss要快不少外，对于下面这种情况：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/3.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;三者IoU Loss和GIoU Loss都一样，但DIoU Loss从左到右依次减小。&lt;/p&gt;
&lt;p&gt;但DIoU Loss仅缓解了bounding box全包含ground truth的问题，但没有彻底解决包含的问题，例如对于下面这种情况：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\mathcal{R}_{CIoU}=\frac{\rho^2(b,b^{gt})}{c^2}+\alpha v
$$$$
v=\frac{4}{\pi^2}\left(\arctan\frac{w^{gt}}{h^{gt}}-\arctan\frac{w}{h}\right)^2
$$$$
\mathcal{L}_{CIoU}=1-IoU+\frac{\rho^2(b,b^{gt})}{c^2}+\alpha v
$$&lt;p&gt;
CIoU Loss需要考虑$v$的梯度，长宽在$[0,1]$的情况下，$w^2+h^2$的值通常很小，会导致梯度爆炸，因此在实现时将$1/(w^2+h^2)$替换为1。&lt;/p&gt;
&lt;h3 id=&#34;损失函数-2&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
&amp;\lambda_{iou}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}L_{CIoU}\\
+&amp;\lambda_{cls}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}\lambda_c(C_i-\hat{C}_i)^2+\lambda_{cls}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{noobj}\lambda_c(C_i-\hat{C}_i)^2 \\
-&amp;\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}\sum_{c\in classes}\lambda_c[\hat{p}_i(c)\log(p_i(c))+(1-\hat{p}_i(c))\log(1-p_i(c))]
\end{aligned}
$$&lt;p&gt;
从上到下三行式子分别为定位损失、目标置信度损失和分类损失。&lt;/p&gt;
&lt;h3 id=&#34;输入端&#34;&gt;输入端
&lt;/h3&gt;&lt;p&gt;YOLOv4对训练的输入端进行改进，比如数据增强Mosaic、CmBN、SAT自对抗训练。&lt;/p&gt;
&lt;h4 id=&#34;mosaic数据增强&#34;&gt;Mosaic数据增强
&lt;/h4&gt;&lt;p&gt;YOLOv4中使用的Mosaic参考了2019年提出的CutMix数据增强方式，CutMix使用两张图片进行拼接，而Mosaic采用四张图片，使用随机缩放、随机裁剪、随机排布的方式进行拼接。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/17.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;小目标的定义是目标框长宽在$0\times0 \sim 32\times 32$的物体，数据集中小目标框占所有框达到41.4%，但仅有52.3%的图片有小目标，中目标和大目标的分布相对来说更加均匀一些。&lt;/p&gt;
&lt;p&gt;使用四张图片拼接的方法大大丰富了检测数据集，并且经过随机缩放后增加了很多小目标，增强网络鲁棒性，并且可以减少GPU，直接计算4张图片的数据，Mini-batch大小不需要很大。&lt;/p&gt;
&lt;h3 id=&#34;backbone-3&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;YOLOv4使用CSPDarknet-53作为backbone。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;csp结构&#34;&gt;CSP结构
&lt;/h4&gt;&lt;p&gt;CSPDarknet-53是在Darknet-53的基础上，借鉴2019年CSPNet的经验产生的结构，其中包含5个CSP模块，经过5次CSP模块得到的特征图大小变化为：$608\rightarrow304\rightarrow152\rightarrow76\rightarrow38\rightarrow19$。&lt;/p&gt;
&lt;p&gt;CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的，先将基础层的特征映射划分为两部分（$w&lt;em&gt;h&lt;/em&gt;i\rightarrow w&lt;em&gt;h&lt;/em&gt;i/2+w&lt;em&gt;h&lt;/em&gt;i/2$），然后通过跨阶段层次结构将其合并，减少计算量的同时保证准确率。&lt;/p&gt;
&lt;h4 id=&#34;mish激活函数&#34;&gt;Mish激活函数
&lt;/h4&gt;&lt;p&gt;YOLOv4的backbone中都使用了Mish激活函数（后面的网络仍为Leaky ReLU）。&lt;/p&gt;
&lt;p&gt;在介绍Mish前，需要先了解softplus和tanh。&lt;/p&gt;
$$
\zeta(x)=\log(1+e^x)
$$&lt;p&gt;
softplus与ReLU的曲线相似，但比ReLU更为平滑：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/Figure_1.png&#34; style=&#34;zoom:72%;&#34; /&gt;
$$
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$$$
Mish(x)=x\times\tanh(\zeta(x))
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/Figure_2.png&#34; style=&#34;zoom:72%;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;dropblock&#34;&gt;Dropblock
&lt;/h4&gt;&lt;p&gt;Dropblock于2018年提出，和Dropout功能类似，是一种缓解过拟合的一种正则化方式。&lt;/p&gt;
&lt;p&gt;Dropout会在神经网络的学习过程中，将部分隐含层节点的权重归零，但卷积层后的dropout层对网络的泛化能力影响不大，即使随机丢弃，卷积层仍然可以从相邻的激活单元学到相同的信息。&lt;/p&gt;
&lt;p&gt;而Dropblock在整个局部区域进行删减丢弃。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/12.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;neck-1&#34;&gt;Neck
&lt;/h3&gt;&lt;p&gt;YOLOv4的neck采用了SPP模块和FPN + PAN。&lt;/p&gt;
&lt;h4 id=&#34;spp模块&#34;&gt;SPP模块
&lt;/h4&gt;&lt;p&gt;SPP模块中采用$k={1\times1,5\times5,9\times9,13\times13}$的最大池化方式，再将不同尺度的特征图concat起来。（池化时采用padding操作，移动的步长为1，故池化后的特征图大小均为$13\times13$）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/13.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h4 id=&#34;fpn--pan&#34;&gt;FPN + PAN
&lt;/h4&gt;&lt;p&gt;PAN是借鉴2018年图像分割领域PANet的创新点，在YOLOv3中引入了FPN，将高层的特征信息通过上采样的方式进行传递融合，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/14.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv4在FPN层后面还添加了一个自底向上的特征金字塔，其中包含两个PAN结构，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/15.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;FPN层自顶向下传达强语义特征，特征金字塔自底向上传达强定位特征，将二者特征进行聚合。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/16.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;yolov5&#34;&gt;YOLOv5
&lt;/h2&gt;&lt;p&gt;YOLOv5结构和YOLOv5结构比较类似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/19.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;输入端-1&#34;&gt;输入端
&lt;/h3&gt;&lt;p&gt;YOLOv5的输入端采用了和YOLOv4相同的Mosaic数据增强。&lt;/p&gt;
&lt;h4 id=&#34;自适应锚框计算&#34;&gt;自适应锚框计算
&lt;/h4&gt;&lt;p&gt;在YOLOv3、YOLOv4中，初始锚框的值是通过单独的程序运行的，YOLOv5在每次训练时都自适应地计算不同训练集中的最佳锚框值。如果觉得计算锚框的效果不好，也可以在代码中将自动计算锚框的功能关闭。&lt;/p&gt;
&lt;h4 id=&#34;letterbox自适应图片缩放&#34;&gt;letterbox自适应图片缩放
&lt;/h4&gt;&lt;p&gt;常用目标检测算法中，对于不同长宽的图片，常用方式是将图片统一缩放到一个标准尺寸再送入检测网络，例如$416\times416$和$608\times608$，但如果简单的使用resize，可能会导致图片信息的丢失。&lt;/p&gt;
&lt;p&gt;letterbox的主要思想是尽可能的利用网络感受野的信息特征，YOLOv5的网络经过5次下采样，$2^5=32$，最后一层特征图上的每个点可以对应原图中$32\times$32的区域信息。&lt;/p&gt;
&lt;p&gt;以$800\times600$的图片为例，原始缩放尺寸为$416\times416$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;416\div800=\bf{0.52} \\
  &amp;416\div600=0.69
  \end{aligned}
  \right.
  $$&lt;/li&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;800\times0.52=416 \\
  &amp;600\times0.52=312
  \end{aligned}
  \right.
  $$&lt;/li&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;416-312=104 \\
  &amp;104\mod32=8 \\
  &amp;8\div2=4
  \end{aligned}
  \right.
  \quad\Rightarrow 416\times320\quad(312+8)
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backbone-4&#34;&gt;Backbone
&lt;/h3&gt;&lt;h4 id=&#34;focus结构&#34;&gt;Focus结构
&lt;/h4&gt;&lt;p&gt;Focus结构中比较关键的是切片操作，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/20.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;以YOLOv5s为例，原始$608\times608\times3$的图像输入到Focus结构中后，得到$304\times304\times12$的特征图，再经过卷积得到$304\times304\times32$的特征图。&lt;/p&gt;
&lt;h4 id=&#34;csp结构-1&#34;&gt;CSP结构
&lt;/h4&gt;&lt;p&gt;与YOLOv4不同点在于，YOLOv4只有主干网络使用了CSP结构，而YOLOv5中设计了两种CSP结构，分别应用在backbone和neck中。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/21.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;四种网络的深度和宽度&#34;&gt;四种网络的深度和宽度
&lt;/h3&gt;&lt;p&gt;YOLOv5s、YOLOv5m、YOLOv5l和YOLOv5x。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/18.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/22.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;yolov7&#34;&gt;YOLOv7
&lt;/h2&gt;&lt;p&gt;Alexey Bochkovskiy大佬的重磅Paper，YOLOv7相同体量下比YOLOv5精度更高，速度快120%（FPS），比 YOLOX 快180%（FPS），比 Dual-Swin-T 快1200%（FPS），比 ConvNext 快550%（FPS），比 SWIN-L快500%（FPS）&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture
&lt;/h3&gt;&lt;h4 id=&#34;extended-efficient-layer-aggregation-networks&#34;&gt;Extended efficient layer aggregation networks
&lt;/h4&gt;&lt;p&gt;除参数量、计算量和计算密度外，还可以从访存代价的角度来设计高效体系结构，ShuffleNet V2中分析了输入/输出通道比、结构的分支数量和Element-wise操作（ReLU、Add）对网络推理速度的影响。&lt;/p&gt;
&lt;p&gt;ShuffleNet V2中给出结论：在计算量和参数量固定的前提下，&lt;strong&gt;输入和输出的通道数相等&lt;/strong&gt;时MAC（Memory Access Cost）取下界，此时的设计是最高效的。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ShuffleNet V2的文章中提出了多个设计准则来帮助模型提速。&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1807.11164&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;vovnet&#34;&gt;VoVNet
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.09730&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;网络结构&#34;&gt;网络结构
&lt;/h6&gt;&lt;p&gt;VoVNet的网络结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;VoVNet-27的前两个stage如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于VoVNet-27：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第1个stage：包含3个$3\times3$卷积，通道数分别为64、64、128；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第2个stage：经过一个OSA模块后，输出特征变为原来的$1/4$，其中OSA模块包括5个通道数为64的$3\times3$卷积，然后将这5个卷积的输出concat在一起，通道数变为$64\times5=320$，再经过1个$1\times1$卷积。输出通道数为128；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后面3个stage均为OSA模块，只是通道数量不同。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;注：每个stage结束后都会有一次$3\times3$、stride=2的max-pooling层，用来减小特征的维度，且每个卷积层都有序列Conv-BN-ReLU。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;osaone-shot-aggregation模块&#34;&gt;OSA（One-Shot Aggregation）模块
&lt;/h6&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/12.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
X_l=H_l([X_0,X_1,...,X_{l-1}])
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/13.png&#34; style=&#34;zoom:42%;&#34; /&gt;
&lt;p&gt;DenseNet的输入与输出通道数不一致，且通道数较高，DenseNet采用了$1\times1$卷积压缩特征，下图第一行是DenseNet各个卷积层之间的相互关系的大小，$(s,l)$表示第$s$和第$l$层之间权重的归一化L1范数，可以看出浅层特征图对深层特征图的贡献很少。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/17.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;DenseNet带来了大量的特征冗余，OSA模块只在最后一层聚集前面所有的层，经过这一改动，每层输入和输出的通道数是固定的，就可以让输入和输出的通道数相等而取到最小的MAC，且不需要$1\times1$卷积来压缩特征（$1\times1$卷积会引入大量的激活函数计算）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/16.png&#34; style=&#34;zoom:56%;&#34; /&gt;
&lt;h5 id=&#34;cspvovnet&#34;&gt;CSPVoVNet
&lt;/h5&gt;&lt;p&gt;CSPVoVNet考虑了梯度路径，使不同层学到更多样化的特征。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/18.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;elan&#34;&gt;ELAN
&lt;/h5&gt;&lt;p&gt;//TODO&lt;/p&gt;
&lt;h5 id=&#34;e-elan&#34;&gt;E-ELAN
&lt;/h5&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/19.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h4 id=&#34;model-scaling-for-concatenation-based-models&#34;&gt;Model scaling for concatenation-based models
&lt;/h4&gt;&lt;p&gt;EfficientNet、Scaled-YOLOv4等方法的模型缩放主要用于PlainNet或ResNet等架构中，当这些架构执行扩容、缩容时，每一层的入度和出度都不会改变，因此可以独立分析每个伸缩因子对参数和计算量的影响，但如果将这些方法用于基于级联的体系结构，对深度执行放大或缩小时，紧接在基于级联的计算快之后的translation层的入度就会增加或减小。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/14.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;对于基于级联的模型，不能单独分析不同的比例因子，当缩放计算块的深度时，还必须计算该块的输出通道变化，然后在过渡层执行宽度因子缩放，提出的复合尺度方法既能保持模型在初始设计时的性质，又能保持最优结构。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/15.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;trainable-bag-of-freebies&#34;&gt;Trainable bag-of-freebies
&lt;/h3&gt;&lt;h4 id=&#34;repconv&#34;&gt;RepConv
&lt;/h4&gt;&lt;h5 id=&#34;repvgg&#34;&gt;RepVGG
&lt;/h5&gt;&lt;p&gt;RepVGG（CVPR-2021）使用“VGG式”单路极简架构，仅用$3\times3$卷积和ReLU，在速度和性能上都达到了SOTA水平。&lt;/p&gt;
&lt;p&gt;“VGG式”模型有以下几大优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$3\times3$卷积速度极快（现有加速库都对$3\times3$卷积有所优化）；&lt;/li&gt;
&lt;li&gt;单路架构速度快（减少分支可以加快推理速度）；&lt;/li&gt;
&lt;li&gt;单路架构省内存；&lt;/li&gt;
&lt;li&gt;单路架构灵活性更好，可灵活改变各层宽度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RepVGG最大的亮点是利用了结构重参数化，训练一个模型后，将多分支模型等价转换为单路模型，通过这种方式同时利用多分支模型训练性能高的优势和单路模型推理速度快的优势。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
Conv(x,W1)+Conv(x,W2)+Conv(x,W3)=Conv(x,(W1+W2+W3))
$$&lt;p&gt;
$1\times1$相当于一个特殊的$3\times3$卷积，而恒等映射相当于一个特殊的$1\times1$卷积，因此也是一个特殊的$3\times3$卷积，所以只需：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 将identity转换为$1\times1$卷积（单位矩阵，当前通道为1，其他通道为0）；&lt;/li&gt;
&lt;li&gt;Step2. 将$1\times1$卷积转换为$3\times3$卷积（用0填充）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是输入、输出通道均为2的情况，$3\times3$卷积的参数是4个$3\times3$矩阵，$1\times1$卷积的参数是1个$2\times2$矩阵。&lt;/p&gt;
&lt;p&gt;三个分支均有BN层，但这并不会妨碍转换的可行性。&lt;/p&gt;
$$
\begin{aligned}
\mu_\mathcal{B}&amp;\leftarrow\frac{1}{m}\sum_{i=1}^mx_i \\
\sigma_\mathcal{B}^2&amp;\leftarrow\frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_\mathcal{B})^2 \\
\hat{x}_i&amp;\leftarrow\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}} \\
y_i&amp;\leftarrow\gamma\hat{x}_i+\beta\equiv\textbf{BN}_{\gamma,\beta}(x_i)
\end{aligned}
$$$$
y_i=\gamma\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}+\beta=\frac{\gamma}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}x_i+\left(\beta-\frac{\gamma\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}\right)
$$$$
W&#39;_{i,:,:,:}=\frac{\gamma_i}{\sigma_i}W_{i,:,:,:}\ ,\quad b&#39;_i=\frac{\mu_i\gamma_i}{\sigma_i}+\beta_i
$$&lt;p&gt;
因此训练好的模型可以等价转换为只有$3\times3$卷积的单路模型。&lt;/p&gt;
&lt;h6 id=&#34;planned-repconv&#34;&gt;Planned RepConv
&lt;/h6&gt;&lt;p&gt;尽管RepConv在VGG上取得了优异的性能，但将其直接应用于ResNet和DenseNet等网络结构上时，其精度会显著降低，作者使用&lt;strong&gt;梯度流传播路径&lt;/strong&gt;来分析不同的重参化模块应该和哪些网络搭配使用。&lt;/p&gt;
&lt;p&gt;通过分析RepConv与不同架构的组合以及产生的性能，作者发现RepConv中的identity破坏了ResNet中的残差结构和DenseNet中的跨层连接，而残差结构和跨层连接为不同的特征图提供了梯度的多样性。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/3.png&#34; style=&#34;zoom:60%;&#34; /&gt;
&lt;p&gt;（RepConvN：RepConv without identity connection）&lt;/p&gt;
&lt;p&gt;得到的结论是：具有残差连接的层，其RepConv不应该具有identity连接。&lt;/p&gt;
&lt;h5 id=&#34;coarse-for-auxiliary-and-fine-for-lead-loss&#34;&gt;Coarse for auxiliary and fine for lead loss
&lt;/h5&gt;&lt;h6 id=&#34;deep-supervision&#34;&gt;Deep supervision
&lt;/h6&gt;&lt;p&gt;深度监督作为一个训练技巧于2014年在&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1409.5185&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Deeply-Supervised Nets&lt;/a&gt;提出来，深度监督又称中继监督，在深度神经网络的某些中间隐藏层加了一个辅助的分类器作为一种网络分支来对主干网络进行监督，目的是为了浅层能够得到更加充分的训练，解决梯度消失和收敛速度过慢的问题。&lt;/p&gt;
&lt;p&gt;如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/6.png&#34; style=&#34;zoom:70%;&#34; /&gt;
&lt;h5 id=&#34;label-assignment&#34;&gt;Label assignment
&lt;/h5&gt;&lt;p&gt;首先简单介绍一下Label assignment，在监督学习中，计算loss，需要有预测结果和标签，在目标检测中最后输出的是框，需要把一些有价值的输出和GT匹配，简单的做法是人工定义规则，例如当生成的矩形框和GT的矩形框之间的IoU大于多少，就认为是目标，小于多少就认为是背景。&lt;/p&gt;
&lt;p&gt;YOLOv7中将负责最终输出的head为lead head，用于辅助训练head称为auxiliary head。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;近年来，研究者经常利用网络预测输出的质量和分布，并结合GT考虑，使用一些计算和优化方法来生成可靠的软标签，例如目标检测中，YOLO使用边界框的回归预测和GT的IoU作为客观的软标签。YOLOv7中，将网络预测结果与GT一起考虑。将结合模型预测结果和GT来获取软标签的模块称为“label assigner”。&lt;/p&gt;
&lt;p&gt;新的问题是：如何将软标签分配给auxiliary head和lead head，目前常用的方法是将auxiliary head和lead head分开，然后使用各自的结果和GT执行标签分配。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/8.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv7中，通过lead head预测来引导auxiliary head和lead  head，使用lead head预测作为指导，生成由粗到细的层次标签，分别用于auxiliary head和lead head的学习。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/9.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;具体地，lead head的预测结果和GT为依据生成软标签，但生成两个不同集合的软标签，粗粒度和细粒度，粗粒度标签是通过放松约束条件来让更多网格当作正样本，专注于提高auxiliary head的召回能力，lead head的输出结果可以从高召回的结果中过滤高准确的结果作为最终输出。&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
