title: 特征工程
author: meurice
date: 2020-07-18 11:42:42
tags:
---
## 前言　　
　　数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。
## 特征工程
　　特征工程是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用，简单来说，就是通过X，创造新的X'，目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系，其本质是一个表示和展现数据的过程。基本的操作包括，衍生（升维），筛选（降维）等。  
　　例如某分类器接收身高、体重两个参数来判断这个人是否肥胖，仅通过体重无法判断某个人的胖瘦，对于该例，一个非常经典的特征工程是，BMI指数，BMI=体重/(身高^2)，通过BMI指数，可以清晰地对一个人的胖瘦进行刻画。  

## 数据预处理
　　常见的数据可分为结构化数据（例如关系型数据库的表）和非结构化数据（文本、图像、音频、视频等）。
### 单特征
#### 标准化与归一化
　　该部分可以参考[数据预处理——归一化与标准化](http://meurice.xyz/2020/ckcqevh3t0004xclxakyx24ma/)。
#### 缺失值
##### 均值/中位数/众数/固定值填充
　　如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。  
　　或可根据某一特征对样本进行分类/聚合后（例如船运GPS数据，根据运单号进行聚合后，对样本数据缺失值进行填充），根据同类其他样本该属性的均值补全缺失值，同上述方法类似。
　　对于缺失值也可以采用固定的数值来进行填充。
##### 建模预测
　　将缺失值字段作为预测对象，建立模型对其进行预测，根据该模型补全原训练集的缺失值。这个方法根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但若模型对预测字段拟合效果相当好，则说明这个缺失属性没必要纳入数据集；一般的情况是介于两者之间。
##### 高维映射
　　将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。  
　　这种做法既保留了所有的信息，也未添加任何额外信息，但会增加数据的维度，增大了计算量，一般在样本量非常大时效果才比较好。
##### 其他
　　如多重插补、压缩感知和矩阵补全等，此处不具体展开，可以参考[这篇文章](https://mp.weixin.qq.com/s/BnTXjzHSb5-4s0O0WuZYlg)。
#### 特征二值化
　　 设立阈值，将特征二值化。
　　![erzhihua](https://wx1.sbimg.cn/2020/07/18/ClGSk.png)
　　可以类比将模拟信号转换成数字信号过程中的量化。
  ```Python
  X_ = preprocessing.Binarizer(threshold=0).transform(X)
  ```
#### 哑编码/独热编码
　　哑编码/独热编码针对定性的特征进行处理。
##### 哑编码(dummy encoding)
　　假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。  
　　例如描述一个人的身材，我们可以用偏瘦、正常、偏胖，这些描述词经过哑编码就会得到：  
　　　　偏廋 —> [1, 0, 0]  
　　　　正常 —> [0, 1, 0]  
　　　　偏胖 —> [0, 0, 1]
  ```Python
  X_ = pd.Categorical(df['c']).codes
  ```
##### 独热编码(one-hot encoding)
　　同上例，实际用2个状态位就足够反应上述3个类别的信息：  
　　　　偏廋 —> [1, 0]  
　　　　正常 —> [0, 1]  
　　　　偏胖 —> [0, 0]  
  ```Python
  encoder=OneHotEncoder(sparse=False) 
  # sparse是一个布尔值，指定结果是否稀疏。
  # 若sparse=True，则每个样本的独热码为一个稀疏矩阵。
  ```
  <br>
　　关于哑编码/独热编码的区别和联系以及连续值的离散化提升模型的非线性能力的原因，可以参考[这篇文章](https://www.cnblogs.com/lianyingteng/p/7792693.html)。
### 多特征
#### 特征选择
　　数据预处理完成后，需要选择有意义的特征输入机器学习的算法和模型进行训练，一般从以下两个方面考虑：  
　　· 特征是否发散（某特征不发散，说明对于区分样本作用并不大）  
　　· 特征与目标的相关性  
  
　　特征选择主要包括：  
　　· Filter Method （过滤式）  
　　· Wrapper Method （包装式）  
　　· Embedded Method （嵌入式）
##### 特征选择原理
　　·去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度
　　
##### Filter
　　过滤式方法先对数据集进行特征选择，然后再训练模型，**特征选择过程与后续模型训练无关**。  
　　通过统计学的方法对每个feature给出一个score，通过score对特征进行排序，然后选取score最高的子集.。这种方法仅仅对每个feature进行**独立考虑**，没有考虑到feture之间的依赖性或相关性。  
###### 方差选择法
　　计算各个特征的方差，根据阈值，**选择方差大于阈值的特征**。即若样本中该特征差异并不大，则认为该特征对于区分样本贡献不大，故可以将其去掉。
  ```Python
  from sklearn.feature_selection import SelectKBest
  
  VarianceThreshold(threshold=0).fit_transform(data)
  ```
###### 相关系数法
　　计算各个特征对目标值的相关系数以及相关系数的P值。
  ```Python
  from scipy.stats import pearsonr
  from sklearn.feature_selection import VarianceThreshold
  
  SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, 
              	k=4).fit_transform(data, target)
  # 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。（在此定义为计算相关系数）
  # 参数k为选择的特征个数，选择k个最好的特征，返回选择特征后的数据
  ```
###### 卡方检验
　　经典的卡方检验是**检验定性自变量对定性因变量的相关性**，是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，如果卡方值越大，二者偏差程度越大；反之，二者偏差越小；若两个值完全相等时，卡方值就为0，表明理论值完全符合。  
　　假设自变量有N种取值，因变量有M种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距。
　　![x2](https://wx1.sbimg.cn/2020/07/19/Cyng1.png)
  ```Python
  from sklearn.feature_selection import SelectKBest
  from sklearn.feature_selection import chi2 
  # 选择k个最佳特征
  SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
  ```
###### 互信息法
　　互信息(Mutual Information)是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。  
　　经典的互信息**评价定性自变量对定性因变量的相关性**。  
　　设两个随机变量(X, Y)的联合分布为p(x, y)，边缘分布分别为p(x), p(y)，互信息I(X, Y)是联合分布p(x, y)与边缘分布p(x)p(y)的相对熵，即：
![mutual info](https://wx1.sbimg.cn/2020/07/19/CV0Ek.png)  
 　　关系图：
![mutual](https://wx2.sbimg.cn/2020/07/19/CVofa.png)
  ```Python
  from sklearn.feature_selection import SelectKBest
  from minepy import MINE

  # 定义mic方法将MINE设为函数式的，返回一个二元组，二元组的第2 项设置成固定的P值0.5
  def mic(x, y):
  	m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)

  SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T,
  				k=2).fit_transform(iris.data, iris.target)
  ```
##### Wrapper
　　包裹式特征选择直接把最终将要使用的模型的性能作为特征子集的评价标准，即包裹式特征选择的目的就是为给定的模型选择最有利于其性能的特征子集。从最终模型的性能来看，包裹式特征选择比过滤式特征选择更好，但需要多次训练模型，计算开销较大。
![filter mutual](https://wx2.sbimg.cn/2020/07/19/CVCpn.png)
###### 递归特征消除法
　　递归特征消除法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。
  ```Python
  from sklearn.feature_selection import RFE
  from sklearn.linear_model import LogisticRegression

  # 此处选择LR为基模型(estimator)
  RFE(estimator=LogisticRegression(), n_features_to_select=4).fit_transform(data, target)
  ```
##### Embedded
　　在前两种特征选择方法中，特征选择过程和模型训练过程是有明显分别的两个过程。嵌入式特征选择是**将特征选择过程与学习器训练过程融为一体**，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。例如岭回归(Ridge)、LASSO回归。常利用正则化，如L1，L2范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。  
　　若使用L2范数正则化，则此时优化目标的公式即为岭回归(ridge regression)，若是L1范数正则化，则是LASSO回归(Least Absolute Shrinkage and Selection Operator)。L1范数和L2范数正则化都有助于降低过拟合风险，但前者还会带来一个额外的好处，它比后者更易于获得稀疏解，即它求得的w会有更少的非零分类。换言之，采用L1范数比L2范数更易于得到稀疏解。（参考[机器学习（六）：特征选择方法—Filter,Wrapper,Embedded](https://zhuanlan.zhihu.com/p/120924870)）
###### 基于惩罚项的特征选择法
　　使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。  
　　带L1惩罚项的LR：
  ```
  from sklearn.feature_selection import SelectFromModel
  from sklearn.linear_model import LogisticRegression
 
  SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(data, target)
  ```
###### 基于树模型的特征选择法
　　GBDT作为基模型
  ```Python
  from sklearn.feature_selection import SelectFromModel
  from sklearn.ensemble import GradientBoostingClassifier

  SelectFromModel(GradientBoostingClassifier()).fit_transform(data, target)
  ```