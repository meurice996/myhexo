<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>zn.yan</title>
        <link>https://demo.stack.jimmycai.com/</link>
        <description>Recent content on zn.yan</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 20 Dec 2022 16:29:46 +0000</lastBuildDate><atom:link href="https://demo.stack.jimmycai.com/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>深大网上评教的批量自动勾选脚本</title>
        <link>https://demo.stack.jimmycai.com/p/%E6%B7%B1%E5%A4%A7%E7%BD%91%E4%B8%8A%E8%AF%84%E6%95%99%E7%9A%84%E6%89%B9%E9%87%8F%E8%87%AA%E5%8A%A8%E5%8B%BE%E9%80%89%E8%84%9A%E6%9C%AC/</link>
        <pubDate>Tue, 20 Dec 2022 16:29:46 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E6%B7%B1%E5%A4%A7%E7%BD%91%E4%B8%8A%E8%AF%84%E6%95%99%E7%9A%84%E6%89%B9%E9%87%8F%E8%87%AA%E5%8A%A8%E5%8B%BE%E9%80%89%E8%84%9A%E6%9C%AC/</guid>
        <description>&lt;p&gt;在评教界面下审查元素，在控制台提交如下所示的JavaScript代码，即可全选5分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/wspj-auto-js.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;btn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;document&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;getElementsByClassName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;bh-choice-helper&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;btn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;nx&#34;&gt;btn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;click&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>图神经网络</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Wed, 30 Nov 2022 11:01:19 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;h1 id=&#34;graph-neural-network&#34;&gt;Graph Neural Network
&lt;/h1&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;h3 id=&#34;graph&#34;&gt;Graph
&lt;/h3&gt;&lt;p&gt;图$G$=点集$N$+边集$E$&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/graph_sample.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;why&#34;&gt;Why?
&lt;/h3&gt;&lt;h4 id=&#34;classification&#34;&gt;Classification
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;根据分子结构预测其性质&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原子用节点表示，化学键用边表示。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/classfication.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;考虑角色之间的关系来预测一个人是不是凶手&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/classification2.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;h4 id=&#34;generation&#34;&gt;Generation
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Generator生成出想要的分子结构&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/generation.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h3 id=&#34;how&#34;&gt;How?
&lt;/h3&gt;&lt;p&gt;对于一个无标签的数据，如何利用仅有的有标签数据和它与其他节点的关系？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solution1：类似CNN的方法，CNN利用卷积来获取一张图片的特征图，在卷积的乘法、加法过程中，利用了&lt;em&gt;邻近&lt;/em&gt;像素点的信息。&amp;raquo; &lt;strong&gt;Spatial-based convolution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Solution2：将信号转到Fourier domain，在&lt;em&gt;频域&lt;/em&gt;上对信号和滤波器做相乘，再做傅立叶反变换。 &amp;raquo; &lt;strong&gt;Spectral-based convolution&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gnn-roadmap&#34;&gt;GNN Roadmap
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/roadmap.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h2 id=&#34;tasks-dataset-and-benchmark&#34;&gt;Tasks, Dataset, and Benchmark
&lt;/h2&gt;&lt;h2 id=&#34;spatial-based-gnn&#34;&gt;Spatial-based GNN
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Aggregate：用邻居的feature更新下一层的hidden state&lt;/li&gt;
&lt;li&gt;Readout：所有节点的feature集合起来代表整个图&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/spatial-based%20convolution.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;nn4g-neural-networks-for-graph&#34;&gt;NN4G (Neural Networks for Graph)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/4773279&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neural Network for Graphs: A Contextual Constructive Approach | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如输入的图有5个节点$v_{0\dots5}$，5条边，每个节点都有自己的feature，例如对于某种化学物质，将每个节点（原子）的性质作为这个节点的feature，。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/input_layer.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;首先需要对每个节点的feature做embeding，得到$h_{0\dots 5}^0$，得到Hidden layer 0。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/hidden0.png&#34; style=&#34;zoom:33%;&#34; /&gt;
$$
h_3^1=\hat{w}_{1,0}(h_0^0+h_2^0+h_4^0)+h_3^0
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/hidden1.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在做Readout时，将各层的节点feature加起来取平均，各自做一个transform后加起来代表整个图的feature。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/readout.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;dcnn-diffusion-convolution-neural-network&#34;&gt;DCNN (Diffusion Convolution Neural Network)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1511.02136&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1511.02136] Diffusion-Convolutional Neural Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
h_3^1=w_3^1MEAN(d(3,\cdot)=2)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/DCNN.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
$$
\begin{bmatrix}
h_1^k \\
\vdots \\
h_1^1 \\
h_1^0
\end{bmatrix}
\times
W=y_1
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/DCNN1.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;dgc-diffusion-graph-convolution&#34;&gt;DGC (Diffusion Graph Convolution)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1707.01926.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1707.01926.pdf (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与DCNN类似，把各层加起来。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/DGC.png&#34; style=&#34;zoom:30%;&#34; /&gt;
&lt;h3 id=&#34;monet-mixture-model-networks&#34;&gt;MoNET (Mixture Model Networks)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1611.08402&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1611.08402] Geometric deep learning on graphs and manifolds using mixture model CNNs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
\mathbf{u}(x,y)=(\frac{1}{\sqrt{\deg(x)}},\frac{1}{\sqrt{\deg{y}}})^T
$$&lt;p&gt;将距离$\mathbf{u}$（与节点的度有关）做一个transform后，再做加权求和。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/MoNET.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;graphsage&#34;&gt;GraphSAGE
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.02216&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1706.02216] Inductive Representation Learning on Large Graphs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在做Aggregation时，使用mean、max-pooling或LSTM。&lt;/p&gt;
&lt;p&gt;对一个节点来说，邻居是无序的，用LSTM来做时，每次update时都随便sample出一个顺序，来忽略顺序的影响，学到比较好表示。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/GraphSAGE.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h3 id=&#34;gat-graph-attention-networks&#34;&gt;GAT (Graph Attention Networks)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1710.10903&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1710.10903] Graph Attention Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不只要做加权求和，权重让网络自己去学，对邻居做Attention。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：节点feature $\mathbf{h}={\vec{h}_1,\vec{h}_2,\dots,\vec{h}_N},\vec{h}_i\in\mathbb{R}^F$&lt;/li&gt;
&lt;li&gt;计算energy：$e_{ij}=\alpha(\mathbf{W}\vec{h}_i,\mathbf{W}\vec{h}_j)$&lt;/li&gt;
&lt;li&gt;通过邻居计算Attention分数：$\alpha_{ij}=\frac{\exp{(\text{LeakyReLU}(\vec{\mathbf{a}}^T[\mathbf{W}\vec{h}_i\Vert\mathbf{W}\vec{h}&lt;em&gt;j]))}}{\sum&lt;/em&gt;{k\in \mathcal{N}_i}\exp{(\text{LeakyReLU}(\mathbf{\vec{a}}^T[\mathbf{W}\vec{h}_i\Vert\mathbf{W}\vec{h}_k])})}$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/GAT.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;gin-graph-isomorphism-network&#34;&gt;GIN (Graph Isomorphism Network)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=ryGs6iA5Km&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How Powerful are Graph Neural Networks? | OpenReview&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
h_v^{(k)}=\text{MLP}^{(k)}\left(\left(1+\epsilon^{(k)}\right)\cdot h_v^{(k-1)}+\sum_{u\in\mathcal{N}(v)}h_{u}^{(k-1)}\right)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/GIN.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;graph-signal-processing--spectral-based-gnn&#34;&gt;Graph Signal Processing &amp;amp; Spectral-Based GNN
&lt;/h2&gt;&lt;h3 id=&#34;signal-and-system&#34;&gt;Signal and System
&lt;/h3&gt;&lt;p&gt;时域上的卷积等于频域上的相乘。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/freq.png&#34; style=&#34;zoom:33%;&#34; /&gt;
$$
\vec{A}=\sum_{k=1}^Na_k\hat{v}_k
$$$$
a_j=\vec{A}\cdot \hat{v}_j
$$$$
\hat{v}_i\cdot\hat{v}_j=\delta_{ij}
$$$$
x(t)=\sum_{k=-\infty}^{\infty}a_ke^{jk\omega_0t}=\sum_{k=-\infty}^{\infty}a_k\phi_k(t)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/time-freq.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;傅立叶变换：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/fourier-trans.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;h3 id=&#34;spectral-graph-theory&#34;&gt;Spectral Graph Theory
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;图$G=(V,E)$，节点数量$N=\vert V\vert$；&lt;/li&gt;
&lt;li&gt;邻接矩阵（权重矩阵）$A\in \mathbb{R}^{N\times N}$：$A_{i,j}=0\ \text{if}\ e_{i,j}\notin E\ \text{else}\ A_{i,j}=w(i,j)$&lt;/li&gt;
&lt;li&gt;度矩阵$D\in \mathbb{R}^{N\times N}$：&lt;/li&gt;
&lt;/ul&gt;
$$
D_{i,j}=\left\{
\begin{aligned} 
&amp;d(i)\ \ (\sum_kA_{i,k})\quad &amp;\text{if}\ i=j \\
&amp;0\quad &amp;\text{if}\ i\ne j
\end{aligned}\right.
$$&lt;ul&gt;
&lt;li&gt;$f:V\rightarrow \mathbb{R}^N$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于下所示的图，有节点$v_{0\dots 3}$，每个节点上面的信号$f(i)$，例如对于城市路网图，节点对应不同的城市，信号对应着城市的气温、人口等。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory1.png&#34; style=&#34;zoom:30%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;图拉普拉斯矩阵$L=D-A,L\succcurlyeq0$（半正定，对称）&lt;/li&gt;
&lt;li&gt;可以对$L$做谱分解：$L=U\Lambda U^T$&lt;/li&gt;
&lt;li&gt;对角矩阵$\Lambda=\text{diag}(\lambda_0,\dots,\lambda_{N-1})\in \mathbb{R}^{N\times N}$，$\lambda_l$：frequency&lt;/li&gt;
&lt;li&gt;正交矩阵$U=[u_0,\dots,u_{N-1}]\in \mathbb{R}^{N\times N}$，$u_l$：$\lambda_l$对应的basis&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/graph-laplacian.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h4 id=&#34;vertex-domain-signal&#34;&gt;Vertex domain signal
&lt;/h4&gt;$$
D=
\begin{bmatrix}
2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 
\end{bmatrix}
\quad
A=
\begin{bmatrix}
0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 
\end{bmatrix}
$$$$
L=
\begin{bmatrix}
2 &amp; -1 &amp; -1 &amp; 0 \\
-1 &amp; 3 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 2 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 1
\end{bmatrix}
$$$$
\Lambda=
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
\quad
U=
\begin{bmatrix}
0.5 &amp; -0.41 &amp; 0.71 &amp; -0.29 \\
0.5 &amp; 0 &amp; 0 &amp; 0.87 \\
0.5 &amp; -0.41 &amp; -0.71 &amp; -0.29 \\
0.5 &amp; 0.82 &amp; 0 &amp; -0.29
\end{bmatrix}
$$&lt;p&gt;
对应在图$G$上：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory2.png&#34; style=&#34;zoom:34%;&#34; /&gt;
&lt;h4 id=&#34;discrete-time-fourier-basis&#34;&gt;Discrete time Fourier basis
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/discrete-time-fourier-basis.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;interpreting-vertex-frequency&#34;&gt;Interpreting vertex frequency
&lt;/h4&gt;&lt;p&gt;把$L$看成是对图的一个运算，对于一个图信号$f$，$Lf=(D-A)f=Df-Af$，对于4个节点的图，会得到形如$Lf=\begin{bmatrix}a \ b \c \ d\end{bmatrix}$的结果，$Lf$代表着某一个节点和他周围节点的能量差异。&lt;/p&gt;
$$
\begin{aligned}
(Lf)(vi)&amp;=\sum_{v_j\in V}w(i,j)(f(v_i)-f(v_j)) \\
&amp;\text{where}\ w_{i,j}\ \text{is the}\ (i,j)^{th}\ \text{entry of}\ A \\
f^TLf&amp;=\sum_{v_i\in V}f(v_i)\sum_{v_j\in V}w_{i,j}(f(v_i)-f(v_j)) \\
&amp;=\cdots \\
&amp;=\frac{1}{2}\sum_{v_i\in V}\sum_{v_j\in V}w_{i,j}\left(f(v_i)-f(v_j)\right)^2
\end{aligned}
$$&lt;p&gt;
可以用$f^TLf$来量化某一个图信号的频率。&lt;/p&gt;
&lt;p&gt;有$u_i^TLu_i=u_i^T\lambda_iu_i=\lambda u_i^Tu_i=\lambda_i$，此时再回到前面$\lambda$和$u$对应在图$G$上的情况，比较小的$\lambda$对应着低通部分。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory2.png&#34; style=&#34;zoom:34%;&#34; /&gt;
&lt;p&gt;一个特例是一条线上的20个节点$v_{0\dots19}$：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/special-example.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在$\lambda=0$时是一个DC-component，随着$\lambda$越来越大，从一个低频sin变成高频sin。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/special-example-2.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h4 id=&#34;graph-fourier-transform-of-signal-x&#34;&gt;Graph Fourier Transform of signal $x$
&lt;/h4&gt;$$
\hat{x}=U^Tx
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/transform.png&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;分别乘内积，将各个节点的$f(i)$转为$u_i\cdot f=\lambda_i$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/transform2.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;h4 id=&#34;inverse-graph-fourier-transform-of-signal-hatx&#34;&gt;Inverse Graph Fourier Transform of signal $\hat{x}$
&lt;/h4&gt;$$
x=U\hat{x}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/inverse.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;filtering&#34;&gt;Filtering
&lt;/h4&gt;&lt;p&gt;前面将时域转到频域，是为了找到在图上做滤波的方式。&lt;/p&gt;
&lt;p&gt;例如对于频域上的信号$X(j\omega)$，有一个$H(j\omega)$的滤波，直接相乘会得到滤波结果$X^\prime(j\omega)$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering.png&#34; style=&#34;zoom:33%;&#34; /&gt;
$$
\hat{y}=g_\theta(\Lambda)\hat{x}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering2.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
$$
g_\theta(\lambda_i)=\theta_i
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering3.png&#34; style=&#34;zoom:33%;&#34; /&gt;&lt;/p&gt;
$$
\begin{aligned}
y=U\hat{y}&amp;=Ug_\theta(\Lambda)U^T \\
&amp;=g_\theta(U\Lambda U^T)x \\
&amp;=g_\theta(L)x
\end{aligned}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/filtering4.png&#34; style=&#34;zoom:38%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;训练模型需要学习的参数是$g_\theta(\Lambda)$。&lt;/p&gt;
$$
g_\theta(L)=\log(I+L)=L-\frac{L^2}{2}+\frac{L^3}{3}+\cdots,\lambda_{\max}\lt 1
$$&lt;p&gt;
此时存在的问题是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算的复杂度为$O(N)$&lt;/li&gt;
&lt;li&gt;$g_\theta(L)$可能会让模型可能会学到一些本不希望学到的内容（not &lt;em&gt;localize&lt;/em&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;localize&#34;&gt;Localize
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/theory1.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;p&gt;对于上图，例如$g_\theta(L)=L$时，$y=Lx$：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/q-L1.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;p&gt;$L$在有些地方（例如1行4列）的值为0，这是因为$v_0$到$v_3$之间没有任何距离为1的路径，此时$v_3$不会影响到$v_0$。&lt;/p&gt;
&lt;p&gt;但当$g_\theta(L)=L^2$时，$y=L^2X$：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/q-L2.png&#34; style=&#34;zoom:35%;&#34; /&gt;
&lt;p&gt;$L^2$不再有为值为0的地方，对于任意节点，到其他节点的距离都小于2，此时$v_3$会影响到$v_0$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt; Let $G$ be a weighted graph, and $\mathscr{L}$ the graph Laplacian of $G$. Fix an integer $s\gt 0$, and pick vertices $m$ and $n$. Then $(\mathscr{L}^s)&lt;em&gt;{m,n}=0$ whenever $d&lt;/em&gt;{G}(m,n)\gt s$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这种情况不是Localize的，例如CNN中，当我们选择$3\times 3$的kernel时，就只能看到邻近区域的内容，希望这个区域变大的话可以选择更大的kernel。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/GNN/CNN-localize.png&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h3 id=&#34;chebnet&#34;&gt;ChebNet
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1606.09375&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1606.09375] Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多可参考：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://distill.pub/2021/gnn-intro/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Gentle Introduction to Graph Neural Networks (distill.pub)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>长短期记忆网络</title>
        <link>https://demo.stack.jimmycai.com/p/%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Thu, 27 Oct 2022 16:37:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Understanding LSTM Networks &amp;ndash; colah&amp;rsquo;s blog&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;long-short-term-memory&#34;&gt;Long Short-Term Memory
&lt;/h1&gt;&lt;h2 id=&#34;rnn&#34;&gt;RNN
&lt;/h2&gt;&lt;p&gt;传统神经网络不能做到根据前文来理解现在的内容，循环神经网络（Recurrent Neural Network, RNN）解决了这个问题，RNN的结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-rolled.png&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;对于神经网络$A$，接收输入$x_t$，输出$h_t$，其中的循环使得信息可以向后传递，RNN与普通神经网络的区别并不是很大，可以认为是同一网络的多个副本，每个副本将一条信息传递给后续的网络，其展开后的结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-unrolled.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;p&gt;链式结构表明RNN与序列数据密切相关。&lt;/p&gt;
&lt;h2 id=&#34;长程依赖问题&#34;&gt;长程依赖问题
&lt;/h2&gt;&lt;p&gt;RNN的亮点在于，其能够将以前的信息和当前的任务连接起来，例如要预测“the clouds are in the &lt;em&gt;[MASK]&lt;/em&gt;”，RNN会比较容易地通过前文预测出这个词是“&lt;em&gt;sky&lt;/em&gt;”，要预测的位置和相关信息距离很近，RNN可以学会使用过去的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-shorttermdepdencies.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;p&gt;下面考虑另一种情况，例如要预测“I grew up in France, &amp;hellip;, I speak fluent &lt;em&gt;[MASK]&lt;/em&gt;”，附近的信息表明这个词可能是某种语言的名称，但是如果要预测出“&lt;em&gt;French&lt;/em&gt;”，需要从获取更远位置的信息，然而，随着预测位置和相关信息的距离增大，RNN会难以使用过去的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/RNN-longtermdependencies.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;h2 id=&#34;lstm&#34;&gt;LSTM
&lt;/h2&gt;&lt;p&gt;长短期记忆网络（Long Short-Term Memory, LSTM）是一种特殊的RNN，能够学习到长期的依赖性。&lt;/p&gt;
&lt;p&gt;标准RNN中，重复模块具有非常简单的结构，例如单个tanh层：
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-SimpleRNN.png&#34; style=&#34;zoom:36%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;LSTM同样具有类似的链式结构，重复模块中有4个交互层：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-chain.png&#34; style=&#34;zoom:36%;&#34; /&gt;
&lt;p&gt;上图中，每条线携带一个完整的向量，从一个节点的输出到其他节点的输入，粉色圆圈代表逐点（point-wise）操作（例如向量加法），黄色框是神经网络层，行合并表示对矩阵的串联（concatenation）连接，行分叉表示内容被复制且副本转到不同的位置。&lt;/p&gt;
&lt;h2 id=&#34;lstm的核心思想&#34;&gt;LSTM的核心思想
&lt;/h2&gt;&lt;p&gt;相比RNN只有一个传递状态$h_t$，LSTM有两个传递状态，$C_t$（cell state）和$h_t$（hidden state）。&lt;/p&gt;
&lt;p&gt;cell state（顶部的水平线）是LSTM的关键，cell state只有一些微小的线性作用，改变很慢。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-C-line.png&#34; style=&#34;zoom:42%;&#34; /&gt;
&lt;p&gt;LSTM通过调整门控结构来选择性让信息通过。&lt;/p&gt;
&lt;p&gt;门由Sigmoid层与按位乘法操作（pointwise multiplication operation）组成。&lt;/p&gt;
&lt;h2 id=&#34;lstm的工作方式&#34;&gt;LSTM的工作方式
&lt;/h2&gt;&lt;h3 id=&#34;遗忘阶段&#34;&gt;遗忘阶段
&lt;/h3&gt;&lt;p&gt;LSTM的第一步是由遗忘门控$\sigma$决定要从cell state中丢弃哪些信息，输入$h_{t-1}$和$x_t$，并对cell state中的每一个元素输出一个0到1之间的数字，1代表完全保留而0代表完全遗忘。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-f.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
f_t=\sigma (W_f\cdot [h_{t-1},x_t]+b_f)
$$
&lt;h3 id=&#34;选择记忆阶段&#34;&gt;选择记忆阶段
&lt;/h3&gt;&lt;p&gt;LSTM的第二步是决定cell state要保留哪些信息，输入门控$\sigma$决定要更新哪些值，$\tanh$创建一个新候选值组成的向量$\widetilde{C}_t$，并加入到状态中。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-i.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
\begin{align}
i_t&amp;=\sigma(W_i\cdot [h_{t-1},x_t]+b_i)\\
\widetilde{C}_t&amp;=\tanh(W_C\cdot[h_{t-1},x_t]+b_C)
\end{align}
$$
至此，我们可以将cell state从$C_{t-1}$更新为$C_t$。将旧的cell state与$f_t$相乘，将决定遗忘的信息丢弃，然后添加$i_t*\widetilde{C}_t$。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-C.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
C_t=f_t*C_{t-1}+i_t*\widetilde{C}_t
$$
&lt;h3 id=&#34;输出阶段&#34;&gt;输出阶段
&lt;/h3&gt;&lt;p&gt;LSTM的最后一步是决定要输出什么，这个输出基于cell state。首先经过输出门控$\sigma$，决定我们要输出cell state的哪些部分，然后将cell state通过$\tanh$，并与输出门控的输出相乘，就得到了决定输出的部分$h_t$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/LSTM/LSTM3-focus-o.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
\begin{align}
o_t&amp;=\sigma(W_o\cdot[h_{t-1},x_t]+b_o) \\
h_t&amp;=o_t *\tanh(C_t)
\end{align}
$$</description>
        </item>
        <item>
        <title>图像质量评价</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/</link>
        <pubDate>Sun, 31 Jul 2022 22:24:28 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/</guid>
        <description>&lt;p&gt;本文总结了图像质量评估的相关内容，同时简要总结了基于深度学习的无参考图像质量评价方法。&lt;/p&gt;
&lt;h1 id=&#34;image-quality-assessment&#34;&gt;Image Quality Assessment
&lt;/h1&gt;$$
\text{QA}\left\{
\begin{aligned}
&amp;视频质量评估\text{VQA} \\
&amp;图像质量评估\text{IQA}
\left\{
\begin{aligned}
&amp;主观评估\text{S-IQA}\ ——\ 主观评分\\
&amp;客观评估\text{O-IQA}
\left\{
\begin{aligned}
&amp;全参考评估\text{FR-IQA} \\
&amp;半参考评估\text{RR-IQA} \\
&amp;无参考评估\text{NR-IQA} \\
\end{aligned}
\right.
\end{aligned}
\right.
\end{aligned}
\right.
$$&lt;h2 id=&#34;主观评估s-iqa&#34;&gt;主观评估S-IQA
&lt;/h2&gt;&lt;p&gt;主观评估方法主要可分为两种：绝对评价和相对评价。&lt;/p&gt;
&lt;p&gt;绝对评价是由观察者根据自己的知识和理解，按照某些特定评价性能对图像的绝对好坏进行评价。在具体执行过程中通常采用双刺激连续质量分级法（Double Stimulus Continuous Scale, DSCQS）将待评价图像和原始图像按一定规则交替播放持续一定时间给观察者，然后在播放后留出一定的时间间隔供观察者打分，最后将所有给出的分数取平均作为该序列的评价值。&lt;/p&gt;
&lt;p&gt;相对评估中没有原始图像作为参考，是由观察者对一批待评价图像进行相互比较，从而判断出每个图像的优劣顺序，并给出相应的评价值。在具体执行过程中通常采用单刺激连续质量评价方法（Single Stimulus Continuous QualityEvaluation, SSCQE）将一批待评价图像按照一定的序列播放，此时观察者在观看图像的同时给出待评图像相应的评价分值。&lt;/p&gt;
&lt;p&gt;平均意见得分（Mean Opinion Score, MOS）是图像质量最具代表性的主观评价方法，它通过对观察者的评价归一判断图像质量。类似的评价方式还有平均主观得分差异（Differential mean opinion score, DMOS）。&lt;/p&gt;
&lt;p&gt;绝对评价尺度：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;分数&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;质量尺度&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;妨碍尺度&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;丝毫看不出图像质量变坏&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;能看出图像质量变化但不妨碍观看&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;清楚看出图像质量变坏，对观看稍有妨碍&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一般&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;对观看有妨碍&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常严重的妨碍观看&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常差&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;相对评价尺度的评分标准：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;分数&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;相对测量尺度&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;绝对测量尺度&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一群中最好的&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;好于该群中平均水平&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;好&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;该群中的平均水平&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一般&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;差于该群中平均水平&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;差&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;该群中最差的&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;非常差&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;客观评估o-iqa&#34;&gt;客观评估O-IQA
&lt;/h2&gt;&lt;h3 id=&#34;评价指标&#34;&gt;评价指标
&lt;/h3&gt;&lt;p&gt;检验一种客观评估算法是否可靠的标准是它“是否与人的主观质量判断相一致”，为了确认某种客观评价指标与主观得分之间的一致性关系，常用四个指标：RMSE、PLCC、SROCC和KROCC。&lt;/p&gt;
&lt;h4 id=&#34;rmse&#34;&gt;RMSE
&lt;/h4&gt;$$
\text{RMSE}=\sqrt{\frac{\sum_{i=1}^n(s_i-p_i)^2}{n}}
$$&lt;p&gt;
&lt;strong&gt;RMSE越接近0，表示算法的性能越好&lt;/strong&gt;。为了避免MOS取值不一样而导致RMSE的计算受影响，所以计算前需要归一化。&lt;/p&gt;
&lt;h4 id=&#34;plcc皮尔逊系数&#34;&gt;PLCC（皮尔逊系数）
&lt;/h4&gt;$$
\text{PLCC}=\frac{Cov(S,P)}{\sigma(S)\cdot\sigma(P)}=\frac{\sum_{i=1}^n(p_i-\bar{p})(s_i-\bar{s})}{\sqrt{\sum_{i=1}^n(p_i-\bar{p})^2}\cdot\sqrt{\sum_{i=1}^n(s_i-\bar{s})^2}}
$$&lt;p&gt;PLCC取值范围为$[-1,1]$，当PLCC的值为零时，表示两组数据完全不相关，&lt;strong&gt;PLCC的值大于0时表示正相关，值越大表示正相关性越强&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;srocc&#34;&gt;SROCC
&lt;/h4&gt;&lt;p&gt;SROCC和KROCC将具体数值抽象为排序等级。&lt;/p&gt;
$$
\text{SROCC}=1-\frac{6\sum_{i=1}^nd_i^2}{n(n^2-1)} \tag{1}
$$&lt;p&gt;
其中$d$为$S$和$P$的等级之差（$rank(s)-rank(p)$）。&lt;/p&gt;
&lt;p&gt;SROCC计算过程可参考下例：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$S$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$P$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$rank(s)$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$rank(p)$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$d$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$d^2$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;56&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;66&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;9&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;25&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;75&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;70&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;45&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;40&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;10&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;10&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;71&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;60&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;7&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;9&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;62&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;65&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;6&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;64&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;56&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;9&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-4&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;16&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;58&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;59&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;8&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;8&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;80&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;77&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;76&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;67&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;-1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;61&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;63&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;7&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;6&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;带入$d^2$的值即可求得SROCC。&lt;/p&gt;
$$
\text{SROCC}=\frac{\sum_{i=1}^n(kp_i-\bar{kp})(ks_i-\bar{ks})}{\sqrt{\sum_{i=1}^n(kp_i-\bar{kp})^2}\cdot\sqrt{\sum_{i=1}^n(ks_i-\bar{ks})^2}}\tag{2}
$$&lt;p&gt;其中$kp$、$ks$分别表示$rank(s)$、$rank(p)$。&lt;/p&gt;
&lt;p&gt;例如，对于两个第二名，则将等级定位1.5（第一和第二的平均）；对于两个第三名，则将等级定位3.5。&lt;/p&gt;
&lt;p&gt;可以看出，SROCC就是“等级”的PLCC。&lt;/p&gt;
&lt;h4 id=&#34;krocc&#34;&gt;KROCC
&lt;/h4&gt;&lt;p&gt;将MOS和算法预测得分表示为数据对的形式：$(s_1,p_1),(s_2,p_2),\cdots,(s_n,p_n)$，从$n$个数据对中任选两对，组成$[(x_i,y_i),(x_j,y_j)]$（$i\neq j$），一共有$\frac{n(n+1)}{2}$对，将这些对按照下面的情况进行划分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P$：$x_i\gt x_j\ and\ y_i\gt y_j$ 或 $x_i\lt x_j\ and\ y_i\lt y_j$ 同序对&lt;/li&gt;
&lt;li&gt;$Q$：$x_i\gt x_j\ and\ y_i\lt y_j$ 或 $x_i\lt x_j\ and\ y_i\gt y_j$ 逆序对&lt;/li&gt;
&lt;li&gt;$X_0$：$x_i=x_j\ and\ y_i\gt y_j$ 或 $x_i=x_j\ and\ y_i\lt y_j$&lt;/li&gt;
&lt;li&gt;$Y_0$：$x_i\gt x_j\ and\ y_i=y_j$ 或 $x_i\lt x_j\ and\ y_i=y_j$&lt;/li&gt;
&lt;li&gt;$XY_0$：$x_i=x_j\ and\ y_i=y_j$&lt;/li&gt;
&lt;/ul&gt;
$$
\text{KROCC}=\frac{P-Q}{\sqrt{P+Q+X_0}\cdot\sqrt{P+Q+Y_0}}
$$&lt;h3 id=&#34;全参考评估&#34;&gt;全参考评估
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;MSE&lt;/li&gt;
&lt;li&gt;PSNR&lt;/li&gt;
&lt;li&gt;SSIM&lt;/li&gt;
&lt;li&gt;VIF&lt;/li&gt;
&lt;li&gt;FSIM&lt;/li&gt;
&lt;li&gt;GMSD&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mse&#34;&gt;MSE
&lt;/h4&gt;$$
\text{MSE}=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2
$$&lt;h4 id=&#34;psnr峰值信噪比&#34;&gt;PSNR（峰值信噪比）
&lt;/h4&gt;$$
\text{PSNR}=10\lg\left(\frac{MAX_I^2}{MSE}\right)
$$&lt;p&gt;
其中，$MAX_I$为图片可能的最大像素值，例如对于8 bit存储的图像，$MAX_I=2^8-1=255$。&lt;/p&gt;
&lt;h4 id=&#34;ssim&#34;&gt;SSIM
&lt;/h4&gt;&lt;p&gt;SSIM首先在文章Image Quality Assessment: From Error Visibility to Structural Similarity（IEEE-2004）被引入，作者提出两个要点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大多数图像质量评估技术依赖于量化参考图像和样本图像之间的误差。一种常用的度量是量化样本和参考图像之间对应的每个像素的值的差异（例如均方误差）。&lt;/li&gt;
&lt;li&gt;人类视觉感知系统能够从一个场景中识别结构信息，从而识别从参考场景和样本场景中提取的信息之间的差异。因此，复制此行为的指标将在涉及区分样本图像和参考图像的任务中执行得更好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SSIM从一幅图像中提取3个关键特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  \mu_x=\frac{1}{N}\sum_{i=1}^Nx_i
  $$&lt;p&gt;
其中$x_i$为图像$x$的第$i$个像素值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
$$
  \sigma_x=\sqrt{\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_x)^2}
  $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;结构：通过一个合并公式来完成&lt;/p&gt;
$$
  r(X,Y)=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}
  $$$$
  r=\frac{\sigma_{xy}}{\sigma_x\sigma_y}
  $$$$
  \sigma_{xy}=\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_x)(y_i-\mu_y)
  $$&lt;/li&gt;
&lt;/ul&gt;
$$
l(x,y)=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}
$$$$
c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}
$$$$
s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3}
$$$$
SSIM(x,y)=[l(x,y)]^\alpha\cdot[c(x,y)]^\beta\cdot [s(x,y)]^\gamma
$$&lt;p&gt;
$\alpha,\beta,\gamma$用来表示三个模块的重要性。&lt;/p&gt;
$$
SSIM(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}
$$&lt;h3 id=&#34;半参考评估&#34;&gt;半参考评估
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;RRED&lt;/li&gt;
&lt;li&gt;OSVP&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rred&#34;&gt;RRED
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Reduced reference entropic differencing for image quality assessment（IEEE Trans. Image Process，2012）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;osvp&#34;&gt;OSVP
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Orientation selectivity based visual pattern for reduced-reference image quality assessment（Information Science，2016）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;无参考评估&#34;&gt;无参考评估
&lt;/h3&gt;&lt;p&gt;由于没有无失真源图像的参考信息，无参考质量评估方法仅根据失真图像来学习预测图像质量分数，难度大于全参考和部分参考评估方法。&lt;/p&gt;
&lt;h4 id=&#34;传统图像清晰度评价算法&#34;&gt;传统图像清晰度评价算法
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Tenengrad梯度函数&lt;/li&gt;
&lt;li&gt;SMD（灰度方差）函数&lt;/li&gt;
&lt;li&gt;Brenner梯度函数&lt;/li&gt;
&lt;li&gt;方差函数&lt;/li&gt;
&lt;li&gt;能量梯度函数&lt;/li&gt;
&lt;li&gt;Vollath函数&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reblur&#34;&gt;Reblur
&lt;/h4&gt;&lt;p&gt;如果一幅图像已经模糊了，那么再对它进行一次模糊处理，高频分量变化不大；但如果原图是清楚的，对它进行一次模糊处理，则高频分量变化会非常大。因此可以通过对待评测图像进行一次高斯模糊处理，得到该图像的退化图像，然后再比较原图像和退化图像相邻像素值的变化情况，根据变化的大小确定清晰度值的高低，计算结果越小表明图像越清晰，反之越模糊，这种思路可称作基于二次模糊的清晰度算法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NRSS（梯度结构相似度）&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;nrss&#34;&gt;NRSS
&lt;/h5&gt;&lt;p&gt;NRSS算法的步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step1. 为待评价图像构造参考图像：定义待评价图像为$I$，NRSS算法首先参考图像$I_r=LPF(I)$，即对待评价图像$I$进行低通滤波得到参考$I_r$；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step2. 提取图像$I$和$I_r$的梯度信息：利用人眼对水平和竖直方向的边缘信息最为敏感的特性，使用Sobel算子分别提取水平和竖直方向的边缘信息，定义$I$和$I_r$的梯度图像是$G$和$G_r$；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step3. 找出梯度图像$G$中梯度信息最丰富的$N$个图像块：通过计算方差找出梯度图像$G$中梯度信息最丰富的$N$个图像块，方差越大说明梯度信息越丰富，根据找到的$G$中的前$N$个块，找出对应的$G_r$的前$N$个块；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
$$
  \text{NRSS}=1-\frac{1}{N}\sum_{i=1}^NSSIM(x_i,y_i)
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;distortion-specific&#34;&gt;Distortion specific
&lt;/h4&gt;&lt;p&gt;早期的传统方法通过假设存在特定某一类型的失真来评价图像质量，即量化特定失真类型，如块效应、模糊、振铃效应、噪声、压缩或传输损伤等。JNBM、CPBDM和LPCM专注于评价Blur类型的失真图像，NJQA和JPEG-NR分别评价噪声失真和JPEG压缩损伤失真。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPBDM&lt;/li&gt;
&lt;li&gt;LPCM&lt;/li&gt;
&lt;li&gt;NJQA&lt;/li&gt;
&lt;li&gt;JPEG-NR&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;nss&#34;&gt;NSS
&lt;/h4&gt;&lt;p&gt;近年来表现优良的无参考图像质量评估模型大部分都是基于自然场景统计特性 (Natural Scene Statistics, NSS)，在不对失真类型做任何假设的前提下设计提取图像特征，通过机器学习回归算法进行质量预测。所选特征具有广泛的感知相关性，且合适的回归模型能自适应地将特征映射到数据集中的主观质量分数，因此基于NSS特征的无参考图像质量评估方法比早期的模型更加通用和一般化。NSS表明经过适当规范化的高质量真实世界摄像图像会遵行一定的统计规律，基于NSS统计量的特征量更能准确预测图像失真。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BRISQUE&lt;/li&gt;
&lt;li&gt;GM-LOG&lt;/li&gt;
&lt;li&gt;HIGRADE&lt;/li&gt;
&lt;li&gt;FRIQUEE&lt;/li&gt;
&lt;li&gt;VBLINDS[V]&lt;/li&gt;
&lt;li&gt;VIDEVAL[V]&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;brisque&#34;&gt;BRISQUE
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;No-Reference Image Quality Assessment in the Spatial Domain（IEEE Trans. Image Process，2012）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BRISQUE是首歌将图像的自然场景统计特性应用到图像质量评估上的模型。其思想是从图像中提取MSCN系数（mean subtracted contrast normalized, 均值减去对比度归一化），将MSCN系数拟合成非对称性广义高斯分布（AGGD），提取拟合的高斯分布的特征，输入到支持向量机SVM中做回归，从而得到图像质量的评估结果。&lt;/p&gt;
&lt;p&gt;自然图像的像素强度分布与失真图像的像素强度分布不同。当我们对像素强度进行归一化并在这些归一化强度上计算分布时，分布上的差异更加明显。特别地，在归一化之后，自然图像的像素强度近似服从高斯分布（贝尔曲线），而非自然或失真图像的像素强度则不服从高斯分布。因此，分布曲线与理想高斯曲线的偏差是图像失真量的度量。&lt;/p&gt;
&lt;p&gt;BRISQUE的整体流程有三步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 提取自然场景统计信息（NSS）&lt;/li&gt;
&lt;li&gt;Step2. 计算特征向量&lt;/li&gt;
&lt;li&gt;Step3. 预测图像质量分数&lt;/li&gt;
&lt;/ul&gt;
$$
\hat{I}(i,j)=\frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}
$$$$
\mu=\mathbf{W}*\mathbf{I}\quad\sigma=\sqrt{\mathbf{W}*(\mathbf{I}-\mu)^2}
$$$$
\begin{aligned}
H(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i,j+1)} \\
V(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i+1,j)} \\
D1(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i+1,j+1)} \\
D2(i,j)&amp;=\hat{H}(i,j)\cdot\hat{H(i+1,j-1)} \\
\end{aligned}
$$&lt;p&gt;
至此，已从原始图像中生成了5张图像：1张MSCN图像和4张成对乘积图像。&lt;/p&gt;
&lt;p&gt;接下来，我们将使用这5张图像来计算大小为$36\times1$的特征向量。&lt;/p&gt;
$$
\begin{aligned}
f(x;\alpha,\sigma^2)=\frac{\alpha}{2\beta\Gamma(1/\alpha)}\exp\left(-\left(\frac{|x|}{\beta}\right)^2\right)
\end{aligned}
$$$$
\beta=\sigma\sqrt{\frac{\Gamma(1/\alpha)}{\Gamma(3/\alpha)}},\quad\Gamma(a)=\int_0^\infty t^{a-1}e^{-t}dt\quad (a\gt0).
$$$$
f(x;\nu,\sigma_l^2,\sigma_r^2)=
\left\{
\begin{aligned}
&amp;\frac{\nu}{(\beta_l+\beta_r)\Gamma(1/\nu)}\exp\left(-\left(\frac{-x}{\beta_l}\right)^\nu\right)\quad x\lt 0 \\
&amp;\frac{\nu}{(\beta_l+\beta_r)\Gamma(1/\nu)}\exp\left(-\left(\frac{-x}{\beta_r}\right)^\nu\right)\quad x\geqslant 0
\end{aligned}
\right.
$$$$
\beta_l=\sigma_l\sqrt{\frac{\Gamma(1/\nu)}{\Gamma(3/\nu)}},\quad \beta_r=\sigma_r\sqrt{\frac{\Gamma(1/\nu)}{\Gamma(3/\nu)}}.
$$&lt;p&gt;
将原始图像缩小到原始大小的一半，并重复相同过程，便得到了$36\times1$的特征向量。&lt;/p&gt;
&lt;p&gt;将特征向量送入到机器学习算法中进行训练，即可使用模型对质量分数进行预测。&lt;/p&gt;
&lt;p&gt;用广义高斯分布来拟合MSCN的分布，GGD的形状参数α和分布方差sigma。接下来对MSCN系数进行二阶分析，在垂直、水平、主对角和次对角方向上进行非对称广义高斯分布的拟合，分别得到表征分布形状的四个参数。这样在原图像尺度上得到18个特征。图像和视频本质上是多尺度的，失真可以在不同的尺度上表现得不同。因此在降采样2倍的图像上再次提取18维度的特征，这样BRISQUE的特征集是36维。通过机器学习训练出能够从高维特征映射到低维MOS分数上的回归模型。&lt;/p&gt;
&lt;h4 id=&#34;bag-of-words&#34;&gt;Bag of words
&lt;/h4&gt;&lt;p&gt;不同于以上基于NSS特征提取模型，传统无参考图像质量评估的另一个方向是词袋 (Bag of Words, BOW) 模型。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CORNIA&lt;/li&gt;
&lt;li&gt;HOSA&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;cornia&#34;&gt;CORNIA
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Unsupervised Feature Learning Framework for No-reference Image Quality Assessment（IEEE Conf. Comput. Vis. Pattern Recognit.，2012）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;completely-blind&#34;&gt;Completely blind
&lt;/h4&gt;&lt;p&gt;Completely Blind方法不需要在数据集上进行训练来学习特征到MOS分数的映射，而是能够通过待测图像或者视频直接输出得到质量分数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NIQE&lt;/li&gt;
&lt;li&gt;IL-NIQE&lt;/li&gt;
&lt;li&gt;SLEEQ[V]&lt;/li&gt;
&lt;li&gt;STEM[V]&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;niqe&#34;&gt;NIQE
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Making a &amp;ldquo;Completely Blind&amp;rdquo; Image Quality Analyzer（IEEE Signal Process，2013）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NIQE（natural image quality evaluator）是Mittal等人提出的基于自然场景统计的盲图像质量评价模型。该方法仅利用从自然图像中观察到的统计规律进行失真偏差的度量，通过构建一系列质量相关的统计特征以实现对图像的质量预测。&lt;/p&gt;
&lt;p&gt;NIQE算法有以下几个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. Spatial Domain NSS：提取NSS特征&lt;/li&gt;
&lt;li&gt;Step2. Patch Selection：图像划分、选择&lt;/li&gt;
&lt;li&gt;Step3. Characterizing Image Patches：提取Patches的特征&lt;/li&gt;
&lt;li&gt;Step4. Multivatiate Gaussian Model（MVG）：拟合多元高斯分布&lt;/li&gt;
&lt;li&gt;Step5. NIQE Index：计算NIQE分数&lt;/li&gt;
&lt;/ul&gt;
$$
\hat{I}(i,j)=\frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}
$$&lt;p&gt;
其中，$i \in 1,2,\dots,M,j\in 1,2,\dots,N$，$M,N$是图像的高和宽，$C=1$是为了数值稳定的常数；$\omega={ \omega_{k,l}\vert k=-K,\dots,K,l=-L,\dots,L}$是高斯核。&lt;/p&gt;
$$
\delta(b)={\sum\sum}_{(i,j)\in patch_b}\sigma(i,j) \quad b=1,2,\dots,P\times P
$$&lt;p&gt;
设定阈值$T$，若$\delta(b)\gt T$，则认为$patch_b$是锐利的。将所有图像块的最大锐利程度的 $p$倍设为阈值，其中$p \in [0.6, 0.9]$，论文中取值为0.75。将大于阈值的图像块保留，小于阈值的图像块淘汰掉。&lt;/p&gt;
&lt;p&gt;选取一些patches后，类似于BRISQUE中的方法，在不同尺度下拟合GGD和AGGD得到36维特征。&lt;/p&gt;
$$
f_X(x_1,\dots,x_k)=\frac{1}{(2\pi)^{k/2}\vert\Sigma\vert^{1/2}}\exp(-\frac{1}{2}(x-\nu)^T\Sigma^{-1}(x-\nu))
$$&lt;p&gt;
注：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这个模型是由一组清晰图像得到的，用来计算低质量图像与它的距离；&lt;/li&gt;
&lt;li&gt;采用高斯分布来处理这些特征的基本前提是假设这里所涉及的特征在真实的图像中所反映的也是服从高斯分布的。&lt;/li&gt;
&lt;/ul&gt;
$$
D(\nu_1,\nu_2,\Sigma_1,\Sigma_2)=\sqrt{(\nu_1-\nu_2)^T(\frac{\Sigma_1+\Sigma_2}{2})^{-1}(\nu_1-\nu_2)}
$$&lt;p&gt;
由上式即可得出最终得分，$\nu_1,\Sigma_1$是一组清晰图像得到的均值向量和协方差矩阵，$\nu_2,\Sigma_2$是输入的图像得到的均值向量和协方差矩阵。&lt;/p&gt;
&lt;h4 id=&#34;handcraft&#34;&gt;Handcraft
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;TLVQM[V]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;deep-learning&#34;&gt;Deep learning
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;VSFA[V]&lt;/li&gt;
&lt;li&gt;VMEON[V]&lt;/li&gt;
&lt;li&gt;PVQ[V]&lt;/li&gt;
&lt;li&gt;RAPIQUE[V]&lt;/li&gt;
&lt;li&gt;CNNIQA&lt;/li&gt;
&lt;li&gt;PaQ-2-PIQ&lt;/li&gt;
&lt;li&gt;Hallucinated-IQA&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;nr-iqa中的deep-based方法&#34;&gt;NR-IQA中的Deep-based方法
&lt;/h1&gt;&lt;h2 id=&#34;score-based&#34;&gt;Score-based
&lt;/h2&gt;$$
\text{Score-based}\left\{
\begin{aligned}
&amp;\text{Patch-wise} \\
&amp;\text{Image-wise}
\end{aligned}
\right.
$$&lt;p&gt;Score-based方法可以分为Patch-wise和Image-wise。Image-wise直接将图片输入到CNN中；Patch-wise对图像进行分块并分别输入CNN，pooling后得到图像质量。&lt;/p&gt;
&lt;h3 id=&#34;iqa-cnnpatchcnn直接预测质量&#34;&gt;IQA-CNN（patch/CNN/直接预测质量）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;L. Kang, P. Ye, Y. Li, and D. Doermann, “Convolutional neural networks for no-reference image quality assessment,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IQA-CNN是首个将空间卷积神经网络模型在图像质量评价领域应用的工作，IQA-CNN以patch作为输入，由一层卷积、最大最小池化以及两层全连接组成，将特征学习和回归集成到一个优化过程中。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/4.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;dliqanssdbn预测分类置信度&#34;&gt;DLIQA（NSS/DBN/预测分类+置信度）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;W. Hou, X. Gao, D. Tao, and X. Li, “Blind image quality assessment via deep learning,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 6, pp. 1275–1286, Jun. 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;心理学表明人类更加偏向于定性的评价而不是定量的评价，一幅图片打分为70还是75实际上是很难抉择的问题，用excellent、good、bad这样的语言定性更加自然。&lt;/p&gt;
&lt;p&gt;DLIQA将BIQA（Bind Image Quality Assessment）作为一个五分类问题，输入图像用NSS特征表示，分类的依据是分类器得到的置信度，然后将分类和对应的置信度转为质量分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/5.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;作者认为IQA问题并不适合用切成patch的方式来做，但直接输入整张图像维度太高，所以使用了可以表征自然图像和畸变图像的差异的NSS特征作为输入。&lt;/p&gt;
&lt;p&gt;为了将分类结果和置信度转化为质量分数，首先有以下假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每张图片都有一个内在质量$Q$；&lt;/li&gt;
&lt;li&gt;每个训练有素的人在评估具有相同内在质量的图像时，都会给出相同的标签。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主观评分的似然分布$P(L=\text{Excellent|Q}),P(L=\text{Good}|Q),P(L=\text{Fair}|Q),P(L=\text{Poor}|Q),P(L=\text{Bad}|Q)$和先验分布$P(Q)$如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/6.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;然后通过三角形分布函数和平均分布来模拟各个似然函数和先验分布：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/11.png&#34; style=&#34;zoom:67%;&#34; /&gt;
$$
P(Q|L)\sim P(L|Q)P(Q)
$$$$
P(Q|X)=\int P(Q|L)P(L|X)dL
$$$$
\text{Quality}=\mathbb{E}[P(Q|X)]
$$&lt;h3 id=&#34;deepiqapatchcnn预测质量权重&#34;&gt;Deepiqa（patch/CNN/预测质量+权重）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;S. Bosse, D. Maniry, T. Wiegand, and W. Samek, “A deep neural network for image quality assessment,” IEEE International Conference on Image Processing, 2016.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deepiqa将无任何预处理的图像patch作为输入，通过池化获得最终的质量分数，网络的输出定义为2维，其中一维输出图像patch的质量分数的对应权重，通过该权重得到最终的质量分数。&lt;/p&gt;
&lt;h3 id=&#34;bieconpatchcnnfr中间图&#34;&gt;BIECON（patch/CNN/FR中间图）
&lt;/h3&gt;&lt;p&gt;使用部分全参考图像质量评价方法中的局部质量图作为中间结果对模型进行训练。&lt;/p&gt;
&lt;h3 id=&#34;diqam-nrwadiqam-nrpatchcnn预测质量权重&#34;&gt;DIQaM-NR/WaDIQaM-NR（patch/CNN/预测质量+权重）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;S. Bosse, D. Maniry, K.-R. M ̈uller, T. Wiegand, and W. Samek, &amp;ldquo;Deep neural networks for no-reference and full-reference image quality assessment,&amp;rdquo; IEEE Transactions on image processing, vol. 27, no. 1, pp. 206–219, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8063957&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;以上分别为FR和NR模型的结构，FR模型去掉孪生网络的分支就是NR模型。&lt;/p&gt;
&lt;p&gt;FR模型的两个输入（distorted和reference）通过一个共享参数的VGG-Style网络，得到两个特征向量$f_r,f_d$，然后将$\text{concat}(f_r,f_d,f_r-f_d)$用于回归，一个网络用于回归质量，另一个用于回归权重，pooling后得到最终的图像质量。&lt;/p&gt;
&lt;h3 id=&#34;nimaimagecnn预测主观分数分布&#34;&gt;NIMA（image/CNN/预测主观分数分布）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;H. Talebi, P. Milanfar, &amp;ldquo;NIMA: neural image assessment,&amp;rdquo; IEEE Trans. Image Process., vol. 27, no. 8, pp. 3998-4011, Aug. 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不同于常见的深度图像质量评价方法，NIMA对人类主观分数的分布$\hat{\mathbf{p}}$进行预测。&lt;/p&gt;
$$
\mathbf{p}=[p_{s_1},\dots, p_{s_N}]\quad\sum_{i=1}^Np_{s_i}=1,s_1\leqslant s_i \leqslant s_N
$$&lt;p&gt;
其中$s_i$表示第$i$个分值，$N$表示分值总数。&lt;/p&gt;
$$
\mu=\sum_{i=1}^Ns_i\cdot p_{s_i}
$$$$
\sigma=\sqrt{\sum_{i=1}^N(s_i-\mu)^2\cdotp_{s_i}}
$$&lt;p&gt;
NIMA中CNN后接的全连接为10维（AVA和TID数据集中均有$N=10$）。&lt;/p&gt;
$$
EMD(\mathbf{p},\hat{\mathbf{p}})=\left(\frac{1}{N}\sum_{k=1}^N|CDF_\mathbf{p}(k)-CDF_{\hat{\mathbf{p}}}(k)|\right)^\frac{1}{r}
$$&lt;p&gt;
其中累计分布函数$CDF_\mathbf{p}(k)=\sum_{i=1}^k p_{s_i}$。&lt;/p&gt;
&lt;h3 id=&#34;hallucinated-iqaimagegan生成幻觉图像感知差异&#34;&gt;Hallucinated-IQA（image/GAN/生成“幻觉”图像感知差异）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Lin, G. Wang, “Hallucinated-IQA: no-reference image quality assessment via adversarial learning” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IQA在失真图像的基础上产生一个“幻觉”参考图像，通过捕捉失真图像和“幻觉”图像之间的感知差异来预测图像质量。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Hallucinated-IQA模型包括生成网络$G$、IQA判别网络$D$和回归网络$R$三个部分，结构如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
$$
\hat{\theta}=\arg\min_\theta \frac{1}{N}\sum_{i=1}^N(l_p(G_\theta(I_d^i), I_r^i)+l_s(G_\theta(I_d^i),I_r^i))
$$$$
l_s(G_\theta(I_d^i),I_r^i)=\Vert\phi(G_\theta(I_d^i))-\phi(I_r^i)\Vert^2
$$$$
l_s(G_\theta(I_d^i),I_r^i)=\lambda_1l_v(G_\theta(I_d^i),I_r^i)+\lambda_2l_q(G_\theta(I_d^i),I_r^i)
$$$$
\begin{aligned}
l_v&amp;=\sum_{c_v=1}^{C_v}\frac{1}{W_jH_j}\sum_{x=1}^{W_j}\sum_{y=1}^{H_j}\Vert\phi_j(G_\theta(I_d^i))_{x,y}-\phi_j(I_r^i)_{x,y}\Vert^2 \\
l_q&amp;=\sum_{c_q=1}^{C_q}\frac{1}{W_kH_k}\sum_{x=1}^{W_k}\sum_{y=1}^{H_k}\Vert\pi_k(G_\theta(I_d^i))_{x,y}-\pi_k(I_r^i)_{x,y}\Vert^2
\end{aligned}
$$&lt;p&gt;
其中$C$表示某一层的特征图数量，$W,H$表示特征图的维度，$\phi_j(\cdot)$表示VGG-19在第$j$层的特征图，$\pi_k(\cdot)$表示$R$在第$k$层的特征图。&lt;/p&gt;
$$
\max_\omega\mathbb{E}[\log D_\omega(\mathbf{I_r})]+\mathbb{E}[\log(1-\vert D_\omega(G_\theta(\mathbf{I}_d))-\mathbf{d}_{fake}\vert)]
$$&lt;p&gt;
其中，$\mathbf{d}&lt;em&gt;{fake}$定义为：
$$
\mathbf{d}_{fake}^i=\left\{
\begin{aligned}
&amp;1\quad \text{if}\ \Vert R(I_d^i,I_{sh}^i)-s^i\Vert_F\lt \epsilon\\
&amp;0\quad \text{if}\ \Vert R(I_d^i,I_{sh}^i)-s^i\Vert_F\geqslant \epsilon
\end{aligned}
\right.
$$
该式所表达的是，当回归网络预测出来的分数与真实质量分数的差距大于阈值时，认为“幻觉”图像降低了回归网络的性能，$\mathbf{d}&lt;/em&gt;{fake}$为0，此时为了最大化损失函数，需要IQA判别网络将生成图像判别为0，真实图像判别为1；当回归网络预测出来得分数与真实质量分数得差距小于阈值时，幻觉图像提升了回归网络的性能，$\mathbf{d}_{fake}$为1，此时为了最大化损失函数，需要辨别器将生成图像和真实图像均辨别为1。、&lt;/p&gt;
$$
\mathcal{L}_{adv}=\mathbb{E}[\log(1-D_\omega(G_\theta(I_d)))]
$$$$
\mathcal{L}_G=\mu_1\mathcal{L}_p+\mu_2\mathcal{L}_s+\mu_3\mathcal{L}_{adv}
$$$$
\hat{\gamma}=\arg\min_r\frac{1}{N}\sum_{i=1}^Nl_r(\mathcal{R}(I_d^i,I_{map}^i),s^i)
$$&lt;p&gt;
其中$I_{map}=\vert I_d-G_{\hat{\theta}}(I_d)\vert$。&lt;/p&gt;
&lt;p&gt;$R$的精确度在很大程度上取决于“幻觉”场景的合格性。具体地说，合格的幻觉图像作为代理参照可以帮助$R$探索失真图像的感知差异，而不合格的“幻觉”图像则会引入对$R$的偏差。&lt;/p&gt;
$$
\mathcal{F}=f(\mathcal{H}_{5,2}(I_d))\otimes(\mathcal{R}_1(I_d,I_{map}))
$$$$
\mathcal{L_R}=\frac{1}{T}\sum_{t=1}^T\Vert\mathcal{R}_2(f(\mathcal{H}_{5,2}(I_d))\otimes (\mathcal{R}_1(I_d,I_{map})))-s^t\Vert_{\ell_1}
$$&lt;h3 id=&#34;ran4iqapatchgan&#34;&gt;RAN4IQA（patch/GAN）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;H. Ren, D. Chen, Y. Wang, “RAN4IQA: Restorative Adversarial Nets for No-reference Image Quality Assessment,” in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过生成式对抗网络对输入的失真图像进行修复，基于修复收益（gain of restoration, GoR）提取失真图像和修复图像的特征并进行比较以感知质量。&lt;/p&gt;
&lt;h3 id=&#34;sfapatch语义特征聚集&#34;&gt;SFA（patch/语义特征聚集）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;D. Li, T. Jiang, W. Lin, and M. Jiang, “Which has better visual quality: the clear blue sky or a blurry animal?,” IEEE Trans. Multimedia., vol. 21, no. 5, pp. 1221-1234, Nov. 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rank-based&#34;&gt;Rank-based
&lt;/h2&gt;&lt;h3 id=&#34;gao-et-al&#34;&gt;Gao et al
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;F. Gao, D. Tao, X. Gao, and X. Li, “Learning to rank for blind image quality assessment,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 10, pp. 2275–2290, Oct. 2015.&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1309.0213v2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1309.0213v2] Learning to Rank for Blind Image Quality Assessment (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Blind image quality assessment（BIQA）旨在预测图像质量分数，但图像质量分数的获取有一定的局限性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分数不精确；&lt;/li&gt;
&lt;li&gt;主观判断不准确；&lt;/li&gt;
&lt;li&gt;不同失真类别之间的质量尺度不一致；&lt;/li&gt;
&lt;li&gt;大规模数据难以获取。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者提出基于偏好图像对（preference image pair, PIPs），对于一个偏好图像对而言，其标签表示的是图像一的质量好于图像二的质量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/9.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;dipiq&#34;&gt;DipIQ
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Ma, W. Liu, T. Liu, Z. Wang and D. Tao, &amp;ldquo;dipIQ: blind image quality assessment by learning-to-rank discriminable image pairs,&amp;rdquo; IEEE Trans. Image Process., vol. 26, no. 8, pp. 3951-3963, Aug. 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rankiqa&#34;&gt;RankIQA
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;X. Liu, J. V. D. Weijer, and A. D. Bagdanov, “RankIQA: learning from rankings for no-reference image quality assessment,” in Proc. IEEE Int. Conf. Comput. Vis. 2017.&lt;/li&gt;
&lt;li&gt;X. Liu, J. V. D. Weijer, and A. D. Bagdanov, “Exploiting unlabeled data in cnns by self-supervised learning to rank,” IEEE Trans. Pattern. Anal. Mach. Intell., vol. 41, no. 8, pp. 1862-1878, Aug. 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lfma&#34;&gt;LFMA
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Ma, X. Liu, Y. Fang, and E. P. Simoncelli, “Blind image quality assessment by learning from multiple annotators,” IEEE International Conference on Image Processing, 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multi-task&#34;&gt;Multi-task
&lt;/h2&gt;&lt;h3 id=&#34;biqi&#34;&gt;BIQI
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;A. K. Moorthy, A. C. Bovik, “A two-step framework for constructing blind image quality indices,” IEEE Signal Process. Lett., vol. 17, no. 5, pp. 513-516, May. 2010.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iqa-cnn&#34;&gt;IQA-CNN++
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;L. Kang, P. Ye, Y. Li, and D. Doermann, “Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks,” in Proc. IEEE Int. Conf. Image Process., 2015, pp. 2791–2795.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mrliq&#34;&gt;MRLIQ
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;L. Xu, J. Li, W. Lin, Y. Zhang and L. Ma, Y. Fang, and Y. Yan, “Multi-task rank learning for image quality assessment,” IEEE Trans. Circuits Syst. Video Technol., vol. 27, no. 9, pp. 1833-1843, Sep. 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;meon&#34;&gt;MEON
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;K. Ma, W. Liu, T. Liu, K. Zhang, Z. Duanmu, Z. Wang, and W. Zuo, &amp;ldquo;End-to-end blind image quality assessment using deep neural networks,&amp;rdquo; IEEE Trans. Image Process., vol. 27, no. 3, pp. 1202-1213, Mar. 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MEON是用于BIQA（Bind Image Quality Assessment）多任务端到端优化的深度神经网络，包括失真判别网络和质量预测网络两部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;MEON的训练过程分为两个阶段：原始图像经过共享参数层，两个子网络一个用于失真类型判别，另一个用于质量预测，并且其中质量预测阶段使用了失真类型判别子网络的输出。&lt;/p&gt;
$$
y_i(m,n)=\frac{x_i(m,n)}{\left(\beta_i+\sum_{j=1}^S\gamma_{ij}x_j(m,n)^2\right)^\frac{1}{2}}
$$&lt;p&gt;
$y_i$是根据$x_i$在空间位置$(m,n)$的激活响应，$\beta$和$\gamma$在训练过程中进行优化，GDN操作是一个可微的变换。&lt;/p&gt;
&lt;h2 id=&#34;more&#34;&gt;More
&lt;/h2&gt;&lt;h3 id=&#34;ser-fiq&#34;&gt;SER-FIQ
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.09373&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.09373] SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SER-FIQ用于人脸图像质量评估，作者认为人脸图像的质量来自于图像的embedding的鲁棒性，同一张图片经过人脸识别模型的不同的子网络得到的不同的embedding的方差反映了人脸图像的质量。&lt;/p&gt;
$$
X(I)=\{x_s\},s\in 1,2,\dots,m
$$$$
q(X(I))=2\sigma\left(-\frac{2}{m^2}\sum_{i\lt j} d(x_i,x_j)\right)
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/10.png&#34; style=&#34;zoom:75%;&#34; /&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>YOLO v1 - v7</title>
        <link>https://demo.stack.jimmycai.com/p/yolo-v1-v7/</link>
        <pubDate>Thu, 14 Jul 2022 09:41:05 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/yolo-v1-v7/</guid>
        <description>&lt;p&gt;本文从Head、Backbone、Neck等方面介绍了YOLO的核心演变过程。&lt;/p&gt;
&lt;h1 id=&#34;yolov1---yolov7&#34;&gt;YOLOv1 - YOLOv7
&lt;/h1&gt;&lt;h2 id=&#34;yolov0&#34;&gt;YOLOv0
&lt;/h2&gt;&lt;p&gt;将目标检测任务当作是遍历性的分类任务，就可以使用分类器来完成检测。&lt;/p&gt;
&lt;p&gt;例如可以用一个框分别落在图片的各个区域，对框住的区域进行二分类，然后因为目标大小的不同，可能还需要使用不同大小的框来遍历图片，另外，为了提高检测精度，还可以使用滑动窗口的方法来遍历图像的所有位置&amp;hellip;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/2.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;R-CNN所采用的就是滑动窗口策略。&lt;/p&gt;
&lt;p&gt;而YOLO的思路是，将分类器的输出从一个One-hot向量换成$(c,x,y,w,h)$，直接输出这个框和置信度，将问题转化为一个回回归问题。要组织训练也比较简单，只需找很多的图片，标注框的位置，并将置信度$c$设为1，送入网络即可。&lt;/p&gt;
&lt;p&gt;以上便是YOLO最简单的版本。&lt;/p&gt;
&lt;h2 id=&#34;yolov1&#34;&gt;YOLOv1
&lt;/h2&gt;&lt;h3 id=&#34;head&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;在YOLOv0中，一张图片输出一个五元组$(c,x,y,w,h)$，只能输出一个目标。要输出多个目标，YOLOv1中采用的方式是将图片分为多个的网格（$S\times S$），每个网格单元对应一个五元组，就可以输出$S\times S$个框，如果目标的中心落在一个网格单元中，那么这个网格单元就会负责检测这个对象，如果网格单元中不存在目标，则置信度为0。&lt;/p&gt;
&lt;p&gt;例如当$S=4$时，输入一张图片：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;所返回的tensor为$(5,4,4)$，其中标签的置信度如下所示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;现在暂不考虑一个网格单元中存在多个目标的问题（上图3行3列），当得到$S\times S$个框后，YOLOv1中使用NMS（非极大值抑制）将目标所在的框保留下来而去除其它框。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 保留得分最高的框；&lt;/li&gt;
&lt;li&gt;Step2. 根据交并比，去掉与置信度最高的框重合度较高的框；&lt;/li&gt;
&lt;li&gt;Step3. 在剩下的框中继续选择得分最高的框，以此类推，直到没有剩下的框。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当要检测多个目标（$C$）时（例如既要检测葫芦娃的头，又要检测葫芦娃的葫芦），在返回的结果中多加一个二维的One-hot向量，此时输入一张图片后，返回的结果为：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;考虑到小目标检测效果不佳的问题，针对不同尺寸的目标，YOLOv1中为每个网格单元预测多个五元组（$B$），分别负责不同尺寸的目标的回归。&lt;/p&gt;
&lt;p&gt;以上，YOLO v1的预测结果为一个$S\times S\times(B*5+C)$的tensor，在YOLOv1中，$S=7$，$B=2$，$C=20$（PASCAL VOC数据集有20类），即每张图片的最终输出为$7\times 7\times 30$的tensor。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv1的整体结构如下（在ImageNet上训练时使用$224\times 224$的分辨率，检测时使用$448\times 448$。）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/7.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;YOLOv1对平方和误差（sum-squared error）进行优化，但它对定位误差和分类误差的权重相等，YOLOv1引入$\lambda_{coord}$和$\lambda_{noobj}$增加边界框坐标预测的损失，并减少不包含对象的框的置信度预测的损失，设$\lambda_{coord}=5,\lambda_{noobj}=.5$。此外，为了反映大检测框中的小偏差要比小检测框中的小偏差影响要小，YOLOv1对边界框宽度和高度的平方根进行预测，而不是直接预测宽度和高度。&lt;/p&gt;
&lt;h3 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
&amp;\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2] \\
+&amp;\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(\sqrt{w_i}-\sqrt{\hat{w}_i})^2+(\sqrt{h_i}-\sqrt{\hat{h}_i})^2] \\
+&amp;\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2 \\
+&amp;\lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2 \\
+&amp;\sum_{i=0}^{S^2}\mathbb{1}_i^{obj}\sum_{c\in classes}(p_i(c)-\hat{p}_i(c))^2
\end{aligned}
$$$$
\mathbb{1}_{ij}^{obj}=\left\{
\begin{aligned}
&amp;1,\quad第i个网格第j个\text{anchor box}负责预测这个物体 \\
&amp;0,\quad其他
\end{aligned}
\right.
$$&lt;h3 id=&#34;backbone&#34;&gt;Backbone
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv1/7.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;还是这张图，YOLOv1的backbone是参考GoogLeNet设计的，除最后两层全连接外，其它都大量使用了卷积层。下图是YOLOv4的论文中总结的检测类网络的结构：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/1.png&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;在YOLOv1中只有backbone，没有neck，属于Dense Prediction，一阶段检测器属于Dense Prediction，二阶段检测器既有Dense Prediction，又有Sparse Prediction。&lt;/p&gt;
&lt;h3 id=&#34;训练过程&#34;&gt;训练过程
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Step1. 在ImageNet上使用$224\times224$分辨率的图片训练分类器网络；&lt;/li&gt;
&lt;li&gt;Step2. 用$448\times448$分辨率端到端地训练目标检测网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yolov2--yolo9000&#34;&gt;YOLOv2 &amp;amp; YOLO9000
&lt;/h2&gt;&lt;h3 id=&#34;head-1&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;YOLOv1虽然检测速度快，但是定位不够准确，召回率较低，YOLOv2在YOLOv1的基础上提出了一些改进策略：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/1.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;h4 id=&#34;基于偏移量的边框回归&#34;&gt;基于偏移量的边框回归
&lt;/h4&gt;&lt;p&gt;同时代的R-CNN所预测的是偏移量，而YOLOv1直接预测框的坐标和大小，范围较大，YOLOv2中使用到了两种边框回归方式：基于anchor的偏移量和基于grid的偏移量。（anchor是R-CNN系列的一个概念，可以简单理解为一个预定义好的框，位置、宽高均已知，供预测时参考。）&lt;/p&gt;
&lt;p&gt;YOLOv2中的预测方式如下图所示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;其中，$b_x,b_y,b_w,b_h$是最终得到的检测结果，$t_x,t_y,t_w,t_h$是模型要预测的值，$c_x,c_y$是grid左上角的坐标，$p_w,p_h$是anchor的宽和高。YOLOv2基于anchor框的宽高和grid的先验位置预测的偏移量。&lt;/p&gt;
&lt;p&gt;例如对于下图，图片被分为9个grid，红色框为ground truth，紫色框为anchor：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\begin{aligned}
c_x&amp;=\frac{149}{149}=1\quad c_y=\frac{149}{149}=1 \\
b_x&amp;=\frac{230}{149}=1.543\quad b_y=\frac{218}{149}=1.463
\end{aligned}
$$$$
\begin{aligned}
b_x&amp;=\sigma(t_x)+c_x \\
b_y&amp;=\sigma(t_y)+c_y \\
b_w&amp;=p_we^{t_w} \\
b_h&amp;=p_he^{t_h}
\end{aligned}
$$$$
\begin{aligned}
t_x&amp;=\log\frac{1.543-1}{1-(1.543-1)}=0.172\\
t_y&amp;=\log\frac{1.463-1}{1-(1.463-1)}=-0.148 \\
t_w&amp;=\log\frac{224}{315}=-0.340 \\
t_h&amp;=\log\frac{202}{280}=-0.326
\end{aligned}
$$$$
\sigma(x)=\frac{1}{1+e^{-x}}\quad(\text{Sigmoid})
$$$$
125=5\times 5(c,x,y,w,h)+5\times 20\text{classes}
$$&lt;p&gt;
最后，关于每个网格中anchor个数的选择，YOLOv2的作者对VOC和COCO数据集的GT bounding box进行聚类，发现$k=5$时能够较好的平衡召回率和模型的复杂度。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv2/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;backbone-1&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;为了进一步提升性能，YOLOv2重新训练了一个DarkNet-19（下图双横线上方）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;VGGNet中得到一个结论，$7\times7$卷积可以用更小的卷积代替，且$3\times 3$卷积更加节约参数，使模型更小，且网络可以做得更深，因为能够引入更多的非线性的激活函数，能够更好地提取到特征。&lt;/p&gt;
&lt;p&gt;此外，还使用了bottleneck结构：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/3.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;使用$1\times1$的网络结构可以很方便的改变维度，灵活设计网络且减少计算量。&lt;/p&gt;
&lt;p&gt;同时在backbone中也没有FC层了，而是使用了GAP（Global Average Pooling）层，将$1000\times7\times7$映射为$1000\times1$，满足了不同尺度的输入图片的需求。并且针对较小的目标，只需把图片放大，就可以放大目标，提高检测精度。&lt;/p&gt;
&lt;p&gt;最后，由于backbone网络DarkNet-19是单独在ImageNet上训练的，所以最后加了softmax。&lt;/p&gt;
&lt;p&gt;完整的YOLOv2网络结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;reorg即passthrough layer，将$26\times26\times64$转为$13\times13\times256$，特征图大小减少4倍，通道数增加4倍，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/5.png&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;h3 id=&#34;训练过程-1&#34;&gt;训练过程
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Step1. 在ImageNet上训练DarkNet-19，输入为$224\times224$，训练160个epoch；&lt;/li&gt;
&lt;li&gt;Step2. 将网络输入调整为$448\times 448$（测试时使用$416\times416$），继续在ImageNet上finetune，训练10个epoch；（将$448\times448$改为$416\times416$，将创建奇数空间维度，对于一些大目标，其中心点往往落入图片的中心位置，使用特征图中心的1个网格单元去预测bounding box要更容易。）&lt;/li&gt;
&lt;li&gt;Step3. 修改DarkNet-19模型：移除最后的卷积层、GAP层和softmax层，新增3个$3\times3\times1024$的卷积层和1个passthrough层，最后用$1\times1$的卷积层输出预测结果，并在检测数据集上继续finetune网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yolov3&#34;&gt;YOLOv3
&lt;/h2&gt;&lt;h3 id=&#34;head-2&#34;&gt;Head
&lt;/h3&gt;&lt;p&gt;YOLOv2对于小目标的检测效果仍然不佳，在YOLOv3中，检测头分成了三部分：$13\times 13\times[3*(4+1+80)]$、$26\times 26\times[3*(4+1+80)]$和$52\times 52\times[3*(4+1+80)]$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv3/2.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;检测头的三个分支分别为32倍、16倍和8倍下采样，感受野由大变小，分别去预测大、中、小目标，对于每一个grid，共设置了9个先验框，大、中、小各3个，每个框预测一个五元组和80维的One-hot分类向量。三个特征图一共可以解码出$(52\times52+26\times26+13\times13)\times3=10647$个bounding box。&lt;/p&gt;
&lt;p&gt;YOLOv3仍然采用k-means聚类来确定先验框的个数，对于COCO数据集，9个聚类簇为$(10\times13),(16\times30),(33\times23),(30\times61),(62\times45),(59\times119),(116\times90),(156\times198),(373\times326)$。&lt;/p&gt;
&lt;p&gt;YOLOv3的网络结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv3/1.png&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;p&gt;YOLOv3使用多标签分类，使用多个独立的logistic分类器代替softmax，因为部分数据集中存在许多重叠的标签（Woman and Person），多标签方法可以建立更优的模型。&lt;/p&gt;
&lt;h3 id=&#34;训练策略&#34;&gt;训练策略
&lt;/h3&gt;&lt;p&gt;YOLOv3的训练策略非常重要，论文中的表述如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following. We use the threshold of .5. Unlike our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;预测框分为正例、负例和忽略样例三种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正例：任取一个ground truth，与10647个框计算IoU，IoU最大的预测框即为正例，下一个ground truth在余下的10646个检测框中寻找IoU最大的检测框作为正例。
&lt;ul&gt;
&lt;li&gt;正例产生置信度loss、检测框loss和类别loss；&lt;/li&gt;
&lt;li&gt;置信度标签为1，对应的类别标签为1（其余为0），预测框为$(t_x,t_y,t_w,t_h)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;负例：正例除外，与全部的ground truth的IoU都小于阈值（论文中为0.5）则为负例。（注：与ground truth计算IoU最大的检测框，即使IoU小于阈值，仍然为正例。）
&lt;ul&gt;
&lt;li&gt;负例仅产生置信度loss；&lt;/li&gt;
&lt;li&gt;置信度标签为0。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;忽略样例：正例除外，与任意一个ground truth的IoU大于阈值则为忽略样例。
&lt;ul&gt;
&lt;li&gt;忽略样例不产生任何loss。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;损失函数-1&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
loss_{N_1}&amp;=\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(t_{xi}-\hat{t}_{xi})^2+(t_{yi}-\hat{t}_{yi})^2] \\
&amp;+\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}[(t_{wi}-\hat{t}_{wi})^2+(t_{hi}-\hat{t}_{hi})^2] \\
&amp;+\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2 \\
&amp;+\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2 \\
&amp;-\sum_{i=0}^{S^2}\sum_{j=0}^B\mathbb{1}_{ij}^{obj}\sum_{c\in classes}[\hat{p}_i(c)\log(p_i(c))+(1-\hat{p}_i(c))\log(1-p_i(c))]
\end{aligned}
$$$$
Loss=loss_{N_1}+loss_{N_2}+loss_{N_3}
$$&lt;h3 id=&#34;backbone-2&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;YOLOv3使用的backbone为Darknet-53：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Darknet-53中依然采用了bottleneck结构，并添加了残差结构，但是没有池化层了，而是使用步长为2的卷积来替代池化层完成下采样的工作。&lt;/p&gt;
&lt;h3 id=&#34;neck&#34;&gt;Neck
&lt;/h3&gt;&lt;p&gt;相比YOLOv1和YOLOv2，YOLOv3增加了neck这一部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;YOLOv3的neck部分使用的是FPN（Feature Pyramid Networks），生成不同尺寸的图片，最后统计所有图片的预测结果。&lt;/p&gt;
&lt;p&gt;YOLOv3的backbone有$(13,13,1024),(26,26,512),(52,52,256)$三个输出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于$(13,13,1024)$的输出，经过5次DBL，输出$(13,13,512)$，然后一部分传到head中，另一部分经过DBL和上采样，得到$(26,26,256)$；&lt;/li&gt;
&lt;li&gt;对于$(26,26,512)$的输出，直接与上一步上采样后的结果concat，得到$(26,26,768)$，同时concat的结果经过5次DBL，得到$(26,26,256)$，一部分传入head中，另一部分经过DBL和上采样，得到$(52,52,128)$；&lt;/li&gt;
&lt;li&gt;对于$(52,52,256)$的输出，直接与上一步上采样后的结构concat，得到$(52,52,384)$，经过5次DBL后输入到head中。&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/9.jpg&#34; style=&#34;zoom:61%;&#34; /&gt;
&lt;h2 id=&#34;yolov4&#34;&gt;YOLOv4
&lt;/h2&gt;&lt;p&gt;YOLOv4在原有YOLO架构的基础上，采用了近年CNN领域中最优秀的优化策略，YOLOv4的文章如同一篇目标检测trick的综述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;head-3&#34;&gt;Head
&lt;/h3&gt;&lt;h4 id=&#34;iou-threshold&#34;&gt;IoU threshold
&lt;/h4&gt;&lt;p&gt;在YOLOv3中，1个anchor负责一个ground truth，而YOLOv4使用多个anchor负责一个ground truth，对于$GT_j$而言，只要$IoU(anchor_i,GT_j)\gt threshold$，就让$anchor_i$负责$GT_j$。&lt;/p&gt;
&lt;p&gt;该方法使得在anchor框数量不变的情况下，正样本比例增加，缓解了正负样本不均衡的问题。&lt;/p&gt;
&lt;h4 id=&#34;eliminate-grid-sensitivity&#34;&gt;Eliminate grid sensitivity
&lt;/h4&gt;$$
\begin{aligned}
b_x&amp;=1.1\cdot\sigma(t_x)+c_x \\
b_y&amp;=1.1\cdot\sigma(t_y)+c_y
\end{aligned}
$$$$
\begin{aligned}
b_x&amp;=scale_{xy}\cdot\sigma(t_x)-\frac{scale_{xy}-1}{2}+c_x \\
b_y&amp;=scale_{xy}\cdot\sigma(t_y)-\frac{scale_{xy}-1}{2}+c_y \\
\end{aligned}
$$&lt;p&gt;此处的1.1也可以替换成一个其它的略大于1的数。&lt;/p&gt;
&lt;h4 id=&#34;ciou-loss&#34;&gt;CIoU-loss
&lt;/h4&gt;$$
L_{IoU}=1-\frac{B\cap B_{gt}}{B\cup B_{gt}}
$$&lt;p&gt;
但这样带来的问题是，当ground truth和bounding box没有重合时，没有梯度回传，并且对于以下这种情况，三者IoU相等，但重合度不一样，左边较好，右边较差：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/1.png&#34; style=&#34;zoom:67%;&#34; /&gt;
$$
L_{GIoU}=1-IoU+\frac{|C-B\cup B_{gt}|}{|C|}
$$&lt;p&gt;
其中，$C$为同时包含预测框和真实框的最小框的面积。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/2.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;GIoU Loss解决了IoU Loss对距离不敏感的问题，但GIoU Loss存在训练过程中发散等问题，针对此，DIoU被提出，DIoU的作者提出了两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是否可以直接最小化anchor与目标框的归一化距离，使其更快速收敛？&lt;/li&gt;
&lt;li&gt;如何使回归在与目标框有重叠甚至包含时更准确、更快？&lt;/li&gt;
&lt;/ul&gt;
$$
L_{DIoU}=1-IoU+\frac{\rho^2(b,b^{gt})}{c^2}
$$&lt;p&gt;
其中$b$、$b^{gt}$分别代表预测框和真实框的中心点，$\rho$表示计算两个中心点间的欧氏距离，$c$代表能够同时包含预测框和真实框的最小闭包区域的对角线距离。&lt;/p&gt;
&lt;p&gt;DIoU Loss除了收敛速度比GIoU Loss要快不少外，对于下面这种情况：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/3.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;三者IoU Loss和GIoU Loss都一样，但DIoU Loss从左到右依次减小。&lt;/p&gt;
&lt;p&gt;但DIoU Loss仅缓解了bounding box全包含ground truth的问题，但没有彻底解决包含的问题，例如对于下面这种情况：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv4/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\mathcal{R}_{CIoU}=\frac{\rho^2(b,b^{gt})}{c^2}+\alpha v
$$$$
v=\frac{4}{\pi^2}\left(\arctan\frac{w^{gt}}{h^{gt}}-\arctan\frac{w}{h}\right)^2
$$$$
\mathcal{L}_{CIoU}=1-IoU+\frac{\rho^2(b,b^{gt})}{c^2}+\alpha v
$$&lt;p&gt;
CIoU Loss需要考虑$v$的梯度，长宽在$[0,1]$的情况下，$w^2+h^2$的值通常很小，会导致梯度爆炸，因此在实现时将$1/(w^2+h^2)$替换为1。&lt;/p&gt;
&lt;h3 id=&#34;损失函数-2&#34;&gt;损失函数
&lt;/h3&gt;$$
\begin{aligned}
&amp;\lambda_{iou}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}L_{CIoU}\\
+&amp;\lambda_{cls}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}\lambda_c(C_i-\hat{C}_i)^2+\lambda_{cls}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{noobj}\lambda_c(C_i-\hat{C}_i)^2 \\
-&amp;\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}\sum_{c\in classes}\lambda_c[\hat{p}_i(c)\log(p_i(c))+(1-\hat{p}_i(c))\log(1-p_i(c))]
\end{aligned}
$$&lt;p&gt;
从上到下三行式子分别为定位损失、目标置信度损失和分类损失。&lt;/p&gt;
&lt;h3 id=&#34;输入端&#34;&gt;输入端
&lt;/h3&gt;&lt;p&gt;YOLOv4对训练的输入端进行改进，比如数据增强Mosaic、CmBN、SAT自对抗训练。&lt;/p&gt;
&lt;h4 id=&#34;mosaic数据增强&#34;&gt;Mosaic数据增强
&lt;/h4&gt;&lt;p&gt;YOLOv4中使用的Mosaic参考了2019年提出的CutMix数据增强方式，CutMix使用两张图片进行拼接，而Mosaic采用四张图片，使用随机缩放、随机裁剪、随机排布的方式进行拼接。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/17.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;小目标的定义是目标框长宽在$0\times0 \sim 32\times 32$的物体，数据集中小目标框占所有框达到41.4%，但仅有52.3%的图片有小目标，中目标和大目标的分布相对来说更加均匀一些。&lt;/p&gt;
&lt;p&gt;使用四张图片拼接的方法大大丰富了检测数据集，并且经过随机缩放后增加了很多小目标，增强网络鲁棒性，并且可以减少GPU，直接计算4张图片的数据，Mini-batch大小不需要很大。&lt;/p&gt;
&lt;h3 id=&#34;backbone-3&#34;&gt;Backbone
&lt;/h3&gt;&lt;p&gt;YOLOv4使用CSPDarknet-53作为backbone。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;csp结构&#34;&gt;CSP结构
&lt;/h4&gt;&lt;p&gt;CSPDarknet-53是在Darknet-53的基础上，借鉴2019年CSPNet的经验产生的结构，其中包含5个CSP模块，经过5次CSP模块得到的特征图大小变化为：$608\rightarrow304\rightarrow152\rightarrow76\rightarrow38\rightarrow19$。&lt;/p&gt;
&lt;p&gt;CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的，先将基础层的特征映射划分为两部分（$w&lt;em&gt;h&lt;/em&gt;i\rightarrow w&lt;em&gt;h&lt;/em&gt;i/2+w&lt;em&gt;h&lt;/em&gt;i/2$），然后通过跨阶段层次结构将其合并，减少计算量的同时保证准确率。&lt;/p&gt;
&lt;h4 id=&#34;mish激活函数&#34;&gt;Mish激活函数
&lt;/h4&gt;&lt;p&gt;YOLOv4的backbone中都使用了Mish激活函数（后面的网络仍为Leaky ReLU）。&lt;/p&gt;
&lt;p&gt;在介绍Mish前，需要先了解softplus和tanh。&lt;/p&gt;
$$
\zeta(x)=\log(1+e^x)
$$&lt;p&gt;
softplus与ReLU的曲线相似，但比ReLU更为平滑：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/Figure_1.png&#34; style=&#34;zoom:72%;&#34; /&gt;
$$
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$$$
Mish(x)=x\times\tanh(\zeta(x))
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/Figure_2.png&#34; style=&#34;zoom:72%;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;dropblock&#34;&gt;Dropblock
&lt;/h4&gt;&lt;p&gt;Dropblock于2018年提出，和Dropout功能类似，是一种缓解过拟合的一种正则化方式。&lt;/p&gt;
&lt;p&gt;Dropout会在神经网络的学习过程中，将部分隐含层节点的权重归零，但卷积层后的dropout层对网络的泛化能力影响不大，即使随机丢弃，卷积层仍然可以从相邻的激活单元学到相同的信息。&lt;/p&gt;
&lt;p&gt;而Dropblock在整个局部区域进行删减丢弃。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/12.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;neck-1&#34;&gt;Neck
&lt;/h3&gt;&lt;p&gt;YOLOv4的neck采用了SPP模块和FPN + PAN。&lt;/p&gt;
&lt;h4 id=&#34;spp模块&#34;&gt;SPP模块
&lt;/h4&gt;&lt;p&gt;SPP模块中采用$k={1\times1,5\times5,9\times9,13\times13}$的最大池化方式，再将不同尺度的特征图concat起来。（池化时采用padding操作，移动的步长为1，故池化后的特征图大小均为$13\times13$）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/13.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h4 id=&#34;fpn--pan&#34;&gt;FPN + PAN
&lt;/h4&gt;&lt;p&gt;PAN是借鉴2018年图像分割领域PANet的创新点，在YOLOv3中引入了FPN，将高层的特征信息通过上采样的方式进行传递融合，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/14.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv4在FPN层后面还添加了一个自底向上的特征金字塔，其中包含两个PAN结构，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/15.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;FPN层自顶向下传达强语义特征，特征金字塔自底向上传达强定位特征，将二者特征进行聚合。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/16.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;yolov5&#34;&gt;YOLOv5
&lt;/h2&gt;&lt;p&gt;YOLOv5结构和YOLOv5结构比较类似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/19.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;输入端-1&#34;&gt;输入端
&lt;/h3&gt;&lt;p&gt;YOLOv5的输入端采用了和YOLOv4相同的Mosaic数据增强。&lt;/p&gt;
&lt;h4 id=&#34;自适应锚框计算&#34;&gt;自适应锚框计算
&lt;/h4&gt;&lt;p&gt;在YOLOv3、YOLOv4中，初始锚框的值是通过单独的程序运行的，YOLOv5在每次训练时都自适应地计算不同训练集中的最佳锚框值。如果觉得计算锚框的效果不好，也可以在代码中将自动计算锚框的功能关闭。&lt;/p&gt;
&lt;h4 id=&#34;letterbox自适应图片缩放&#34;&gt;letterbox自适应图片缩放
&lt;/h4&gt;&lt;p&gt;常用目标检测算法中，对于不同长宽的图片，常用方式是将图片统一缩放到一个标准尺寸再送入检测网络，例如$416\times416$和$608\times608$，但如果简单的使用resize，可能会导致图片信息的丢失。&lt;/p&gt;
&lt;p&gt;letterbox的主要思想是尽可能的利用网络感受野的信息特征，YOLOv5的网络经过5次下采样，$2^5=32$，最后一层特征图上的每个点可以对应原图中$32\times$32的区域信息。&lt;/p&gt;
&lt;p&gt;以$800\times600$的图片为例，原始缩放尺寸为$416\times416$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;416\div800=\bf{0.52} \\
  &amp;416\div600=0.69
  \end{aligned}
  \right.
  $$&lt;/li&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;800\times0.52=416 \\
  &amp;600\times0.52=312
  \end{aligned}
  \right.
  $$&lt;/li&gt;
&lt;li&gt;
$$
  \left\{
  \begin{aligned}
  &amp;416-312=104 \\
  &amp;104\mod32=8 \\
  &amp;8\div2=4
  \end{aligned}
  \right.
  \quad\Rightarrow 416\times320\quad(312+8)
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backbone-4&#34;&gt;Backbone
&lt;/h3&gt;&lt;h4 id=&#34;focus结构&#34;&gt;Focus结构
&lt;/h4&gt;&lt;p&gt;Focus结构中比较关键的是切片操作，如下图：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/20.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;以YOLOv5s为例，原始$608\times608\times3$的图像输入到Focus结构中后，得到$304\times304\times12$的特征图，再经过卷积得到$304\times304\times32$的特征图。&lt;/p&gt;
&lt;h4 id=&#34;csp结构-1&#34;&gt;CSP结构
&lt;/h4&gt;&lt;p&gt;与YOLOv4不同点在于，YOLOv4只有主干网络使用了CSP结构，而YOLOv5中设计了两种CSP结构，分别应用在backbone和neck中。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/21.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;四种网络的深度和宽度&#34;&gt;四种网络的深度和宽度
&lt;/h3&gt;&lt;p&gt;YOLOv5s、YOLOv5m、YOLOv5l和YOLOv5x。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/18.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/backbone/22.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;yolov7&#34;&gt;YOLOv7
&lt;/h2&gt;&lt;p&gt;Alexey Bochkovskiy大佬的重磅Paper，YOLOv7相同体量下比YOLOv5精度更高，速度快120%（FPS），比 YOLOX 快180%（FPS），比 Dual-Swin-T 快1200%（FPS），比 ConvNext 快550%（FPS），比 SWIN-L快500%（FPS）&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture
&lt;/h3&gt;&lt;h4 id=&#34;extended-efficient-layer-aggregation-networks&#34;&gt;Extended efficient layer aggregation networks
&lt;/h4&gt;&lt;p&gt;除参数量、计算量和计算密度外，还可以从访存代价的角度来设计高效体系结构，ShuffleNet V2中分析了输入/输出通道比、结构的分支数量和Element-wise操作（ReLU、Add）对网络推理速度的影响。&lt;/p&gt;
&lt;p&gt;ShuffleNet V2中给出结论：在计算量和参数量固定的前提下，&lt;strong&gt;输入和输出的通道数相等&lt;/strong&gt;时MAC（Memory Access Cost）取下界，此时的设计是最高效的。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ShuffleNet V2的文章中提出了多个设计准则来帮助模型提速。&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1807.11164&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;vovnet&#34;&gt;VoVNet
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.09730&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;网络结构&#34;&gt;网络结构
&lt;/h6&gt;&lt;p&gt;VoVNet的网络结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;VoVNet-27的前两个stage如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于VoVNet-27：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第1个stage：包含3个$3\times3$卷积，通道数分别为64、64、128；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第2个stage：经过一个OSA模块后，输出特征变为原来的$1/4$，其中OSA模块包括5个通道数为64的$3\times3$卷积，然后将这5个卷积的输出concat在一起，通道数变为$64\times5=320$，再经过1个$1\times1$卷积。输出通道数为128；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后面3个stage均为OSA模块，只是通道数量不同。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;注：每个stage结束后都会有一次$3\times3$、stride=2的max-pooling层，用来减小特征的维度，且每个卷积层都有序列Conv-BN-ReLU。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;osaone-shot-aggregation模块&#34;&gt;OSA（One-Shot Aggregation）模块
&lt;/h6&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/12.png&#34; style=&#34;zoom:42%;&#34; /&gt;
$$
X_l=H_l([X_0,X_1,...,X_{l-1}])
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/13.png&#34; style=&#34;zoom:42%;&#34; /&gt;
&lt;p&gt;DenseNet的输入与输出通道数不一致，且通道数较高，DenseNet采用了$1\times1$卷积压缩特征，下图第一行是DenseNet各个卷积层之间的相互关系的大小，$(s,l)$表示第$s$和第$l$层之间权重的归一化L1范数，可以看出浅层特征图对深层特征图的贡献很少。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/17.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;DenseNet带来了大量的特征冗余，OSA模块只在最后一层聚集前面所有的层，经过这一改动，每层输入和输出的通道数是固定的，就可以让输入和输出的通道数相等而取到最小的MAC，且不需要$1\times1$卷积来压缩特征（$1\times1$卷积会引入大量的激活函数计算）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/16.png&#34; style=&#34;zoom:56%;&#34; /&gt;
&lt;h5 id=&#34;cspvovnet&#34;&gt;CSPVoVNet
&lt;/h5&gt;&lt;p&gt;CSPVoVNet考虑了梯度路径，使不同层学到更多样化的特征。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/18.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;elan&#34;&gt;ELAN
&lt;/h5&gt;&lt;p&gt;//TODO&lt;/p&gt;
&lt;h5 id=&#34;e-elan&#34;&gt;E-ELAN
&lt;/h5&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/19.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h4 id=&#34;model-scaling-for-concatenation-based-models&#34;&gt;Model scaling for concatenation-based models
&lt;/h4&gt;&lt;p&gt;EfficientNet、Scaled-YOLOv4等方法的模型缩放主要用于PlainNet或ResNet等架构中，当这些架构执行扩容、缩容时，每一层的入度和出度都不会改变，因此可以独立分析每个伸缩因子对参数和计算量的影响，但如果将这些方法用于基于级联的体系结构，对深度执行放大或缩小时，紧接在基于级联的计算快之后的translation层的入度就会增加或减小。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/14.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;对于基于级联的模型，不能单独分析不同的比例因子，当缩放计算块的深度时，还必须计算该块的输出通道变化，然后在过渡层执行宽度因子缩放，提出的复合尺度方法既能保持模型在初始设计时的性质，又能保持最优结构。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/15.png&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;trainable-bag-of-freebies&#34;&gt;Trainable bag-of-freebies
&lt;/h3&gt;&lt;h4 id=&#34;repconv&#34;&gt;RepConv
&lt;/h4&gt;&lt;h5 id=&#34;repvgg&#34;&gt;RepVGG
&lt;/h5&gt;&lt;p&gt;RepVGG（CVPR-2021）使用“VGG式”单路极简架构，仅用$3\times3$卷积和ReLU，在速度和性能上都达到了SOTA水平。&lt;/p&gt;
&lt;p&gt;“VGG式”模型有以下几大优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$3\times3$卷积速度极快（现有加速库都对$3\times3$卷积有所优化）；&lt;/li&gt;
&lt;li&gt;单路架构速度快（减少分支可以加快推理速度）；&lt;/li&gt;
&lt;li&gt;单路架构省内存；&lt;/li&gt;
&lt;li&gt;单路架构灵活性更好，可灵活改变各层宽度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RepVGG最大的亮点是利用了结构重参数化，训练一个模型后，将多分支模型等价转换为单路模型，通过这种方式同时利用多分支模型训练性能高的优势和单路模型推理速度快的优势。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
Conv(x,W1)+Conv(x,W2)+Conv(x,W3)=Conv(x,(W1+W2+W3))
$$&lt;p&gt;
$1\times1$相当于一个特殊的$3\times3$卷积，而恒等映射相当于一个特殊的$1\times1$卷积，因此也是一个特殊的$3\times3$卷积，所以只需：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 将identity转换为$1\times1$卷积（单位矩阵，当前通道为1，其他通道为0）；&lt;/li&gt;
&lt;li&gt;Step2. 将$1\times1$卷积转换为$3\times3$卷积（用0填充）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是输入、输出通道均为2的情况，$3\times3$卷积的参数是4个$3\times3$矩阵，$1\times1$卷积的参数是1个$2\times2$矩阵。&lt;/p&gt;
&lt;p&gt;三个分支均有BN层，但这并不会妨碍转换的可行性。&lt;/p&gt;
$$
\begin{aligned}
\mu_\mathcal{B}&amp;\leftarrow\frac{1}{m}\sum_{i=1}^mx_i \\
\sigma_\mathcal{B}^2&amp;\leftarrow\frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_\mathcal{B})^2 \\
\hat{x}_i&amp;\leftarrow\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}} \\
y_i&amp;\leftarrow\gamma\hat{x}_i+\beta\equiv\textbf{BN}_{\gamma,\beta}(x_i)
\end{aligned}
$$$$
y_i=\gamma\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}+\beta=\frac{\gamma}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}x_i+\left(\beta-\frac{\gamma\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2+\epsilon}}\right)
$$$$
W&#39;_{i,:,:,:}=\frac{\gamma_i}{\sigma_i}W_{i,:,:,:}\ ,\quad b&#39;_i=\frac{\mu_i\gamma_i}{\sigma_i}+\beta_i
$$&lt;p&gt;
因此训练好的模型可以等价转换为只有$3\times3$卷积的单路模型。&lt;/p&gt;
&lt;h6 id=&#34;planned-repconv&#34;&gt;Planned RepConv
&lt;/h6&gt;&lt;p&gt;尽管RepConv在VGG上取得了优异的性能，但将其直接应用于ResNet和DenseNet等网络结构上时，其精度会显著降低，作者使用&lt;strong&gt;梯度流传播路径&lt;/strong&gt;来分析不同的重参化模块应该和哪些网络搭配使用。&lt;/p&gt;
&lt;p&gt;通过分析RepConv与不同架构的组合以及产生的性能，作者发现RepConv中的identity破坏了ResNet中的残差结构和DenseNet中的跨层连接，而残差结构和跨层连接为不同的特征图提供了梯度的多样性。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/3.png&#34; style=&#34;zoom:60%;&#34; /&gt;
&lt;p&gt;（RepConvN：RepConv without identity connection）&lt;/p&gt;
&lt;p&gt;得到的结论是：具有残差连接的层，其RepConv不应该具有identity连接。&lt;/p&gt;
&lt;h5 id=&#34;coarse-for-auxiliary-and-fine-for-lead-loss&#34;&gt;Coarse for auxiliary and fine for lead loss
&lt;/h5&gt;&lt;h6 id=&#34;deep-supervision&#34;&gt;Deep supervision
&lt;/h6&gt;&lt;p&gt;深度监督作为一个训练技巧于2014年在&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1409.5185&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Deeply-Supervised Nets&lt;/a&gt;提出来，深度监督又称中继监督，在深度神经网络的某些中间隐藏层加了一个辅助的分类器作为一种网络分支来对主干网络进行监督，目的是为了浅层能够得到更加充分的训练，解决梯度消失和收敛速度过慢的问题。&lt;/p&gt;
&lt;p&gt;如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/6.png&#34; style=&#34;zoom:70%;&#34; /&gt;
&lt;h5 id=&#34;label-assignment&#34;&gt;Label assignment
&lt;/h5&gt;&lt;p&gt;首先简单介绍一下Label assignment，在监督学习中，计算loss，需要有预测结果和标签，在目标检测中最后输出的是框，需要把一些有价值的输出和GT匹配，简单的做法是人工定义规则，例如当生成的矩形框和GT的矩形框之间的IoU大于多少，就认为是目标，小于多少就认为是背景。&lt;/p&gt;
&lt;p&gt;YOLOv7中将负责最终输出的head为lead head，用于辅助训练head称为auxiliary head。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;近年来，研究者经常利用网络预测输出的质量和分布，并结合GT考虑，使用一些计算和优化方法来生成可靠的软标签，例如目标检测中，YOLO使用边界框的回归预测和GT的IoU作为客观的软标签。YOLOv7中，将网络预测结果与GT一起考虑。将结合模型预测结果和GT来获取软标签的模块称为“label assigner”。&lt;/p&gt;
&lt;p&gt;新的问题是：如何将软标签分配给auxiliary head和lead head，目前常用的方法是将auxiliary head和lead head分开，然后使用各自的结果和GT执行标签分配。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/8.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;YOLOv7中，通过lead head预测来引导auxiliary head和lead  head，使用lead head预测作为指导，生成由粗到细的层次标签，分别用于auxiliary head和lead head的学习。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/YOLO/YOLOv7/9.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;具体地，lead head的预测结果和GT为依据生成软标签，但生成两个不同集合的软标签，粗粒度和细粒度，粗粒度标签是通过放松约束条件来让更多网格当作正样本，专注于提高auxiliary head的召回能力，lead head的输出结果可以从高召回的结果中过滤高准确的结果作为最终输出。&lt;/p&gt;</description>
        </item>
        <item>
        <title>异常检测</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</link>
        <pubDate>Wed, 15 Jun 2022 09:34:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=85&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;anomaly-detection&#34;&gt;Anomaly Detection
&lt;/h1&gt;&lt;h2 id=&#34;problem-formulation&#34;&gt;Problem Formulation
&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Given a set of training data ${x^1,x^2,\cdots,x^N}$&lt;/li&gt;
&lt;li&gt;We want to find a function detecting input $x$ is &lt;em&gt;similar&lt;/em&gt; to training data or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;categories&#34;&gt;Categories
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;case-1-with-classifier&#34;&gt;Case 1: With Classifier
&lt;/h2&gt;&lt;h3 id=&#34;example-application&#34;&gt;Example Application
&lt;/h3&gt;&lt;p&gt;例：人物是否来自辛普森一家？&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\hat{y}^1=霸子\quad \hat{y}^2=麗莎\quad \hat{y}^3=荷馬\quad \hat{y}^2=美枝
$$
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Simpsons Characters Data | Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-the-classifier&#34;&gt;How to use the Classifier
&lt;/h3&gt;&lt;p&gt;分类器不知做分类这件事，除了输出它是辛普森一家的哪一个人物以外，还会输出信心分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Anomaly Detection：&lt;/strong&gt;
$$
f(x)=
\left{
\begin{aligned}
&amp;amp;normal,\quad &amp;amp;c(x)\gt\lambda \
&amp;amp;anomaly,\quad &amp;amp;c(x)\leqslant\lambda
\end{aligned}&lt;/p&gt;
&lt;p&gt;\right.
$$&lt;/p&gt;
&lt;h3 id=&#34;how-to-estimate-confidence&#34;&gt;How to estimate Confidence
&lt;/h3&gt;&lt;p&gt;Softmax Layer反映了Output的概率分布。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一般使用最高的分数当作信心分数，也可以使用负熵（negative entropy）。&lt;/p&gt;
&lt;h3 id=&#34;outlook-network-for-confidence-estimation&#34;&gt;Outlook: Network for Confidence Estimation
&lt;/h3&gt;&lt;p&gt;可以直接让一个模型去学如何输出信心分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Terrance DeVries, Graham W. Taylor, Learning Confidence for Out-of-Distribution  Detection in Neural Networks, arXiv, 2018&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation
&lt;/h3&gt;&lt;p&gt;正负样本比例往往比较悬殊，评估模型效果时正确率并不是很好的指标。&lt;/p&gt;
&lt;p&gt;Cost Table（给予假阴和假阳的权重）不同，得到的结果也不同。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/8.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一般使用Area under ROC curve，只考虑排序，不考虑阈值$\lambda$。&lt;/p&gt;
&lt;h3 id=&#34;possible-issues&#34;&gt;Possible Issues
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/9.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/10.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;h4 id=&#34;learn-a-classifier-giving-low-confidence-score-to-anomaly&#34;&gt;Learn a classifier giving low confidence score to anomaly
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin, Training Confidencecalibrated Classifiers for Detecting Out-of-Distribution Samples, ICLR 2018&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;how-can-you-obtain-anomaly&#34;&gt;How can you obtain anomaly?
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Mark Kliger, Shachar Fleishman, Novelty Detection with GAN, arXiv, 2018&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-2-without-labels&#34;&gt;Case 2: Without Labels
&lt;/h2&gt;&lt;h3 id=&#34;twitch-plays-pokémon&#34;&gt;Twitch Plays Pokémon
&lt;/h3&gt;&lt;p&gt;多人同时玩一款游戏，因为有“Troll”（网络小白）不会玩游戏、乱按或者出于恶意，导致游戏非常困难。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/11.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;假设多数玩家是想要通关的，从多数玩家的行为去侦测出异常的玩家。&lt;/p&gt;
&lt;h3 id=&#34;problem-formulation-1&#34;&gt;Problem Formulation
&lt;/h3&gt;&lt;p&gt;需要先把一个玩家$x$表示成一个向量$[x_1,x_2,\cdots]^T$，例如$x_1$表示说垃圾话（不能操控游戏的多余的发言），假设游戏中有民主状态（每隔20秒，系统选发言最多的指令）和无政府状态（系统不断地选看到的输入指令），$x_2$表示在无政府状态时的发言。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/12.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在没有分类器的情况下，可以建立一个模型，这个几率模型告诉我们某一种行为发生的概率$P(X)$有多大。&lt;/p&gt;
&lt;p&gt;如果$P(X)\geqslant \lambda$，则说他是正常的，否则是异常的。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/13.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;假设每一个玩家可以用二维向量$x=\begin{bmatrix}x_1 \ x_2\end{bmatrix}$表示：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/14.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;maximum-likelihood&#34;&gt;Maximum Likelihood
&lt;/h3&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;假设数据点是从一个概率密度函数$f_\theta(x)$采样得到的
&lt;ul&gt;
&lt;li&gt;$\theta$ determines the shape of $f_\theta(x)$&lt;/li&gt;
&lt;li&gt;$\theta$ is unknown, to be found from data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
$$
L(\theta)=f_\theta(x^1)f_\theta(x^2)\cdots f_\theta(x^N)
$$$$
\theta^*=\arg\max_\theta L(\theta)
$$$$
f_{\mu,\Sigma}(x)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}
$$&lt;ul&gt;
&lt;li&gt;输入：vector $x$&lt;/li&gt;
&lt;li&gt;输出：probability density of sampling $x$&lt;/li&gt;
&lt;li&gt;均值$\mu$和协方差矩阵$\Sigma$是决定函数形态的$\theta$&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
L(\theta)&amp;=f_\theta(x^1)f_\theta(x^2)\cdots f_\theta(x^N) \\
&amp;\Downarrow \\
L(\mu,\Sigma)&amp;=f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)\cdots f_{\mu,\Sigma}(x^N) \\
\\
\theta^*&amp;=\arg\max_\theta L(\theta)\\
&amp;\Downarrow \\
\mu^*,\Sigma^*&amp;=\arg\max_{\mu,\Sigma}L(\mu,\Sigma)
\end{aligned}
$$$$
\mu^*=\frac{1}{N}\sum_{n=1}^Nx^n\quad \Sigma^*=\frac{1}{N}\sum_{n=1}^N(x-\mu^*)(x-\mu^*)^T
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/15.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;outlook-auto-encoder&#34;&gt;Outlook: Auto-encoder
&lt;/h3&gt;&lt;p&gt;用辛普森一家的图片去训练Auto-encoder，Auto-encoder就会特别还原辛普森一家的图片，但如果输入一张异常图片，重建的结果可能会差很多。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/16.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/Anomaly%20Detection/17.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>自编码器</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</link>
        <pubDate>Sun, 05 Jun 2022 16:57:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=83&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——自编码器（Auto-encoder）&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;auto-encoder&#34;&gt;AUTO-ENCODER
&lt;/h1&gt;&lt;h2 id=&#34;reviewself-supervised-learning-framework&#34;&gt;Review：Self-supervised Learning Framework
&lt;/h2&gt;&lt;p&gt;AUTO-ENCODER可以算是Self-supervised Learning的一环。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在BERT、GPT之前有更古老的不需要标注资料的任务，就叫做Auto-Encoder，可以看作是Self-supervised Learning的一种预训练方法，跟填空、预测接下来的token是很相似的概念。&lt;/p&gt;
&lt;h2 id=&#34;basic-idea-of-auto-encoder&#34;&gt;Basic Idea of Auto-encoder
&lt;/h2&gt;&lt;h3 id=&#34;auto-encoder-1&#34;&gt;Auto-encoder
&lt;/h3&gt;&lt;p&gt;以影像为例，在Auto-encoder里面有两个网络，分别是Encoder和Decoder，Encoder（可能是很多层的CNN）读入一张图片，将其变成一个向量，向量作为Decoder的输入，Decoder（可能是GAN中的Generator）会产生一张图片。训练的目标是希望Encoder的输入和Decoder的输出越接近越好（Reconstruction，重建）。&lt;/p&gt;
&lt;p&gt;Auto-encoder的概念和Cycle GAN是一模一样的。&lt;/p&gt;
&lt;p&gt;Encoder的输出叫做Embedding，也叫做Representation、Code&amp;hellip;&lt;/p&gt;
&lt;p&gt;要把Auto-encoder用在下游任务中，常见的用法是，输入的图片这个向量可能会比较长，经过Auto-encoder的Encoder后得到一个比较短的向量，再用这个Embedding去做接下来想做的事情。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;more-dimension-reduction&#34;&gt;More Dimension Reduction
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;PCA&lt;/li&gt;
&lt;li&gt;t-SNE&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-auto-encoder&#34;&gt;Why Auto-encoder？
&lt;/h3&gt;&lt;p&gt;例如一张$3\times 3$的图片经过Encoder得到2维的Embedding，再经过Decoder得到$3\times 3$的图片，虽然9个数值才能描述这张图，但实际上图片的变化可能非常有限，一个低维的Embedding就可以描述。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;在神雕侠侣中，杨过三招之内把樊一翁的胡子剪掉，因为樊一翁的胡子是由他的头控制的，虽然胡子可以甩两丈长，但头能够做的变化非常有限，表面上胡子的鞭法很厉害，但只要打他的头，就能让他胡子的变化非常有限。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;de-noising-auto-encoder&#34;&gt;De-noising Auto-encoder
&lt;/h3&gt;&lt;p&gt;将原始的图片上加上一些噪声，再通过Auto-encoder，再让生成的图片和原始图片越接近越好，此时Encoder和Decoder又增加了一个任务：消除噪声。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Vincent, Pascal, et al. &amp;ldquo;Extracting and composing robust featureswith denoising autoencoders.&amp;rdquo; ICML, 2008.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在BERT中，我们加入的Mask其实就是噪声，BERT就是Encoder，BERT的输出是Embedding，输出经过的线性层是Decoder，结果是要做填空题，即和真实内容越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/6.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;feature-disentanglement&#34;&gt;Feature Disentanglement
&lt;/h2&gt;&lt;h3 id=&#34;representation-includes-information-of-different-aspects&#34;&gt;Representation includes information of different aspects
&lt;/h3&gt;&lt;p&gt;一张图片包含了色泽、纹理等多方面的内容，一段语音讯号包含了文字、语者等方面的内容&amp;hellip;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/7.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;feature-disentangle&#34;&gt;Feature Disentangle
&lt;/h3&gt;&lt;p&gt;Feature Disentagle想要做的事情是，有没有办法在训练一个Auto-encoder时，同时有办法知道这个Embedding的哪些维度代表了哪些资讯。举例来说，一个语音讯号经过Encoder，得到的100维的Embedding中，前50维是这句话的内容，后50维是这句话语者的特征。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.05742&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.05742] One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1804.02812&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1804.02812] Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.05879&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.05879] AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;application-voice-conversion&#34;&gt;Application: Voice Conversion
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/8.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;discrete-latent-representation&#34;&gt;Discrete Latent Representation
&lt;/h2&gt;&lt;p&gt;到目前位置，我们都假设Embedding是Real numbers（$[0.9\ 0.1\ 0.3\ 0.7]$）；此外它也可以是Binary（$[1\ 0\ 0\ 1]$），它代表了某种特性有或是没有，例如第一维的1代表是男性，0代表是女性；甚至也有可能是一个One-hot向量，例如做手写数字识别，代表着不同的数字。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/9.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;discrete-representation&#34;&gt;Discrete Representation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1711.00937&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1711.00937] Neural Discrete Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discrete Representation中，最知名的是VQVAE，VQVAE中，输入一张图片，Encoder输出一个连续的向量，接下来有一个Codebook（一排向量，学出来的），让Encoder的输出和每一个向量都计算相似度（类似Self-attention），再从Codebook中拿出相似度最高的向量，把这个向量输入到Decoder里面，输出一张图片，和输入越接近越好。&lt;/p&gt;
&lt;p&gt;VQVAE中，Decoder的输入只能是Codebook的其中一个，Embedding没有无穷无尽的可能，用这种方式在语音讯号学习时，可以学到最基本的发音单位（arxiv 1901.08810）。&lt;/p&gt;
&lt;h3 id=&#34;text-as-representation&#34;&gt;Text as Representation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.02851&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1810.02851] Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Representation也可以不是向量，而是一段文字，Encoder输出的文字可能是这篇文章的精华内容，此时Auto-encoder可以看作是两个Seq2seq的模型（&lt;strong&gt;seq2seq2seq auto-encoder&lt;/strong&gt;），但实际上这样做行不通，Encoder和Decoder间可能会学出它们之间的“暗号”，此时需要一个Discriminator，来判断生成的这个句子是不是人写的句子，Encoder要想办法产生一段句子不只透过Decoder产生原文，而且要想办法骗过Discriminator觉得是人写的句子。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/10.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;如何训练？：&lt;strong&gt;RL硬做&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;tree-as-embedding&#34;&gt;Tree as Embedding
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/11.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1806.07832&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1806.07832] StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.03746&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.03746] Unsupervised Recurrent Neural Network Grammars (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-applications&#34;&gt;More Applications
&lt;/h2&gt;&lt;h3 id=&#34;generator&#34;&gt;Generator
&lt;/h3&gt;&lt;p&gt;此前介绍的基本都是拿Encoder的输出做下游任务，实际上Decoder就相当于一个Generator，吃一个向量产生一个图片。&lt;/p&gt;
&lt;p&gt;除了GAN外，还有另外两种生成式模型，variational auto-encoder（VAE）就是其中一个，它就是把Auto-encoder的Decoder拿出来当Generator来用。&lt;/p&gt;
&lt;h3 id=&#34;compression&#34;&gt;Compression
&lt;/h3&gt;&lt;p&gt;可以把Encoder的输出当作是图片压缩后的结果，Decoder的输出当作是解压后的结果，只是经过压缩和解压缩后，图片会有一定的失真。&lt;/p&gt;
&lt;h3 id=&#34;anomaly-detection&#34;&gt;Anomaly Detection
&lt;/h3&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Given a set of training data ${x^1,x^2\cdots,x^N}$&lt;/li&gt;
&lt;li&gt;Detecting input $x$ is &lt;em&gt;similar&lt;/em&gt; to training data or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/12.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一个对象是否正常，取决于训练资料中的内容。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/13.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;fraud-detection&#34;&gt;Fraud Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Traning data: credit card transactions, $x$: fraud or not
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Credit Card Fraud Detection | Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;network-intrusion-detection&#34;&gt;Network Intrusion Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Traning data: connection, $x$: attack or not
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;KDD Cup 1999 Data (uci.edu)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cancer-detection&#34;&gt;Cancer Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Traning data: normal cells, $x$: cancer or not?
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Breast Cancer Wisconsin (Diagnostic) Data Set | Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;异常检测中往往是有一大堆正常的样本，而只有少量或者没有负样本，所以它不是一个一般的分类问题（One Class分类问题）。&lt;/p&gt;
&lt;h4 id=&#34;approach-auto-encoder&#34;&gt;Approach: Auto-encoder
&lt;/h4&gt;&lt;p&gt;例：真实人脸检测&lt;/p&gt;
&lt;p&gt;用真人的人脸去训练一个Auto-encoder，对于一张异常的图片，它的reconstruction loss会比较大。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/8/14.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>语音与影像上的自监督式学习模型</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%AF%AD%E9%9F%B3%E4%B8%8E%E5%BD%B1%E5%83%8F%E4%B8%8A%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</link>
        <pubDate>Fri, 20 May 2022 21:39:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%AF%AD%E9%9F%B3%E4%B8%8E%E5%BD%B1%E5%83%8F%E4%B8%8A%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=76&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——语音与影像上的神奇自督导式学习模型&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-supervised-learning-for-speech-and-image&#34;&gt;Self-supervised Learning for Speech and Image
&lt;/h1&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;h3 id=&#34;review-self-supervised-learning-for-text&#34;&gt;Review: Self-supervised Learning for &lt;strong&gt;Text&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;使用大量无标注的资料来训练BERT，然后在BERT的输出后接下游模型，并提供少量有标注的资料来完成下游任务的训练，下游模型往往都是比较简单的模型。这些有标注的训练资料也可以同时拿来微调BERT。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/review.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;self-supervised-learning-for-speech&#34;&gt;Self-supervised Learning for &lt;strong&gt;Speech&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;使用未标注的语音讯号来训练语音版的BERT，BERT会将声音讯号变为一排向量，将少量的标注资料提供给下游模型。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/speech%20bert.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://superbbenchmark.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;S&lt;/strong&gt;peech processing &lt;strong&gt;U&lt;/strong&gt;niversal &lt;strong&gt;PER&lt;/strong&gt;formance &lt;strong&gt;B&lt;/strong&gt;enchmark（SUPERB）&lt;/a&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/superb.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;self-supervised-learning-for-image&#34;&gt;Self-supervised Learning for &lt;strong&gt;Image&lt;/strong&gt;
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/for%20image.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2011.13377&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2011.13377] How Well Do Self-Supervised Models Transfer? (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.01235&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.01235] Scaling and Benchmarking Self-Supervised Visual Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不仅是在语言上，自监督式学习在语音和图像上也有着非常好的表现，下面将介绍在影像和语音上使用自监督学习的五大方法。&lt;/p&gt;
&lt;h2 id=&#34;generative-approaches&#34;&gt;Generative Approaches
&lt;/h2&gt;&lt;p&gt;将文字上已经非常成功的方法（BERT、GPT）拿来语音和图像上使用。&lt;/p&gt;
&lt;h3 id=&#34;maskingbert-series&#34;&gt;Masking（BERT series）
&lt;/h3&gt;&lt;p&gt;将某些声音讯号盖起来，让模型去还原被盖起来的部分。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.12638&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1910.12638] Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/mockingjay.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Smoothness of acoustic features（Mockingjay）&lt;/p&gt;
&lt;p&gt;语音和文字有所不同，声音讯号表示成一排向量，但相邻的向量和向量之间非常接近，把某个向量盖起来后，只需拿两边的向量做内插，就可以大致还原出被盖住的内容，故每次Mask时，Mask一长排的数据。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/masking%20consecutive%20features.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Masking strategies for speech&lt;/p&gt;
&lt;p&gt;在语音上可以做新的尝试，不再时间的方向上做Masking，而是Mask这些向量的某几个dimension。这种方法可以学到更多语者方面的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/masking%20specific%20dimensions.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;predicting-futuregpt-series&#34;&gt;Predicting Future（GPT series）
&lt;/h3&gt;&lt;p&gt;在文字上，GPT预测下一个会出现的token（$n=1$），而在语音上，由于相邻向量很接近，所以通常会预测接下来&lt;strong&gt;某一段时间之后&lt;/strong&gt;的向量（$n\gt 3$）。&lt;/p&gt;
&lt;p&gt;代表性的模型是APC（Autoregressive Predictive Coding）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/predicting%20future.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;how-about-image&#34;&gt;How about &lt;strong&gt;image&lt;/strong&gt;?
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://openai.com/blog/image-gpt/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Image GPT (openai.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将一张图片的各个pixel拉直，训练模型，再将训练后的模型用在下游任务中。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;影像版的GPT：给一段pixel，预测下一个pixel。&lt;/li&gt;
&lt;li&gt;影像版的BERT：将一段pixel的一部分盖起来，让机器预测被盖起来的部分。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相较由token来表示的文字，语音和影像中包含了更多的信息，让机器产生复杂的影像和声音讯号往往不是一件容易的事情。除了让机器还原影像和声音讯号外，能否还原、预测其他内容，从而达到和Self-supervised learning一样的效果？&lt;/p&gt;
&lt;h2 id=&#34;predictive-approach&#34;&gt;Predictive Approach
&lt;/h2&gt;&lt;h3 id=&#34;image---predicting-rotation&#34;&gt;Image - Predicting Rotation
&lt;/h3&gt;&lt;p&gt;让机器去学习一张图片有没有被旋转过。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1803.07728&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1803.07728] Unsupervised Representation Learning by Predicting Image Rotations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/predicting%20rotation.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image---context-prediction&#34;&gt;Image - Context Prediction
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1505.05192&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1505.05192] Unsupervised Visual Representation Learning by Context Prediction (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;给一张比较大的图片，将图片中的两个小块切出来，让模型去回答第一块在第二块的哪个方向。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/context%20prediction.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;similar-idea-on-speech&#34;&gt;Similar idea on &lt;strong&gt;Speech&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/9060816&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Pre-Training Audio Representations With Self-Supervision | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从一句话中切两段出来，让机器去判断两段声音讯号相距多长时间。&lt;/p&gt;
&lt;h3 id=&#34;predict-simplified-objects&#34;&gt;Predict Simplified Objects
&lt;/h3&gt;&lt;p&gt;把原来要生成的复杂的内容简化。&lt;/p&gt;
&lt;p&gt;以语音讯号为例，对声音讯号中的向量做clustering（K-means, etc.），将其离散化，每个向量变成一个token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/simplified%20object.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speech&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HuBERT：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2106.07447&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2106.07447] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEST-RQ：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2202.01855&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2202.01855] Self-supervised Learning with Random-projection Quantizer for Speech Recognition (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DeepCluster：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1807.05520&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1807.05520] Deep Clustering for Unsupervised Learning of Visual Features (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;模型是否在不产生内容的情况下学习？&lt;/p&gt;
&lt;h2 id=&#34;contrastive-learning&#34;&gt;Contrastive Learning
&lt;/h2&gt;&lt;h3 id=&#34;basic-idea-of-contrastive-learning&#34;&gt;Basic Idea of Contrastive Learning
&lt;/h3&gt;&lt;p&gt;一对positive（属于同一类）的样本，扔到Encoder中，他们的输出越接近越好，而对于一对negative（属于不同类）的样本，他们的产生的向量越远越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/basic%20idea%20of%20contrastive%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;没有让机器去预测或产生任何内容，而是找出positive的pair，让其向量越近越好，找出negative的pair，其向量越远越好。但目前并没有任何标签，不知道哪些图片是同样类别而哪些图片是不同类别。&lt;/p&gt;
&lt;h3 id=&#34;simclr&#34;&gt;SimCLR
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.05709&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.05709] A Simple Framework for Contrastive Learning of Visual Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将同一张图片做数据增强（随机裁切、颜色变型、高斯噪声等），同一张图片做数据增强得到的图片为正样本，而和另一张图片的增强结果为负样本。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/simclr.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;文章中探讨了各种数据增强的组合，比较一致的发现是，通常随机裁切是不可或缺的方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speech&lt;/strong&gt; SimCLR：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.13991&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2010.13991] Speech SIMCLR: Combining Contrastive and Reconstruction Objective for Self-supervised Speech Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;moco&#34;&gt;MoCo
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1911.05722&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1911.05722] Momentum Contrast for Unsupervised Visual Representation Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/moco.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MoCo v2&lt;/strong&gt;（吸收SimCLR的优点后产生）：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.04297&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.04297] Improved Baselines with Momentum Contrastive Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contrastive-learning-for-speech&#34;&gt;Contrastive Learning for &lt;strong&gt;Speech&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;CPC：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1807.03748&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1807.03748] Representation Learning with Contrastive Predictive Coding (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wav2vec：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.05862&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.05862] wav2vec: Unsupervised Pre-training for Speech Recognition (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两段声音讯号，经过Encoder后各产生一排向量，将Encoder的输出再通过一个Predicter产生新的输出，新的输出和与它相邻的向量是positive的pair，而与另一个句子中的向量是negative的向量。新的输出向量通过不同的线性变换以后，和下一个时间点的输出越接近越好，和其他的句子的向量越远越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/contrastive%20learning%20for%20speech.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可以拿Encoder或Encoder和Predicter用在下游任务中。&lt;/p&gt;
&lt;p&gt;CPC和Wav2vec的主要差别在Predicter上，后来又出现了VQ-wav2vec。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VQ-wav2vec &lt;em&gt;+ BERT&lt;/em&gt;：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.05453&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1910.05453] vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/vq-wav2vec.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;VQ-wav2vec中，Encoder的输出不是向量，而是离散的token。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How to train with quantization：&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=JZvEzb5PV3U&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【機器學習2021】自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用 - YouTube&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在原论文中，VQ-wav2vec的提出是为了在后面接上一个类似文字BERT的Encoder，VQ-wav2vec的输出正好可以当成BERT架构Encoder的输入。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/vq-wav2vec_2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;这样做的另一个好处是模型可以比较容易的抽出和声音的内容有关的资讯。&lt;/p&gt;
&lt;p&gt;Discrete BERT中说明了VQ-wav2vec + BERT的结构具有较好的表现。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discrete BERT：[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1911.03912&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1911.03912] Effectiveness of self-supervised pre-training for speech recognition (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在VQ-wav2vec + BERT架构的结构中，VQ-wav2vec和BERT架构是分开训练的，在Wav2vec 2.0中，将二者合并起来一起训练。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.11477&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.11477] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但合起来训练时，由于第一个Encoder的输出是离散token，不方便训练，所以将离散token改为向量，再将向量输入到第二个Encoder中，第二个Encoder会对输入的向量做masking，再用被盖住位置的输出向量去预测这个位置的离散token，同时希望这个向量产生其他向量的可能性越小越好。（&lt;em&gt;在原论文中，离散token都经过一个变换转为了embedding，希望输出的向量和对应位置的embedding越接近越好，实际上这就等价于把第二个Encoder的输出向量做线性变换后当作分类问题，希望分类为当前token的概率越高越好，分类为其他token的概率越小越好。&lt;/em&gt;）&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/wav2vec2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;不直接将离散token扔到第二个Encoder中，而是将连续向量扔进去，直观的解释是离散token包含的信息较少，而连续向量包含的信息相对要多。若第二个Encoder中输入的是离散token，其表现会掉一大截。&lt;/p&gt;
&lt;p&gt;另外，没有将其当作一个分类问题来做的原因可能是，语音中的token数量太大了，大概是十万量级，而常见语言模型的token数量也只有两万左右。&lt;/p&gt;
&lt;h3 id=&#34;alterative-way-to-understand-wav2vec-20&#34;&gt;Alterative way to understand Wav2vec 2.0
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;文字是否可以做Contrastive learning？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;事实上，BERT也可以看作是一种Contrastive learning的方式。&lt;/p&gt;
&lt;p&gt;BERT可以看作是在完成分类任务，但分类任务也可以看作Contrastive learning，例如“深 [MASK] 学 习”经过BERT得到embedding，通过线性变换和Softmax，希望得到“度”的概率越大越好，其他词的概率越小越好，即BERT输出的embedding和“度”的embedding距离越近越好，和其他词的embedding距离越远越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/bert%20contrastive%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在语音上，不像文字中negative样本是可以穷举的，语音讯号通常会经过一个网络后，将其离散化再进行处理。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/wav2vec2%20speech.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;classification-vs-contrastive&#34;&gt;Classification vs. Contrastive
&lt;/h4&gt;&lt;h5 id=&#34;classification&#34;&gt;Classification
&lt;/h5&gt;&lt;p&gt;对于分类问题，模型最后一层的输出向量通过一个线性变换（乘一个矩阵，和矩阵里的行做dot-product），希望和对应的行向量做dot-product后值越大越好，和其他行向量计算后结果越小越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/classification.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;contrastive&#34;&gt;Contrastive
&lt;/h5&gt;&lt;p&gt;希望模型最后一层的输出向量和对应分类的embedding越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/contrastive.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;selecting-negative-examples-is-not-trivial&#34;&gt;Selecting Negative Examples is not trivial
&lt;/h3&gt;&lt;p&gt;negative样本的选取应该足够困难，但同时也不应该过难。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/selecting%20negative%20examples.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如何避开负样本的选择？&lt;/p&gt;
&lt;h2 id=&#34;bootstrapping-approaches&#34;&gt;Bootstrapping Approaches
&lt;/h2&gt;&lt;p&gt;如果训练时不使用正样本，仅用正样本来训练，最后模型对于所有图像都会判断他们很接近。&lt;/p&gt;
&lt;p&gt;解决的办法是，在其中一个Encoder后接一个Predictor（可能是只有几层的前馈神经网络），训练的目标是两个向量越接近越好。但只计算Predictor所在的这一边的梯度，更新这一边的参数，再将这一边Encoder更新后的参数复制到另一边。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/bootstrapping%20approaches.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;这看起来像是某种“妖术”，但大量实验表明这种方法的确有效&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;alterative-way-to-understand-bootstrapping&#34;&gt;Alterative way to understand Bootstrapping
&lt;/h3&gt;&lt;h4 id=&#34;typical-knowledge-distillation知识蒸馏&#34;&gt;Typical Knowledge Distillation（知识蒸馏）
&lt;/h4&gt;&lt;p&gt;Teacher的参数固定，更新Student的参数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/knowledge%20distillation.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image&#34;&gt;Image
&lt;/h3&gt;&lt;h4 id=&#34;bootstrap-your-own-latentbyol&#34;&gt;Bootstrap your own latent（BYOL）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.07733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.07733] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
$$
\theta\leftarrow\lambda\theta+(1-\lambda)\theta&#39;
$$&lt;p&gt;Teacher Encoder的参数不会马上变得和Student Encoder一样，用一个渐进的方法影响Teacher Encoder。&lt;/p&gt;
&lt;h4 id=&#34;simple-siamesesimsiam&#34;&gt;Simple Siamese（SimSiam）
&lt;/h4&gt;&lt;p&gt;Moving Average不是必要的，直接复制参数同样可行。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.07733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.07733] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;speech&#34;&gt;Speech
&lt;/h3&gt;&lt;h4 id=&#34;data2vec&#34;&gt;Data2vec
&lt;/h4&gt;&lt;p&gt;在文字、影像、语音上都有不错的结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2202.03555&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2202.03555] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;simply-extra-regularization&#34;&gt;Simply Extra Regularization
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.03230&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2103.03230] Barlow Twins: Self-Supervised Learning via Redundancy Reduction (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.04906&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.04906] VICReg: &lt;strong&gt;Variance-Invariance-Covariance Regularization&lt;/strong&gt; for Self-Supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;variance-invariance-convariance-regularization&#34;&gt;&lt;strong&gt;Variance&lt;/strong&gt;-&lt;strong&gt;Invariance&lt;/strong&gt;-&lt;strong&gt;Convariance&lt;/strong&gt; Regularization
&lt;/h3&gt;&lt;h4 id=&#34;invariance&#34;&gt;Invariance
&lt;/h4&gt;&lt;p&gt;用正样本来训练，输出向量越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/invariance.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;variance&#34;&gt;Variance
&lt;/h4&gt;&lt;p&gt;给Encoder一个batch的图片，得到一个batch数量的向量，这些向量的每一个维度的方差要大于某一个阈值。&lt;/p&gt;
&lt;p&gt;通过这种方法来避免Encoder看到什么图片都输出同一个向量。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/variance.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;covariance&#34;&gt;Covariance
&lt;/h4&gt;&lt;p&gt;把一个batch里的向量拿出来计算协方差，协方差矩阵中非对角线的元素接近0。&lt;/p&gt;
&lt;p&gt;有了Covariance后，让所有维度都充分利用到，散布更平均。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/Self-supervised%20Learning%20for%20Speech%20and%20Image/covariance.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>自监督式学习</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Tue, 17 May 2022 14:32:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=71&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-supervised-learning&#34;&gt;Self-Supervised Learning
&lt;/h1&gt;&lt;h2 id=&#34;芝麻街与进击的巨人&#34;&gt;芝麻街与进击的巨人
&lt;/h2&gt;&lt;h3 id=&#34;芝麻街&#34;&gt;芝麻街
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-1/stavreal.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;进击的巨人bertolt-hoover&#34;&gt;进击的巨人：Bertolt Hoover
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-1/%E8%B4%9D%E7%89%B9%E9%9C%8D%E5%B0%94%E5%BE%B7%E8%83%A1%E4%BD%9B.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;主流模型参数量&#34;&gt;主流模型参数量
&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Model&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Parameters&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ELMO&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;94M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;BERT&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;340M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;GPT-2&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1542M&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Megatron&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;8B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;T5&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;11B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Turing NLG&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;17B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;GPT-3&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;175B&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Switch Transformer&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;1.6T&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;bert&#34;&gt;BERT
&lt;/h2&gt;&lt;h3 id=&#34;self-supervised-learning-1&#34;&gt;Self-supervised Learning
&lt;/h3&gt;&lt;p&gt;对于数据$x$，监督学习需要知道数据的标签$\hat{y}$，来让模型输出我们想要的$y$。而自监督学习没有标注，将$x$分为两部分，一部分$x&amp;rsquo;$输入到模型得到$y$，另一部分$x&amp;rsquo;&amp;rsquo;$作为标签，然后让$y$和$x&amp;rsquo;&amp;rsquo;$越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/self-supervised%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;自监督学习可以看作是一种无监督学习的方法，无监督学习的范围很大，里面有很多不同的方法，为了明确说明现在说做的工作，就称为自监督学习。&lt;/p&gt;
&lt;h3 id=&#34;masked-token-prediction&#34;&gt;Masked token prediction
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.04805&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BERT的架构和Transfromer Encoder相同，输入一排向量，输出另一排向量，一般用在文字处理上。&lt;/p&gt;
&lt;p&gt;输入一串token（token是处理一段文字的单位，在中文里一般把一个方块字当作一个token。），随机盖住一些token，盖住token有两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;变为某个特殊的token&lt;/li&gt;
&lt;li&gt;随机换为另一个token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对BERT的输出序列分别做线性变换（乘矩阵），再做Softmax就得到了一个分布。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/masking%20input.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;BERT不知道被盖住的部分是什么内容，但我们知道这部分内容，BERT学习的目标是输出和盖住的部分越接近越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/masking%20input2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;next-sentence-prediction&#34;&gt;Next Sentence Prediction
&lt;/h3&gt;&lt;p&gt;从资料库中拿出两个句子，两个句子之间加入特殊的分隔符号[SEP]，再整个序号的最前面加[CLS]符号，整个序列输入BERT，看[CLS]对应的输出，[CLS]经过BERT的输出再经过线性变换后输出为Yes/No，代表这两个句子是不是相接的。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;例：[CLS] I like cat. [SEP] He likes dog&lt;/em&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/next%20sentence%20prediction.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Next Sentence Prediction对于BERT接下来要做的事情可能无用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1907.11692&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1907.11692] RoBERTa: A Robustly Optimized BERT Pretraining Approach (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next Sentence Prediction这个任务可能比较简单，BERT可能学习不到太多有用的东西。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SOP&lt;/strong&gt;：Sentence order prediction Used in ALBERT&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1909.11942&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1909.11942] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两个句子本来就连在一起，人为拆分开，本来放在前面的句子作为Sentence 1，本来放在后面的句子作为Sentence 2，或本来放在前面的句子作为Sentence 2，本来放在后面的句子作为Sentence 1。然后让BERT去回答是哪一种顺序。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;downstream-tasks&#34;&gt;Downstream Tasks
&lt;/h3&gt;&lt;p&gt;在训练BERT时，给了BERT两个任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Masked token prediction&lt;/li&gt;
&lt;li&gt;Next sentence prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在训练BERT时，似乎仅仅在教BERT如何去做“填空题”，但BERT可以用在其他地方，BERT真正在下游任务（Downstream Tasks）中被使用，但需要少量有标注的数据。BERT经过微调（Fine-tune）可以去完成各种其他的任务。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/Fine-tune.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;glue&#34;&gt;GLUE
&lt;/h3&gt;&lt;p&gt;任务集GLUE（General Language Understanding Evaluation）共有9个任务，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corpus of Linguistic Acceptability（CoLA）&lt;/li&gt;
&lt;li&gt;Stanford Sentiment Treebank（SST-2）&lt;/li&gt;
&lt;li&gt;Microsoft Research Paraphrase Corpus（MRPC）&lt;/li&gt;
&lt;li&gt;Quora Question Pairs（QQP）&lt;/li&gt;
&lt;li&gt;Semantic Textual Similarity Benchmark（STS-B）&lt;/li&gt;
&lt;li&gt;Multi-Genre Natural Language Inference（MNLI）&lt;/li&gt;
&lt;li&gt;Question-answering NLI（QNLI）&lt;/li&gt;
&lt;li&gt;Recognizing Textual Entailment（RTE）&lt;/li&gt;
&lt;li&gt;Winograd NLI（WNLI）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以在这9个任务上分别微调模型得到9个模型，通过结果数值来判断模型的好坏。&lt;/p&gt;
&lt;p&gt;中文版本的GLUE：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cluebenchmarks.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CLUE中文语言理解基准测评 (cluebenchmarks.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-bert&#34;&gt;How to use BERT
&lt;/h3&gt;&lt;h4 id=&#34;case-1&#34;&gt;Case 1
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Input：sequence&lt;/li&gt;
&lt;li&gt;Output：class&lt;/li&gt;
&lt;li&gt;Example：Sentiment analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;给BERT输入一个句子，前面放[CLS] token，对[CLS]输出的向量做线性变换，Softmax后输出class。需要提供大量的已标注的训练资料。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Linear部分是随机初始化，而BERT部分将学会了做“填空题”的BERT模型的参数拿来初始化。&lt;/p&gt;
&lt;h4 id=&#34;case-2&#34;&gt;Case 2
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Input：sequence&lt;/li&gt;
&lt;li&gt;Output：sequence&lt;/li&gt;
&lt;li&gt;Example：POS tagging（词性标注）&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;给BERT输入一个句子，前面放[CLS] token，对句子里面每一个token输出的向量做线性变换，Softmax后输出每一个token的类别。需要提供已标注的训练资料。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;case-3&#34;&gt;Case 3
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Input：two sequences&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output：a class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example：Natural Language Inference（NLI）&lt;/p&gt;
&lt;p&gt;前提：一个人骑马越过了一架坏掉的飞机，假设：这个人在一个小餐馆里面，输出：矛盾。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case3_eg.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;输入两个句子，两个句子之间放[SEP] token，第一个句子前放[CLS] token，整串内容输入BERT，对[CLS]输出的向量做线性变换，Softmax后输出class。需要提供已标注的训练资料。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;case-4&#34;&gt;Case 4
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Extraction-based Question Answering&lt;/strong&gt;（有限制的QA，答案一定能在文章中找到。）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Input：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Document：$D={d_1,d_2,\cdots,d_N}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Query：$Q={q_1,q_2,\cdots,q_M}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;对于中文，$d_i$和$q_i$都是汉字。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output：two integers$(s,e)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Answer：$A={d_s,\cdots,d_e}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;输出两个正整数，代表答案的范围。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_eg.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/blockquote&gt;
&lt;p&gt;输入问题和文章，问题和文章之间放[SEP] token，问题前放[CLS] token，整串内容输入BERT。&lt;/p&gt;
&lt;p&gt;文章的各个token输出的向量先和一个向量（橙）做内积，再对结果做Softmax，得到答案起始的位置。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;文章的各个token输出的向量再和一个向量（蓝）做内积，再对结果做Softmax，得到答案结束的位置。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;以上只有和BERT输出做内积的两个向量是随机初始化的，即这两个向量是重头开始学习的。&lt;/p&gt;
&lt;h3 id=&#34;bert-embryology胚胎学&#34;&gt;BERT Embryology（胚胎学）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.02480&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2010.02480] Pretrained Language Model Embryology: The Birth of ALBERT (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BERT的训练需要耗费大量的资源，BERT的训练资料大概是30亿个词，是哈利波特全集的3000倍。有没有什么方法去节省计算资源？&lt;/p&gt;
&lt;p&gt;从观察BERT的训练过程开始，BERT在什么时候学会填什么样的词汇？他的填空能力是怎么增进的？&lt;/p&gt;
&lt;h3 id=&#34;pre-training-a-seq2seq-model&#34;&gt;Pre-training a seq2seq model
&lt;/h3&gt;&lt;p&gt;BERT只有预训练的Encoder。&lt;/p&gt;
&lt;p&gt;Encoder和Decoder间通过Cross Atention连接起来，在Encoder的输入中故意加一些扰动，希望Decoder输出的句子和弄坏前的句子是一样的。&lt;/p&gt;
&lt;h4 id=&#34;mass--bart&#34;&gt;MASS / BART
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.02450&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.02450] MASS: Masked Sequence to Sequence Pre-training for Language Generation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.13461&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1910.13461] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对Encoder的输入加一些扰动来弄坏原本的内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;盖住一些词&lt;/li&gt;
&lt;li&gt;删掉一些词&lt;/li&gt;
&lt;li&gt;打乱词顺序&lt;/li&gt;
&lt;li&gt;词顺序旋转&lt;/li&gt;
&lt;li&gt;混合&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/MASS_BART.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;t5---comparison&#34;&gt;T5 - Comparison
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Transfer Text-to-Text Transformer（T5）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;T5在Colossal Clean Crawled Corpus（C4）进行训练，对比了多种弄坏的方法。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/T5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;why-does-bert-work&#34;&gt;Why does BERT work?
&lt;/h3&gt;&lt;p&gt;将文字输入到BERT中，得到的输出向量称为&lt;strong&gt;embedding&lt;/strong&gt;，代表了各个token的意思，有相似意思的token有着非常相似的embeddng。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/embedding.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在语言中常常有一词多义的情况，例如“吃苹果”的“果”和“苹果手机”的“果”的含义可能相差较大。通过Cosine Similarity计算“吃苹果”和“苹果手机”的Embedding的相似度，可以发现它们之间的相似度较低。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;You shall know a word by the company it keeps.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一个词的意思可以从上下文看出来，BERT在做“填空题”的过程中所学习的内容也许就是根据上下文来预测当前被盖住的词汇。事实上，BERT之前已经有这样的方法：word embedding，word embedding中的CBOW就是把中间挖空然后预测内容。BERT所抽取出来的向量也叫做&lt;strong&gt;Contextualized word embedding&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;现在尝试将BERT拿来做蛋白质分类、DNA分类、音乐分类。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.07162&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2103.07162] Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models&amp;rsquo; Transferability (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以DNA分类为例：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/DNA.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;将DNA中的序列替换成文字，输入到BERT中，输出分类，当作是文章分类的任务来处理。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/DNA_2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;类似地，对于蛋白质，随意给各个氨基酸映射到词汇上，对于音乐，将各个音符映射到词汇上，得到了下面的结果：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/Protein_DNA_music_res.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;BERT的表现是比较好的，就算给BERT乱七八糟的句子，它可能也能把任务完成的比较好，这也说明BERT的表现可能并不完全来自于它“看得懂”文章这件事，关于BERT到底为什么好的问题可能还有很大的研究空间。&lt;/p&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=1_gRK9EIQpc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DLHLP 2020] BERT and its family - Introduction and Fine-tune - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=Bywo7m6ySlk&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DLHLP 2020] BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-lingual-bert&#34;&gt;Multi-lingual BERT
&lt;/h3&gt;&lt;p&gt;在训练的时候，会拿各种各样的语言来给BERT做“填空题”，Multi-BERT使用了104种语言来训练。拿英文的QA资料去训练，Multi-BERT就会做中文的QA问题。&lt;/p&gt;
&lt;h4 id=&#34;zero-shot-reading-comprehension&#34;&gt;Zero-shot Reading Comprehension
&lt;/h4&gt;&lt;p&gt;在英文数据集SQuAD和中文数据集DRCD上：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/zero-shot.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1909.09587&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1909.09587] Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cross-lingual-alignment&#34;&gt;Cross-lingual Alignment
&lt;/h4&gt;&lt;p&gt;也许对Multi-lingual BERT来说，不同语言间没有什么差别。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/cross-lingual.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;mean-reciprocal-rankmrr&#34;&gt;Mean Reciprocal Rank（MRR）
&lt;/h4&gt;&lt;p&gt;MRR值越高，两个不同语言align的越好（同样意思但不同语言的词汇的向量比较接近）。&lt;/p&gt;
&lt;p&gt;在1000k资料量下，相比200k资料量下的效果显著提升：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/mrr1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;一个神奇的实验&#34;&gt;一个神奇的实验
&lt;/h3&gt;&lt;p&gt;BERT可以让同样意思但不同语言的词汇的向量很接近，但在训练Multi-lingual BERT时，还是给BERT喂中文，它能够做中文填空，喂英文能够做英文填空，不会混在一起，给他喂英文他并没有填中文进去。说明来自不同语言的符号终究还是不一样，并没有完全抹掉语言的资讯。&lt;/p&gt;
&lt;p&gt;把所有英文的embedding平均起来，再把所有中文的embedding平均起来，两者相减得到的向量就是中文和英文之间的差距。给Multi-lingual BERT一句英文，得到一串embedding，将embedding加上相减得到的向量，这些向量对于Multi-lingual BERT就变成了中文的句子，再让BERT去做“填空题”，就填出了中文的答案。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/wired.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;语言的资讯还是藏在Multi-lingual BERT中：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/unsupervised%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;gpt&#34;&gt;GPT
&lt;/h2&gt;&lt;h3 id=&#34;predict-next-token&#34;&gt;Predict Next Token
&lt;/h3&gt;&lt;p&gt;GPT修改了BERT中模型的任务，GPT的任务是预测接下来的句子是什么。&lt;/p&gt;
&lt;p&gt;对于训练资料“台湾大学”，在最前面加上[BOS] token，对于[BOS] token，GPT输出一个embedding，接下来用这个embedding预测下一个应该出现的“台”这个token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/predict%20next%20sentence.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;GPT与拿掉Cross attention后的Transformer Decoder结构类似。&lt;/p&gt;
&lt;p&gt;GPT要预测下一个token，有生成的能力，GPT最知名的例子就是用GPT写了一篇关于独角兽的假新闻。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://app.inferkit.com/demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Demo – InferKit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-gpt&#34;&gt;How to use GPT?
&lt;/h3&gt;&lt;p&gt;GPT有一个更“狂“的使用方式，和人类更接近。&lt;/p&gt;
&lt;h4 id=&#34;few-shot-learning&#34;&gt;“Few-shot” Learning
&lt;/h4&gt;&lt;p&gt;例如在进行外语考试时，首先会看题目的说明（&lt;em&gt;“&amp;hellip;从A、B、C、D四个选项中选出最佳选项&amp;hellip;”&lt;/em&gt;），再会看一个例子（&lt;em&gt;“&amp;hellip;衬衫的价格是9镑15便士，所以你选择&amp;hellip;”&lt;/em&gt;）。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/few-shot%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;“Few-shot” Learning中完全没有Gradient Descent，GPT文献中将这种训练称为“In-context Learning”。&lt;/p&gt;
&lt;p&gt;类似地，还有“One-shot” Learning、甚至“Zero-shot” Learning。&lt;/p&gt;
&lt;h4 id=&#34;one-shot-learning&#34;&gt;“One-shot” Learning
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/one-shot%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;zero-shot-learning&#34;&gt;“Zero-shot” Learning
&lt;/h4&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/zero-shot%20learning.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;第三代GPT测试了42个任务：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/GPT_benchmarks.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more-1&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=DOG1L9lvsDY&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DLHLP 2020] 來自獵人暗黑大陸的模型 GPT-3 - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;beyond-text&#34;&gt;Beyond Text
&lt;/h2&gt;&lt;p&gt;不止NLP，在语音、图像上都可以用Self-Supervised Learning的技术。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/beyond%20text.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image---simclr&#34;&gt;Image - SimCLR
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.05709&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.05709] A Simple Framework for Contrastive Learning of Visual Representations (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/google-research/simclr&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;google-research/simclr: SimCLRv2 - Big Self-Supervised Models are Strong Semi-Supervised Learners (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/SimCLR.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;image---byol&#34;&gt;Image - BYOL
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.07733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.07733] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/BYOL.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;speech&#34;&gt;Speech
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/speech.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;speech-glue---superb&#34;&gt;Speech GLUE - SUPERB
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;S&lt;/strong&gt;peech processing &lt;strong&gt;U&lt;/strong&gt;niversal &lt;strong&gt;PER&lt;/strong&gt;formance &lt;strong&gt;B&lt;/strong&gt;enchmark，包含了十多个下游任务，包含内容、说话的人、情感、语义等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Toolkit：&lt;a class=&#34;link&#34; href=&#34;https://github.com/s3prl/s3prl/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;s3prl/s3prl: Self-Supervised Speech Pre-training and Representation Learning Toolkit. (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/toolkit.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>生成式对抗网络</title>
        <link>https://demo.stack.jimmycai.com/p/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Fri, 13 May 2022 17:45:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=58&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——生成式对抗网络&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;generative-adversarial-networks&#34;&gt;Generative Adversarial Networks
&lt;/h1&gt;&lt;h2 id=&#34;generator&#34;&gt;Generator
&lt;/h2&gt;&lt;h3 id=&#34;network-as-generator&#34;&gt;Network as Generator
&lt;/h3&gt;&lt;p&gt;输入$x$和一个简单的分布$z$（不固定，从一个分布中采样得到，每次使用网络时都会随机生成。），经过网络输出一个复杂的分布$y$。这样的网络称为&lt;strong&gt;Generator&lt;/strong&gt;。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/generator.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;why-distribution&#34;&gt;Why distribution？
&lt;/h3&gt;&lt;p&gt;当任务需要一些“创造力”时（同样的输入有多种可能的输出），需要预测分布。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;eg1. &lt;em&gt;Video Prediction&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;输入：吃豆人游戏的历史帧序列&lt;/p&gt;
&lt;p&gt;输出：吃豆人游戏新一帧的内容（吃豆人可能向不同的方向移动）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eg2. &lt;em&gt;Drawing&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;输入：“Character with red eyes”&lt;/p&gt;
&lt;p&gt;输出1：酷拉皮卡&lt;/p&gt;
&lt;p&gt;输出2：辉夜&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eg3. &lt;em&gt;Chatbot&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;输入：“你知道辉夜是谁吗？”&lt;/p&gt;
&lt;p&gt;输出1：“她是秀知院学生会&amp;hellip;”&lt;/p&gt;
&lt;p&gt;输出2：“她开创了忍者时代&amp;hellip;”&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generative-adversarial-networkgan&#34;&gt;Generative Adversarial Network（GAN）
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/hindupuravinash/the-gan-zoo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;hindupuravinash/the-gan-zoo: A list of all named GANs! (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anime-face-generation&#34;&gt;Anime Face Generation
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Unconditional generation&lt;/strong&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/unconditional.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/24767059&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GAN学习指南：从原理入门到制作生成Demo - 知乎 (zhihu.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discriminator&#34;&gt;Discriminator
&lt;/h3&gt;&lt;p&gt;Discriminator本身也是一个网络，Discriminator拿一张图片作为输入，输出一个数值。数字越大，表示图片越接近真实的图片。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/discriminator.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;basic-idea-of-gan&#34;&gt;Basic Idea of GAN
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;写作敌人，念做朋友。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;二者关系好比Generator造假钞，Discriminator是抓造假钞的警察，Generator越来越像，Discriminator的辨别能力越来越强。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/basic%20idea.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm
&lt;/h3&gt;&lt;p&gt;首先初始化generator $G$和discriminator $D$，在每一次训练中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step 1：定住$G$，更新$D$&lt;/p&gt;
&lt;p&gt;$D$学习去给真实二次元人物赋予更高的分数，而为生成的二次元人物赋予更低的分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/step1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;$D$分辨真正的二次元人物和生成的二次元人物之间的差异，可以当作一个分类或回归任务处理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 2：定住$D$，更新$G$&lt;/p&gt;
&lt;p&gt;$G$学习去“骗过”$D$，经过调整后，使得生成的图片能在$D$中产生更高的分数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-1/step2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;theory-behind-gan&#34;&gt;Theory behind GAN
&lt;/h2&gt;&lt;h3 id=&#34;objective&#34;&gt;Objective
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/objective.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
G^\ast=\arg\min_G Div(P_G,P_{data})
$$&lt;p&gt;其中$Div(P_G,P_{data})$是$P_G$与$P_{data}$之间的散度（Divergence），可以看作是两个分布之间某种距离，散度越大，代表两个分布越不像，散度越小，代表两个分布越相近。c.f. $w^\ast,b^\ast=\arg\min_{w,b}L$&lt;/p&gt;
&lt;h3 id=&#34;sampling&#34;&gt;Sampling
&lt;/h3&gt;&lt;p&gt;虽然不清楚$P_G$和$P_{data}$的分布，但是可以从其中采样来计算散度。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/sampling.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;discriminator-1&#34;&gt;Discriminator
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2661&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1406.2661] Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discriminator的训练目标是看到真实数据就给出一个高的分数，看到生成数据就给出一个低的分数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;：$D^\ast=\arg\max_DV(D,G)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective Function&lt;/strong&gt; for $D$：$V(G,D)=E_{y\sim P_{data}}[\log D(y)]+E_{y\sim P_G}[\log(1-D(y))]$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$V(D,G)$是交叉熵的相反数，Discriminator可以等同于一个分类器，最小化交叉熵。而正好$\max_DV(D,G)$就和&lt;strong&gt;JS散度&lt;/strong&gt;有关。&lt;/p&gt;
&lt;p&gt;最开始，$P_G$和$P_{data}$混在一起，散度很小，Discriminator难以分辨哪些数据是生成数据而哪些数据是真实数据，即Discriminator难以区分小的$max_D{V(D,G)}$。&lt;/p&gt;
&lt;p&gt;若$P_G$和$P_{data}$散度很大，$max_DV(D,G)$比较大，DIscriminator则很容易区分生成数据和真实数据。&lt;/p&gt;
$$
G^\ast=\arg\min_G\max_DV(G,D)
$$&lt;h3 id=&#34;why-js-divergence&#34;&gt;Why JS Divergence？
&lt;/h3&gt;&lt;p&gt;除了JS散度，也可以使用例如KL散度等其他散度。如何设计目标函数，得到不同的散度，在f-GAN论文中有详细的证明。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1606.00709&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1606.00709] f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tips-for-gan&#34;&gt;Tips for GAN
&lt;/h2&gt;&lt;h3 id=&#34;js散度的问题&#34;&gt;JS散度的问题
&lt;/h3&gt;&lt;p&gt;在大多数情况下，$P_G$和$P_{data}$重复的部分非常少。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据本身的特性：数据是高维空间中的低维流形，重叠的部分可以忽略。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;流形学习的观点认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。例如单位圆上有无穷多个点，无法用二唯坐标系上的点来表示圆上所有的点，而若使用极坐标，圆心在原点的圆只需一个参数——半径，就可以确定。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/manifold.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;采样：即使$P_G$和$P_{data}$有重叠，若采样的点不够多，对Discriminator来说也是没有重叠的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而以上的问题会导致JS散度出现问题。&lt;/p&gt;
&lt;p&gt;在两个分布完全不重叠时，无论两个分布的中心距离有多近，其JS散度都是一个常数$\log2$，无法判断哪个case更好，从而无法更新参数。&lt;/p&gt;
&lt;p&gt;参考证明：&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/MorStar/p/14882813.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;JS散度(Jensen–Shannon divergence) - MorStar - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/js%20div%20problem.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;直观来看，如果两个分布不重叠，二分类的准确率几乎可以达到100%。&lt;/p&gt;
&lt;h3 id=&#34;wasserstein-distance&#34;&gt;Wasserstein distance
&lt;/h3&gt;&lt;p&gt;考虑两个分布$P$和$Q$，想象一台推土机，$P$是一堆土，$Q$是要堆放的目的地，把$P$挪动到$Q$的平均距离就是Wasserstein distance。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;考虑更复杂的情况，移动的方案可能有无穷多种。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;穷举所有的移动方法，看哪一个移动方法可以让平均的距离最小，最小的值就是Wasserstein distance。但计算方法似乎比较复杂，要计算距离还需要求解这样一个最优化问题。&lt;/p&gt;
&lt;p&gt;假设现在我们已经可以计算Wasserstein distance，就可以解决JS散度带来的问题。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;wgan&#34;&gt;WGAN
&lt;/h3&gt;&lt;p&gt;WGAN使用Wasserstein distance取代JS散度。&lt;/p&gt;
$$
\max_{D\in 1-Lipschitz}\{E_{y\sim P_{data}}[D(y)]-E_{y\sim P_G}[D(y)]\}
$$&lt;p&gt;
$D\in 1-Lipschitz$：$D$需要是一个足够平滑的函数，不能是变动很剧烈的函数，若没有这个限制，单纯让生成数据越小越好，真实数据越大越好，在生成数据和真实数据没有重叠的情况下，会给真实数据无穷大的正值而给生成数据无穷小的负值。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/wasserstein%20distance4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;din-1-lipschitz&#34;&gt;$D\in 1-Lipschitz$
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Original WGAN：Weight&lt;/p&gt;
&lt;p&gt;强制参数$w$在$c$和$-c$之间，参数更新后若$w\gt c$，则$w=c$，若$w\lt -c$，则$w=-c$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Improved WGAN：Gradient Penalty&lt;/p&gt;
&lt;p&gt;在真实数据和生成数据中各取一个样本，两点连线中再取一个样本，要求这个点的梯度接近1。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-2/gradient%20penalty.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.00028&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.00028] Improved Training of Wasserstein GANs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spectral Normalization（SNGAN）：让梯度模长在任何地方都小于1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1802.05957&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1802.05957] Spectral Normalization for Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;more-tips&#34;&gt;More Tips
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Tops from Soumith
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/soumith/ganhacks&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;soumith/ganhacks: starter from &amp;ldquo;How to Train a GAN?&amp;rdquo; at NIPS2016 (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tips in DCGAN：Guideline for network architecture design for image generation
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1511.06434&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1511.06434] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Improved techniques for training GANs
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1606.03498&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1606.03498] Improved Techniques for Training GANs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tips from BigGAN
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1809.11096&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1809.11096] Large Scale GAN Training for High Fidelity Natural Image Synthesis (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GAN的训练需要Generator和Discriminator共同配合，若有其中一方不再进步，另一方也会停下来，&lt;em&gt;Generator和Discriminator需要棋逢敌手&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;gan-for-sequence-generation&#34;&gt;GAN for Sequence Generation
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/GAN%20seq.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Decoder参数改变后，经过max输出的文字可能不会发生改变，就无法完成参数更新。可以用RL来训练。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.09922&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.09922] Training language GANs from Scratch (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/scratch.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;more-generative-models&#34;&gt;More Generative Models
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/playlist?list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GAN（Full version）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/8zomhgKrsmQ&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Variational Autoencoder（VAE）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/uXY18nzdSsM&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;FLOW-based Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation-of-generation&#34;&gt;Evaluation of Generation
&lt;/h2&gt;&lt;h3 id=&#34;quality-of-image&#34;&gt;Quality of Image
&lt;/h3&gt;&lt;p&gt;评价图像质量最直接的做法是找人来看，在Generator研究初期，有人会选几张图说“看，这个结果应该比目前的结果都要好，应该是SOTA。”，这显然不够客观，如何自动地评价生成图像的质量？&lt;/p&gt;
&lt;p&gt;一个方法是使用图像分类器，输入一张图片$y$，输出图片属于各个类的概率分布$p(c|y)$，分布越集中，产生的图片可能就越好。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/quality%20of%20img.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;diversity---mode-collapse&#34;&gt;Diversity - Mode Collapse
&lt;/h3&gt;&lt;p&gt;仅使用以上这种方法评估图像质量时可能会遇到Mode Collapse（模式坍塌）的问题。&lt;/p&gt;
&lt;p&gt;训练GAN的过程可能会遇到以下问题：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20collapse.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Generator生成出来的图片可能来来去去都是那几张：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20collapse2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;直觉上看，这样的点是Discriminator的“盲点”，Discriminator没办法看出这样的图片是假的，当Generator学会产生这种图片后，就永远都可以骗过Discriminator。&lt;/p&gt;
&lt;h3 id=&#34;diversity---mode-dropping&#34;&gt;Diversity - Mode Dropping
&lt;/h3&gt;&lt;p&gt;Mode Dropping可能比Mode Collapse更难侦测出来，产生出来的数据可能只能贴近已有真实数据的分布，但真实的数据分布的多样性其实是更大的。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20dropping.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;一个人脸生成的例子：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/mode%20dropping2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;diversity&#34;&gt;Diversity
&lt;/h3&gt;&lt;p&gt;过去判定多样性的做法是将一批图片输入到图片分类器中，计算所有图片的概率分布的均值，若平均的分布非常集中，则代表多样性不够。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/diversity.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;若输入这批图片产生的分布都非常不同，平均后的结果非常平坦，则代表多样性是足够的。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/diversity2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Diversity和Quality的评估方式相反，Diversity看的是一批图片的平均，而Quality看的是一张图片。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inception Score（IS）&lt;/strong&gt;：质量越高，多样性越大，则IS越高。&lt;/p&gt;
&lt;h3 id=&#34;fréchet-inception-distancefid&#34;&gt;Fréchet Inception Distance（FID）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.08500&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1706.08500] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;取Softmax前的输出向量，假设真实图像和生成图像都是高斯分布，计算这两个高斯分布之间的Fréchet distance，这个距离越小说明真实图像与生成图像越接近，生成图像品质越高。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/FID.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可能需要大量的图片样本才能做到。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1711.10337&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1711.10337] Are GANs Created Equal? A Large-Scale Study (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;we-dont-want-memory-gan&#34;&gt;We don&amp;rsquo;t want memory GAN
&lt;/h3&gt;&lt;p&gt;生成出来的图片有可能和训练集一模一样，这种情况下FID非常小，也有可能仅仅把图片翻转，这样很难侦测出来。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1511.01844&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1511.01844] A note on the evaluation of generative models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;more-about-evaluation&#34;&gt;More about evaluation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1802.03446&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1802.03446] Pros and Cons of GAN Evaluation Measures (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conditional-generation&#34;&gt;Conditional Generation
&lt;/h2&gt;&lt;p&gt;前面所提到的Unconditional GAN的输入都是一个随机的分布。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/conditional%20generation.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;例如要做文本转图片，需要给模型一个文本输入$x$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/text-to-img.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;conditional-gan&#34;&gt;Conditional GAN
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/conditional%20GAN.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在Unconditional GAN中，Discriminator接受一个图片$y$作为输入，输出一个数值，代表图片是真实的或是生成的，但这样的方法无法解Conditional GAN的问题，Generator可以产生非常接近真实的图片，但是忽略了输入的条件。&lt;/p&gt;
&lt;p&gt;Conditional GAN中，需要成对的训练数据。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-3/conditional%20GAN2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Conditional GAN也可以用图像来生成图像，例如图像去雾，黑白转彩色，白天转黑夜，素描转实物。也叫&lt;strong&gt;Image translation&lt;/strong&gt;或&lt;strong&gt;pix2pix&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;通常可以将GAN和有监督学习结合，得到更好的结果。&lt;/p&gt;
&lt;h3 id=&#34;其他应用&#34;&gt;其他应用
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Sound-to-image&lt;/li&gt;
&lt;li&gt;Talking Head Generation
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.08233&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1905.08233] Few-Shot Adversarial Learning of Realistic Neural Talking Head Models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learning-from-unpaired-data&#34;&gt;Learning from Unpaired Data
&lt;/h2&gt;&lt;p&gt;有一堆$x$和一堆$y$，但$x$和$y$不成对（未标注）。pseudo labeling（伪标签）和back translation（反向翻译）都需要一些成对的数据。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/unpaired%20data.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;例如在影像风格转换，将定义域$\mathcal{X}$真人头像转为定义域$\mathcal{Y}$二次元头像：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/image%20style%20transfer.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;cycle-gan&#34;&gt;Cycle GAN
&lt;/h3&gt;&lt;p&gt;输入一个Domain $\mathcal{X}$，输出Domain $\mathcal{Y}$。但如果仍然按照之前的方法学习，GAN无法判断生成的二次元图像是否与输入的真人图像是相似的，可能会将输入当作为高斯噪声，忽略输入的内容。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/cycle%20gan.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Cycle GAN中会训练两个Generator，第一个Generator $G_{\mathcal{X}\rightarrow \mathcal{Y}}$将$\mathcal{X}$ domain的图变成$\mathcal{Y}$ domain的图，第二个Generator $G_{\mathcal{Y}\rightarrow \mathcal{X}}$将$\mathcal{Y}$ domain的图还原为$\mathcal{X}$ domain的图。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/cycle%20gan2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Cycle GAN能保证真实图片和生成图片有一些关系，但如何保证这种关系是我们想要的呢？例如输入一个戴眼镜的人，$G_{\mathcal{X}\rightarrow \mathcal{Y}}$将眼镜转成痣，但$G_{\mathcal{Y}\rightarrow \mathcal{X}}$又会把痣转成眼镜。理论上可能会出现这样的情况，不过在实际中这种情况往往不会出现。&lt;/p&gt;
&lt;p&gt;类似地还有Disco GAN和Dual GAN，思想与Cycle GAN基本相同。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1703.05192&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1703.05192] Learning to Discover Cross-Domain Relations with Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.02510&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.02510] DualGAN: Unsupervised Dual Learning for Image-to-Image Translation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外还有能够在多种风格之间转换的Star GAN：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.02510&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.02510] DualGAN: Unsupervised Dual Learning for Image-to-Image Translation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/starGAN.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;selfie2anime&#34;&gt;SELFIE2ANIME
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://selfie2anime.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Selfie2Anime&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1907.10830&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1907.10830] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;text-style-transfer&#34;&gt;Text Style Transfer
&lt;/h3&gt;&lt;p&gt;文字风格转换，例如将负面的句子转为正面的句子。和Cycle GAN的做法类似。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/6/6-4/text%20style%20transfer.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Unsupervised Abstractive Summarization
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.02851&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1810.02851] Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised Translation
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1710.04087&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1710.04087] Word Translation Without Parallel Data (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1710.11041&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1710.11041] Unsupervised Neural Machine Translation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised ASR
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1804.00316&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1804.00316] Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1812.09323&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1812.09323] Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.04100&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.04100] Completely Unsupervised Speech Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>各式各样的自注意力机制</title>
        <link>https://demo.stack.jimmycai.com/p/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
        <pubDate>Sat, 30 Apr 2022 19:29:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=51&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——各式各样的神奇的自注意力&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-attention变型&#34;&gt;Self-attention变型
&lt;/h1&gt;&lt;p&gt;Sequence length=$N$，产生的$N$个key向量和$N$个query向量两两之间做Dot-product，共$N^2$平方次计算，得到一个$N\times N$的矩阵Attention Matrix，根据该矩阵对value向量加权求和。Self-attention往往是模型里面的一个小模块，当$N$很大时，模型的主要计算量都集中在Self-attention上，对于计算速度的优化往往都是用在影像上。&lt;/p&gt;
&lt;h2 id=&#34;human-knowledge&#34;&gt;Human knowledge
&lt;/h2&gt;&lt;h3 id=&#34;local-attention--truncated-attention&#34;&gt;Local Attention / Truncated Attention
&lt;/h3&gt;&lt;p&gt;某些问题不用看完整的序列，只用看左右邻居的信息即可，将其他位置的信息设为0。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/local%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可以加快运算速度，但每次做Attention只能看到某个小范围的信息，和CNN的差别就不大了。&lt;/p&gt;
&lt;h3 id=&#34;stride-attention&#34;&gt;Stride Attention
&lt;/h3&gt;&lt;p&gt;与Local Attention类似，看更远的邻居，例如看三个位置之前和三个位置之后的信息。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/stride%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;global-attention&#34;&gt;Global Attention
&lt;/h3&gt;&lt;p&gt;在原始序列中加入一些特殊的token，代表该位置要做Global Attention，Global Attention会从序列中的每一个token去收集信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attend to every token -&amp;gt; 收集所有的信息&lt;/li&gt;
&lt;li&gt;Attended by every token -&amp;gt; 能获取全局的信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Global Attention有两种做法，可以从原始序列中选择一些已有的字符（例如BERT中的[CLS]标志、句号等）作为token，或外加额外的token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/global%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;papers&#34;&gt;Papers
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2004.05150&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2004.05150] Longformer: The Long-Document Transformer (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2007.14062&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2007.14062] Big Bird: Transformers for Longer Sequences (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;clustering&#34;&gt;Clustering
&lt;/h2&gt;&lt;p&gt;在Attention矩阵，可能有些值很大，有些值特别小，可以直接把较小的权值置0，问题在于如何快速估计哪些地方的Attention值较高，而哪些地方的Attention值较低。&lt;/p&gt;
&lt;h3 id=&#34;reformer&#34;&gt;Reformer
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=rkgNKkHtvB&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Reformer: The Efficient Transformer | OpenReview&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.05997&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.05997] Efficient Content-Based Sparse Attention with Routing Transformers (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;步骤&#34;&gt;步骤
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step 1：对query和key做聚类&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/clustering1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;聚类有很多可以加速的方法，对query和key做聚类时，会采取精度相对较低但速度很快的方法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 2 对同一Cluster的query和key计算Attention分数&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/clustering2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;不属于同一类的直接将Attention值设为0。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learnable-patterns&#34;&gt;Learnable Patterns
&lt;/h2&gt;&lt;h3 id=&#34;sinkhorn-sorting-network&#34;&gt;Sinkhorn Sorting Network
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.11296&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.11296] Sparse Sinkhorn Attention (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;让机器去学习两个位置的向量要不要做Attention。&lt;/p&gt;
&lt;p&gt;Sinkhorn Sorting Network里面有一个额外需要学习的矩阵，来决定哪些地方需要计算Attention。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/sinkhorn%20sorting%20network.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;多个向量会共享一个矩阵以加快计算速度。（例如对于长度为100的输入，会分成10组，每组都是同一个矩阵。）&lt;/p&gt;
&lt;h2 id=&#34;representative-key&#34;&gt;Representative key
&lt;/h2&gt;&lt;p&gt;Attention矩阵中有很多冗余列，往往无需$N\times N$的Attention矩阵。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;linformer&#34;&gt;Linformer
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.04768&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.04768] Linformer: Self-Attention with Linear Complexity (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从$N$个key中选出最有代表性的$K$个key，只需算$N\times K$的Attention矩阵。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;reduce-number-of-keys&#34;&gt;Reduce Number of Keys
&lt;/h4&gt;$$
M_{d\times N}\times M_{N\times K}=M_{d\times K}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer3.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;Compressed Attention&lt;/strong&gt;中的处理方式是对输入的较长序列用CNN去处理，得到一个较长的序列。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/compressed%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1801.10198&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1801.10198] Generating Wikipedia by Summarizing Long Sequences (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kq-first-rightarrow-vk-first&#34;&gt;$k,q$ first $\rightarrow$ $v,k$ first
&lt;/h2&gt;&lt;h3 id=&#34;忽略softmax的情况&#34;&gt;忽略Softmax的情况
&lt;/h3&gt;$$
O\approx VK^TQ
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
$$
O\approx V[K^TQ]\rightarrow O\approx [VK^T]Q
$$&lt;ul&gt;
&lt;li&gt;对于计算方法$O\approx V[K^TQ]$：
&lt;ul&gt;
&lt;li&gt;$A=K^TQ$：$N\times d\times N$&lt;/li&gt;
&lt;li&gt;$O=VA$：$d&amp;rsquo;\times N\times N$&lt;/li&gt;
&lt;li&gt;求和：$(d+d&amp;rsquo;)N^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;而对于计算方法$O\approx [VK^T]Q$：
&lt;ul&gt;
&lt;li&gt;$M_1=VK^T$：$d&amp;rsquo;\times N\times d$&lt;/li&gt;
&lt;li&gt;$M_2=M_1Q$：$d&amp;rsquo;\times d\times N$&lt;/li&gt;
&lt;li&gt;求和：$2d&amp;rsquo;dN$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;加回softmax&#34;&gt;加回Softmax
&lt;/h3&gt;&lt;p&gt;已知存在一个$\phi$，使得有&lt;a class=&#34;link&#34; href=&#34;#%e5%ae%9e%e7%8e%b0&#34; &gt;以下式子&lt;/a&gt;成立：&lt;/p&gt;
&lt;blockquote&gt;
$$
&gt; \exp(q\cdot k)\approx \phi(q)\cdot\phi(k)
&gt; $$&lt;/blockquote&gt;
$$
\begin{aligned}
b^1=\sum_{i=1}^Na&#39;_{1,i}v^i&amp;=\sum_{i=1}^N\frac{\exp{(q^1\cdot k^i)}}{\sum_{j=1}^{N}\exp{(q^1\cdot k^j)}}v^i \\
&amp;=\sum_{i=1}^N\frac{\phi(q^1)\cdot\phi(k^i)}{\sum_{j=1}^{N}\phi(q^1)\cdot\phi(k^j)}v^i \\
&amp;=\frac{\sum_{i=1}^{N}[\phi(q^1)\cdot\phi(k^i)]v^i}{\sum_{j=1}^{N}\phi(q^1)\cdot\phi(k^j)}
\end{aligned}
$$$$
\sum_{j=1}^{N}\phi(q^1)\cdot\phi(k^j)=\phi(q^1)\cdot\sum_{j=1}^{N}\phi(k^j)
$$$$
\phi(q^1)=
\begin{bmatrix}
q_1^1 \\
q_2^1 \\
\vdots
\end{bmatrix}
\quad
\phi(k^1)=
\begin{bmatrix}
k_1^1 \\
k_2^1 \\
\vdots
\end{bmatrix}
$$&lt;p&gt;
则有：
$$
\begin{aligned}
&amp;amp;\quad\sum_{i=1}^N[\phi(q^1)\cdot \phi(k^i)]v^i \
&amp;amp;=[\phi(q^1)\cdot\phi(k^1)]v^1+[\phi(q^1)\cdot\phi(k^2)]v^2+\cdots \
&amp;amp;=(q_1^1k_1^1+q_2^1k_2^1+\cdots)v^1+(q_1^1k_1^2+q_2^1k_2^2+\cdots)v^2+\cdots \
&amp;amp;=(q_1^1k_1^1v^1+q_2^1k_2^1v^1+\cdots)+(q_1^1k_1^2v^2+q_2^1k_2^2v^2+\cdots)+\cdots \
&amp;amp;=q_1^1(k_1^1v^1+k_1^2v^2+\cdots)+q_2^1(k_2^1v^1+k_2^2v^2+\cdots)+\cdots \&lt;/p&gt;
&lt;p&gt;\end{aligned}
$$
设$\phi(q^1)$的维度为$M$，则：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;即在分子中的$M$个向量中，每一个向量都是通过，拿出$\phi(k^1)$、$\phi(k^2)$、&amp;hellip;、$\phi(k^N)$中的第$i$个分量，对$v^1$、$v^2$、&amp;hellip;、$v^N$做加权和。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;可以看出，每次计算$b^i$时，除了$\phi(q^i)$以外，其他部分没有发生变化，这部分内容无需&lt;strong&gt;再重复计算&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;self-attention中的qkv&#34;&gt;Self-attention中的$q$、$k$、$v$
&lt;/h3&gt;&lt;p&gt;计算$b^1$：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;产生的$M$个向量以及$\sum_{j=1}^{N}\phi(k^j)$在后面$b^2$、$b^3$、$b^4$的计算中无需再进行计算。&lt;/p&gt;
&lt;h3 id=&#34;实现&#34;&gt;实现
&lt;/h3&gt;&lt;p&gt;关于$\phi$的实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1812.01243&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1812.01243] Efficient Attention: Attention with Linear Complexities (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://linear-transformers.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linear Transformers (linear-transformers.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.02143&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2103.02143] Random Feature Attention (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2009.14794&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2009.14794] Rethinking Attention with Performers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;new-framework&#34;&gt;New framework
&lt;/h2&gt;&lt;h3 id=&#34;无需qk产生attentionsynthesizer&#34;&gt;无需$q,k$产生Attention——Synthesizer
&lt;/h3&gt;$$
\begin{bmatrix}
\alpha_{1,1} &amp; \alpha_{1,2} &amp; \alpha_{1,3} &amp; \alpha_{1,4} \\
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{2,3} &amp; \alpha_{2,4} \\
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp; \alpha_{3,4} \\
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp; \alpha_{4,4} \\
\end{bmatrix}
$$&lt;h2 id=&#34;attention-free&#34;&gt;Attention-free
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.03824&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.03824] FNet: Mixing Tokens with Fourier Transforms (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.08050&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.08050] Pay Attention to MLPs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.01601&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2105.01601] MLP-Mixer: An all-MLP Architecture for Vision (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/summary.png&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
        </item>
        <item>
        <title>Attention Is All You Need</title>
        <link>https://demo.stack.jimmycai.com/p/attention-is-all-you-need/</link>
        <pubDate>Mon, 25 Apr 2022 23:01:32 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/attention-is-all-you-need/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=49&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——Transformer&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;transformer&#34;&gt;Transformer
&lt;/h1&gt;&lt;p&gt;Transformer是一个Sequence-to-sequence（Seq2seq）的模型，输出的长度由模型自己来决定。&lt;/p&gt;
&lt;h2 id=&#34;sequence-to-sequence&#34;&gt;Sequence-to-sequence
&lt;/h2&gt;&lt;h3 id=&#34;应用&#34;&gt;应用
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Maching Translation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speech Translation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Text-to-Speech（TTS）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;questions-answeringqa&#34;&gt;Questions Answering（QA）
&lt;/h4&gt;$$
\text{question}, \text{context} \stackrel{Seq2seq}{\longrightarrow} \text{answer}
$$&lt;h4 id=&#34;multi-label-classification&#34;&gt;Multi-label Classification
&lt;/h4&gt;$$
\text{data} \stackrel{Seq2seq}{\longrightarrow} \text{class 7, class 9, class 13}
$$&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1909.03434&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1909.03434] Order-free Learning Alleviating Exposure Bias in Multi-label Classification (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1707.05495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1707.05495] Order-Free RNN with Visual Attention for Multi-Label Classification (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;object-detection&#34;&gt;Object Detection
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.12872&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2005.12872] End-to-End Object Detection with Transformers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;结构&#34;&gt;结构
&lt;/h3&gt;$$
\text{Encoder}\longrightarrow \text{Decoder}
$$&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1409.3215&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1409.3215] Sequence to Sequence Learning with Neural Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.03762&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1706.03762] Attention Is All You Need (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;encoder&#34;&gt;Encoder
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;输入一排向量：${x^1,x^2,x^3,x^4}$&lt;/li&gt;
&lt;li&gt;输出一排向量：${h^1,h^2,h^3,h^4}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Self-attention、RNN、CNN&amp;hellip;均可用来作为Encoder。&lt;/p&gt;
&lt;h3 id=&#34;transformer-encoder&#34;&gt;Transformer Encoder
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/seq2seq.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在Transformer Encoder中，加入了Residual Connection，经过Self-attention输出的向量加上原输入的向量后当作新的输出向量。&lt;/p&gt;
&lt;p&gt;得到Residual的结果以后，进行Normalization，但此处使用的是Layer Normalization而非Batch Normalization。对于输入的向量，Layer Norm会计算它的均值$m$和标准差$\sigma$，与Batch Norm不同点在于，Batch Norm是对不同特征、样本的同一个维度计算均值和标准差，而Layer Norm是对同一个特征、样本的不同维度去计算均值和标准差。&lt;/p&gt;
&lt;p&gt;Layer Norm的结果将作为FC的输入，经过FC Network得到新的向量，在FC层也同样地加入了Residual Connection，得到的结果再做一次Layer Norm，则得到了此Block的输出。&lt;/p&gt;
&lt;p&gt;即一个Block的结构如下：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/encoder.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;此Block会重复N次，组成Transformer的Encoder，BERT与Transformer Encoder采用了相同的结构。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/encoder2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;more&#34;&gt;More
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.04745&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2002.04745] On Layer Normalization in the Transformer Architecture (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2003.07845&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2003.07845] PowerNorm: Rethinking Batch Normalization in Transformers (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;decoderautoregressiveat&#34;&gt;Decoder——Autoregressive（AT）
&lt;/h2&gt;&lt;h3 id=&#34;decoder的运作方式&#34;&gt;Decoder的运作方式
&lt;/h3&gt;&lt;p&gt;除了Encoder产生的输出以外，Decoder中还会加入一个BOS（Begin of Sequence）符号（token），用来表示开始，BOS token是一个One-hot表示的向量。&lt;/p&gt;
&lt;p&gt;输出一个向量，向量的长度应该和Vocabulary Size相等来表示所有的汉字（对于中文，Vocabulary Size就是所有汉字的数量），每一个中文对应向量中的一个数值，这个向量是经过Soft-max得到的，取最大的作为输出的文字，Decoder会将这个输出的文字的One-hot向量作为新的输入。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/decoder.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;但是按照这样的运作方式，后面会产生无穷尽的文字，像文字接龙一样一直不能停下来，所以，还应该加一个EOS（End of Sequence） token来表示文字的结束，一般情况下，EOS token和BOS token都用一个相同的向量来表示，故向量的长度应该为Vocabulary Size + 1。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/decoder2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在这种运作方式下，某步的错误预测可能影响后面的预测（“一步错，步步错。”），具体参考最后一节&lt;a class=&#34;link&#34; href=&#34;#Scheduled-Sampling&#34; &gt;Scheduled Sampling&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;transformer-decoder&#34;&gt;Transformer Decoder
&lt;/h3&gt;&lt;p&gt;Transformer中的Decoder和Encoder结构类似，除中间的Multi-Head Attention和Add &amp;amp; Norm结构外，在第一次Multi-Head Attention计算中加入了Mask。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/encodervsdecoder.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;masked-self-attention&#34;&gt;Masked Self-attention
&lt;/h4&gt;&lt;p&gt;在产生$b^1$的时候，只能考虑$a^1$的信息，而不能考虑$a^2$、$a^3$、$a^4$的信息；在产生$b^2$的时候，只能考虑$a^1$、$a^2$的信息，而不能考虑$a^3$、$a^4$的信息。&lt;/p&gt;
&lt;p&gt;具体来说，在产生$b^2$时，只会拿第二个位置的query去跟第一个位置的key和第二个位置的key来计算Attention，而不管第三、四个位置的key。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/mask.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;对于Decoder而言，先有$a^1$才有$a^2$，才有接下来的$a^3$、$a^4$，计算$b^2$的时候无法考虑$a^3$、$a^4$。&lt;/p&gt;
&lt;h2 id=&#34;decodernon-autoregressivenat&#34;&gt;Decoder——Non-autoregressive（NAT）
&lt;/h2&gt;&lt;h3 id=&#34;at-vs-nat&#34;&gt;AT v.s. NAT
&lt;/h3&gt;&lt;p&gt;AT分别输入BOS token、$w_1$、$w_2$、$w_3$、EOS token，而NAT一次输入一整排BOS token。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/ATvsNAT.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;并行&lt;/li&gt;
&lt;li&gt;输出长度可控&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;问题：模型如何知道输出的长度，从而确定输入的BOS token的数量？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用一个模型来预测输出长度；&lt;/li&gt;
&lt;li&gt;输出一个很长的句子，忽略EOS token以后的内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NAT的表现往往比AT要差（Multi-modality）。&lt;/p&gt;
&lt;h2 id=&#34;encoder-decoder&#34;&gt;Encoder-Decoder
&lt;/h2&gt;&lt;p&gt;Transformer中由&lt;strong&gt;Cross Attention&lt;/strong&gt;模块来连接Encoder和Decoder。该模块接收Encoder的两个输出和Decoder的一个输出作为输入。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/cross%20attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;将BOS token输入到Decoder的Masked Self-attention模块后，将输出的向量进行线性变换得到query $q$，再将$q$与Encoder中的key $k^1$、$k^2$、$k^3$计算Attention分数，与value $v^1$、$v^2$、$v^3$相乘加权求和得到$v$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/cross%20attention2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Cross Attention比Self-Attention出现要更早。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/7472621&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Listen, attend and spell: A neural network for large vocabulary conversational speech recognition | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transformer中，Decoder中每一层都与Encoder的最后一层做Cross Attention，也有论文的工作中尝试了与其他层的不同的连接方式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.08081&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2005.08081] Layer-Wise Multi-View Decoding for Improved Natural Language Generation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training&#34;&gt;Training
&lt;/h2&gt;&lt;p&gt;以下以一段标签为“机器学习”的语音数据为例。&lt;/p&gt;
&lt;p&gt;在Decoder中输入BOS token后，输出的向量应该和“机”对应的向量越接近越好，即通过计算两个向量的交叉熵，交叉熵越小越好，这个过程和分类非常相似。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/training.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;包括最后一个EOS token，模型希望最后一个字所输出的内容与EOS token的One-hot向量是接近的。&lt;/p&gt;
&lt;p&gt;在训练过程中，Decoder的输入是真实标签，训练过程中会给Decoder看正确答案，即给Decoder输入BOS token和“机”以后，希望模型的输出是“器”，给Decoder输入BOS token、“机”和“器”之后，希望模型输出的是“学”。这种方法叫做&lt;strong&gt;Teacher Forcing&lt;/strong&gt;，将正确答案作为输入。&lt;/p&gt;
&lt;p&gt;训练时Decoder可以看到完全正确的信息，而测试的时候Decoder可能会看到一些错误的信息，可能会导致“一步错，步步错。”，训练与测试不一致的现象叫做&lt;strong&gt;exposure bias&lt;/strong&gt;，方法是在学习时，给Decoder的输入加入一些错误的信息，具体参考最后一节&lt;a class=&#34;link&#34; href=&#34;#Scheduled-Sampling&#34; &gt;Scheduled Sampling&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;tips&#34;&gt;Tips
&lt;/h2&gt;&lt;h3 id=&#34;copy-mechanism&#34;&gt;Copy Mechanism
&lt;/h3&gt;&lt;p&gt;某些信息并不需要机器来学习，可能是从输入信息中复制出来，例如聊天机器人中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;eg1&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;User：你好，我是&lt;em&gt;库洛洛&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;Machine：&lt;em&gt;库洛洛&lt;/em&gt;你好，很高兴认识你。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eg2&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;User：小杰&lt;em&gt;不能使用念能力了&lt;/em&gt;！&lt;/p&gt;
&lt;p&gt;Machine：你所谓的*「不能使用念能力」*是什么意思？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;又例如从文章中提取摘要这一任务，从文章中复制一些信息是很模型很关键的能力。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.04368&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1704.04368] Get To The Point: Summarization with Pointer-Generator Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1603.06393&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1603.06393] Incorporating Copying Mechanism in Sequence-to-Sequence Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;guided-attention&#34;&gt;Guided Attention
&lt;/h3&gt;&lt;p&gt;在一些任务中（例如语音辨识、TTS等），对于输入的每一个内容都要看到，不能漏掉某些信息。&lt;/p&gt;
&lt;p&gt;Guided Attention要求机器以特定的方式完成Attention的计算，应该由左向右分别产生输出。例如在TTS中，应该先看最左边的文字产生输出，最后看最右的文字产生输出。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monotonic Attention Location-aware attention&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;beam-search&#34;&gt;Beam Search
&lt;/h3&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/beam%20search.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;要找到最优解，暴力搜索难以计算。通过Beam Search找一个不是完全精准的解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.09751&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1904.09751] The Curious Case of Neural Text Degeneration (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设一个任务的答案非常明确，Beam Search会比较有帮助，但对于一些答案不唯一的任务（例如文本补全），分数最高的路径可能结果并不是很好，往往需要在Decoder中加入随机性（noise）。&lt;/p&gt;
&lt;h2 id=&#34;optimizing-evaluation-metrics&#34;&gt;Optimizing Evaluation Metrics
&lt;/h2&gt;&lt;p&gt;训练时使用交叉熵，在评估时使用BLEU。BLEU不可微分，无法作为Loss。不过对于无法优化的Loss，可以将其当作Reinforcement Learning（RL）的reward，Decoder作为Agent，将其看作是RL问题来解决。&lt;/p&gt;
&lt;h2 id=&#34;scheduled-sampling&#34;&gt;Scheduled Sampling
&lt;/h2&gt;&lt;p&gt;Schedule可能会影响计算的并行化，对于Transformer的Scheduled Sampling另有方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1506.03099&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1506.03099] Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.07651&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1906.07651] Scheduled Sampling for Transformers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.04331&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1906.04331] Parallel Scheduled Sampling (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>自注意力机制</title>
        <link>https://demo.stack.jimmycai.com/p/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
        <pubDate>Sat, 23 Apr 2022 17:45:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Wv411h7kN?p=38&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;李宏毅2021/2022春机器学习课程——自注意力机制（Self-attention）&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;self-attention&#34;&gt;Self-attention
&lt;/h1&gt;&lt;p&gt;考虑两种不同的输入：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入是一个向量&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入是一排向量&lt;/strong&gt;（输入的向量个数可能会改变）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;将一排向量作为输入&#34;&gt;将一排向量作为输入
&lt;/h2&gt;&lt;h3 id=&#34;方法&#34;&gt;方法
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;One-hot Encoding&lt;/li&gt;
&lt;li&gt;Word Embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;输入&#34;&gt;输入
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;一段语音窗口&lt;/li&gt;
&lt;li&gt;一张图&lt;/li&gt;
&lt;li&gt;分子结构&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;输出&#34;&gt;输出
&lt;/h3&gt;&lt;h4 id=&#34;n个向量对应n个标签&#34;&gt;N个向量对应N个标签
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;句子中每个词的词性：I saw a saw -&amp;gt; N V DET N&lt;/li&gt;
&lt;li&gt;社交网络中每个人的购买意向：甲 -&amp;gt; buy;乙 -&amp;gt; not&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;n个向量对应一个标签&#34;&gt;N个向量对应一个标签
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;情感分析：this is good -&amp;gt; positive&lt;/li&gt;
&lt;li&gt;分子属性分析：一个分子图 -&amp;gt; 亲水性&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;n个向量对应多个标签seq2seq&#34;&gt;N个向量对应多个标签（seq2seq）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;机器翻译&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fully-connected-network&#34;&gt;Fully-connected Network
&lt;/h2&gt;&lt;p&gt;使用全连接网络，设置窗口大小，每次输入邻近的多个词。但限制于窗口大小，无法考虑整个句子的影响，且窗口覆盖整个句子比较困难。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/fc.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;self-attention-1&#34;&gt;Self-attention
&lt;/h2&gt;&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：一排向量${a^1,a^2,a^3,a^4}$&lt;/li&gt;
&lt;li&gt;输出：一排向量${b^1,b^2,b^3,b^4}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;$b^1$、$b^2$、$b^3$、$b^4$分别都是考虑了$a^1$、$a^2$、$a^3$、$a^4$而产生的。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;如何生成b1&#34;&gt;如何生成$b^1$？
&lt;/h3&gt;&lt;h4 id=&#34;a1与其他向量的相关性alpha的计算方法&#34;&gt;$a^1$与其他向量的相关性$\alpha$的计算方法
&lt;/h4&gt;&lt;h5 id=&#34;dot-product&#34;&gt;&lt;strong&gt;Dot-product&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;输入两个向量，分别与矩阵$W^q$和$W^k$相乘，再将得到的向量$q$和$k$做点乘。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/dot-product.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\begin{aligned}
q&amp;=a^1\times W^q \\
k&amp;=a^2\times W^k \\
\alpha &amp;= q\cdot k
\end{aligned}
$$
&lt;h5 id=&#34;additive&#34;&gt;Additive
&lt;/h5&gt;&lt;p&gt;输入两个向量，分别与矩阵$W^q$和$W^k$相乘，将得到的向量$q$和$k$串起来并通过激活函数，最后通过一个变换得到$alpha$。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/additive.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;b1的计算&#34;&gt;$b^1$的计算
&lt;/h4&gt;&lt;p&gt;本节中的例子中，对于$b^1$，计算步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step1.计算$q^1$&lt;/li&gt;
&lt;/ul&gt;
$$
q^1=W^qa^1
$$&lt;ul&gt;
&lt;li&gt;step2.计算$k^1$、$k^2$、$k^3$、$k^4$&lt;/li&gt;
&lt;/ul&gt;
$$
k^i=W^ka^i
$$&lt;ul&gt;
&lt;li&gt;step3.计算$\alpha_{1,1}$、$\alpha_{1,2}$、$\alpha_{1,3}$、$\alpha_{1,4}$&lt;/li&gt;
&lt;/ul&gt;
$$
\alpha_{1,i}=q^1\cdot k^i \\
$$&lt;ul&gt;
&lt;li&gt;step4.通过Soft-max（也可使用ReLU等）计算$\alpha_{1,1}&amp;rsquo;$、$\alpha_{1,2}&amp;rsquo;$、$\alpha_{1,3}&amp;rsquo;$、$\alpha_{1,4}&#39;$&lt;/li&gt;
&lt;/ul&gt;
$$
\alpha_{1,i}&#39;=\frac{\exp(\alpha_{1,i})}{\sum_j\exp(\alpha_{1,j})}
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-1.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  v^i=W^va^i
  $$&lt;/li&gt;
&lt;li&gt;
$$
  b^1=\sum_j\alpha_{1,j}&#39;v^j
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;输出向量组b1b2b3b4的完整计算过程&#34;&gt;输出向量组${b^1,b^2,b^3,b^4}$的完整计算过程
&lt;/h3&gt;&lt;p&gt;整理以上过程，$Q$、$K$、$V$和Attention分数的计算过程如下：&lt;/p&gt;
&lt;h4 id=&#34;qkv的计算&#34;&gt;$Q$、$K$、$V$的计算
&lt;/h4&gt;&lt;p&gt;由：&lt;/p&gt;
$$
\begin{aligned}
q^i &amp;=W^qa^i \\
k^i &amp;=W^ka^i \\
v^i &amp;=W^va^i 
\end{aligned}
$$$$
\begin{aligned}
\begin{bmatrix}
q^1 &amp; q^2 &amp; q^3 &amp; q^4
\end{bmatrix}
&amp;=W^q
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\\
\begin{bmatrix}
k^1 &amp; k^2 &amp; k^3 &amp; k^4
\end{bmatrix}
&amp;=W^k
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\\
\begin{bmatrix}
v^1 &amp; v^2 &amp; v^3 &amp; v^4
\end{bmatrix}
&amp;=W^v
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\end{aligned}
$$&lt;p&gt;即：&lt;/p&gt;
$$
\begin{aligned}
Q&amp;=W^qI \\
K&amp;=W^kI \\
V&amp;=W^vI
\end{aligned}
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-3.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;attention分数的计算&#34;&gt;Attention分数的计算
&lt;/h4&gt;&lt;p&gt;由：&lt;/p&gt;
$$
\alpha_{1,i}=k^i\cdot q^1
$$&lt;p&gt;有：&lt;/p&gt;
$$
\begin{bmatrix}
\alpha_{1,1} \\
\alpha_{1,2} \\
\alpha_{1,3} \\
\alpha_{1,4} \\
\end{bmatrix}=
\begin{bmatrix}
k^1 \\
k^2 \\
k^3 \\
k^4 
\end{bmatrix}
\cdot
q^1
$$&lt;p&gt;进一步有：&lt;/p&gt;
$$
\begin{bmatrix}
\alpha_{1,1} &amp; \alpha_{2,1} &amp; \alpha_{3,1} &amp;\alpha_{4,1} \\
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{3,2} &amp;\alpha_{4,2} \\
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp;\alpha_{4,3} \\
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp;\alpha_{4,4} \\
\end{bmatrix}=
\begin{bmatrix}
k^1 \\
k^2 \\
k^3 \\
k^4 
\end{bmatrix}
\cdot
\begin{bmatrix}
q^1 &amp; q^2 &amp; q^3 &amp;q^4
\end{bmatrix}
$$&lt;p&gt;即：&lt;/p&gt;
$$
A=K^TQ
$$&lt;p&gt;对$A$进行Soft-max得到$A&amp;rsquo;$：&lt;/p&gt;
$$
A&#39;=
\begin{bmatrix}
\alpha_{1,1}&#39; &amp; \alpha_{2,1}&#39; &amp; \alpha_{3,1}&#39; &amp;\alpha_{4,1}&#39; \\
\alpha_{1,2}&#39; &amp; \alpha_{2,2}&#39; &amp; \alpha_{3,2}&#39; &amp;\alpha_{4,2}&#39; \\
\alpha_{1,3}&#39; &amp; \alpha_{2,3}&#39; &amp; \alpha_{3,3}&#39; &amp;\alpha_{4,3}&#39; \\
\alpha_{1,4}&#39; &amp; \alpha_{2,4}&#39; &amp; \alpha_{3,4}&#39; &amp;\alpha_{4,4}&#39; \\
\end{bmatrix}
$$&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-4.png&#34; style=&#34;zoom:50%;&#34; /&gt;
$$
\begin{bmatrix}
b^1 &amp; b^2 &amp; b^3 &amp; b^4
\end{bmatrix}=
\begin{bmatrix}
v^1 &amp; v^2 &amp; v^3 &amp; v^4
\end{bmatrix}
\cdot
\begin{bmatrix}
\alpha_{1,1}&#39; &amp; \alpha_{2,1}&#39; &amp; \alpha_{3,1}&#39; &amp;\alpha_{4,1}&#39; \\
\alpha_{1,2}&#39; &amp; \alpha_{2,2}&#39; &amp; \alpha_{3,2}&#39; &amp;\alpha_{4,2}&#39; \\
\alpha_{1,3}&#39; &amp; \alpha_{2,3}&#39; &amp; \alpha_{3,3}&#39; &amp;\alpha_{4,3}&#39; \\
\alpha_{1,4}&#39; &amp; \alpha_{2,4}&#39; &amp; \alpha_{3,4}&#39; &amp;\alpha_{4,4}&#39; \\
\end{bmatrix}
$$$$
O=VA&#39;
$$&lt;h4 id=&#34;总结&#34;&gt;总结
&lt;/h4&gt;$$
\begin{aligned}
&amp;Q=W^qI \\
&amp;K=W^kI \\
&amp;V=W^vI \\
&amp;A=K^TQ \\
&amp;A \rightarrow A&#39; \\
&amp;O=VA&#39;
\end{aligned}
$$&lt;p&gt;$W^q$、$W^k$、$W^v$是需要学习的参数。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-5.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;multi-head-self-attention&#34;&gt;Multi-head Self-attention
&lt;/h2&gt;&lt;p&gt;多个$q$，对应不同种类的相关性。&lt;/p&gt;
$$
\begin{aligned}
&amp;q^{i,1}=W^{q,1}q^i \\
&amp;q^{i,2}=W^{q,2}q^i
\end{aligned}
$$&lt;p&gt;
类似的，$a^j$对应的$q^j$、$k^j$、$v^j$具体有$q^{j,1}$、$k^{j,1}$、$v^{j,1}$：&lt;/p&gt;
$$
b^i=W^O
\begin{bmatrix}
b^{i,1} \\
b^{i,2}
\end{bmatrix}
$$&lt;p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/multi-head%20self-attention.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;位置编码&#34;&gt;位置编码
&lt;/h2&gt;&lt;p&gt;Self-attention中，对输入的几个向量所进行的操作是相同的，与位置无关，可能丢失了位置信息。&lt;/p&gt;
$$
e^i+a^i\rightarrow q^i,k^i,v^i
$$&lt;p&gt;
向量$e^i$可以通过一个规则设定（人工）或从训练数据中学习出来。&lt;/p&gt;
&lt;p&gt;有各种不同的方法产生位置编码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sinusoidal&lt;/li&gt;
&lt;li&gt;Position embedding&lt;/li&gt;
&lt;li&gt;FLOATER&lt;/li&gt;
&lt;li&gt;RNN&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention用于语音&#34;&gt;Self-attention用于语音
&lt;/h2&gt;&lt;p&gt;把声音讯号表示为一排向量，一般一个向量只有10ms的长度，会导致向量个数过多，$A&amp;rsquo;$计算的复杂度是$O(L^2)$，一般使用&lt;strong&gt;Truncated Self-attention&lt;/strong&gt;，不看一整句话，只看一小部分。&lt;/p&gt;
&lt;h2 id=&#34;self-attention用于图像&#34;&gt;Self-attention用于图像
&lt;/h2&gt;&lt;p&gt;一张图片可以看成是一排向量。&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20for%20img.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;Self-Attention GAN&lt;/li&gt;
&lt;li&gt;Detection Transformer(DETR)&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20for%20img2.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;self-attention-vs-cnn&#34;&gt;Self-attention v.s. CNN
&lt;/h2&gt;&lt;p&gt;Self-attention可以看作复杂化的CNN，Self-attention考虑全局。CNN是Self-attention的特例，Self-attention可以通过设定合适的参数，达到和CNN同样的效果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1911.03584&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1911.03584] On the Relationship between Self-Attention and Convolutional Layers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention-vs-rnn&#34;&gt;Self-attention v.s. RNN
&lt;/h2&gt;&lt;p&gt;RNN（Recurrent Neural Network）：&lt;/p&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20vs%20RNN.png&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;RNN的缺点很明显，很难去考虑到输入位置较远的向量，并且无法并行计算。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.16236&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2006.16236] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention用于图&#34;&gt;Self-attention用于图
&lt;/h2&gt;&lt;p&gt;图中每个结点可以表示为一个向量，边可以用来考虑结点之间的关联性，计算Attention分数时，只需计算相连的结点。&lt;/p&gt;
&lt;p&gt;Self-attention用在图上面，是某一种类型的GNN（Graph Neural Network）。&lt;/p&gt;
&lt;h2 id=&#34;more&#34;&gt;More
&lt;/h2&gt;&lt;p&gt;Self-Attention的计算量较大，优化效率是一个研究方向。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2011.04006&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2011.04006] Long Range Arena: A Benchmark for Efficient Transformers (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2009.06732&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2009.06732] Efficient Transformers: A Survey (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Archives</title>
        <link>https://demo.stack.jimmycai.com/archives/</link>
        <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>数字图像处理</title>
        <link>https://demo.stack.jimmycai.com/p/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</link>
        <pubDate>Sat, 05 Feb 2022 11:23:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</guid>
        <description>&lt;p&gt;参考视频是广东海洋大学的课程录像，参考书目为冈萨雷斯等著，阮秋琦等译的《数字图像处理》（第四版）。&lt;/p&gt;
&lt;h1 id=&#34;图像的空域增强技术&#34;&gt;图像的空域增强技术
&lt;/h1&gt;&lt;h2 id=&#34;概述&#34;&gt;概述
&lt;/h2&gt;&lt;h3 id=&#34;空域的概念&#34;&gt;空域的概念
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;空域&lt;/strong&gt;：像素组成的空间。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;空域增强技术&lt;/strong&gt;：直接作用于图像像素的增强技术。&lt;/li&gt;
&lt;/ul&gt;
$$
像素的空间坐标(x,y) \rightarrow 像素的灰度值f(x,y)
$$&lt;h3 id=&#34;空域增强的模型&#34;&gt;空域增强的模型
&lt;/h3&gt;$$
g(x,y)=E_H[f(x,y)]
$$&lt;h3 id=&#34;分类&#34;&gt;分类
&lt;/h3&gt;&lt;h4 id=&#34;基于像素的空域增强&#34;&gt;基于像素的空域增强
&lt;/h4&gt;&lt;p&gt;$E_H$定义在每个&lt;strong&gt;像素&lt;/strong&gt;$(x,y)$上。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;像素点操作：$g(x,y)=P_{xy}[f(x,y)]$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;几何操作：$(x&amp;rsquo;,y&amp;rsquo;)=M(x,y)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;基于模板的空域增强&#34;&gt;基于模板的空域增强
&lt;/h4&gt;$$
t=E_H[s,n(s)]
$$&lt;h2 id=&#34;图像间运算&#34;&gt;图像间运算
&lt;/h2&gt;&lt;h3 id=&#34;算数运算&#34;&gt;算数运算
&lt;/h3&gt;&lt;p&gt;两幅图像对应位置像素$p、q$：$p+q、p-q、p\times q、p\div q$&lt;/p&gt;
&lt;h3 id=&#34;应用&#34;&gt;应用
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;图像去噪&lt;/p&gt;
$$
  \begin{aligned}
  g(x,y)&amp;=f(x,y)+e(x,y) \\
  \overline{g}(x,y)&amp;=\frac{1}{M} \sum_{i=1}^{M}g_i(x,y)
  \end{aligned}
  $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;医学图像的数字减影&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;图像局部显示&lt;/p&gt;
&lt;p&gt;二值模板图像与原图像做&lt;strong&gt;乘法&lt;/strong&gt;，进行图像的局部显示。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;直接灰度映射&#34;&gt;直接灰度映射
&lt;/h2&gt;&lt;h3 id=&#34;原理&#34;&gt;原理
&lt;/h3&gt;$$
灰度值\xrightarrow{f}另一灰度值
$$&lt;h3 id=&#34;典型灰度映射&#34;&gt;典型灰度映射
&lt;/h3&gt;&lt;h4 id=&#34;图像求反&#34;&gt;图像求反
&lt;/h4&gt;$$
t=(L-1)-s
$$&lt;h4 id=&#34;对比度增强&#34;&gt;对比度增强
&lt;/h4&gt;&lt;p&gt;基于像素的图像增强，即增强原图的各部分反差。&lt;/p&gt;
&lt;h4 id=&#34;分段线性增强&#34;&gt;分段线性增强
&lt;/h4&gt;&lt;p&gt;拉伸感兴趣的图像细节的灰度级。&lt;/p&gt;
&lt;p&gt;eg.对于$0\sim 255$的灰度取值范围，划分为$0\sim 100、101\sim 200、201\sim 255$三个取值区间，只改变其中某段进行灰度变换。&lt;/p&gt;
$$
\large
t =
\begin{cases} 
\frac{t_1}{s_1}s,  &amp; 0\le s \le s_1\\
\frac{t_2-t_1}{s_2-s_1}[s-s_1]+t_1, &amp;s_1 \lt s \le s_2 \\
\frac{L-1-t_2}{L-1-s_2}[s-s_2]+t_2, &amp;s2 \lt s \le L-1
\end{cases}
$$&lt;p&gt;经过&lt;strong&gt;斜率小于1&lt;/strong&gt;的线性变换函数后：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;压缩了灰度的动态范围&lt;/li&gt;
&lt;li&gt;对比度下降&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;经过&lt;strong&gt;斜率大于1&lt;/strong&gt;的线性变换函数后，反之。&lt;/p&gt;
&lt;h4 id=&#34;对数变换&#34;&gt;对数变换
&lt;/h4&gt;&lt;p&gt;原图动态范围太大，超出某些设备所允许的动态范围，需要压缩其动态范围，即$0 \sim L&amp;rsquo;(\gt L-1) \longrightarrow 0 \sim L-1$。&lt;/p&gt;
$$
t=C\log(1+|s|)
$$&lt;p&gt;
其中$C$为尺度比例常数。&lt;/p&gt;
&lt;p&gt;可以使用对数变换来扩展图像中暗像素的值，同时压缩高灰度级的值。&lt;/p&gt;
&lt;h4 id=&#34;幂律伽马变换&#34;&gt;幂律（伽马）变换
&lt;/h4&gt;$$
t=c\times s^\gamma
$$&lt;p&gt;其中$c$和$\gamma$为正常数。&lt;/p&gt;
&lt;p&gt;对于$\gamma (\gamma \lt 1)$的幂律曲线，将较窄范围的暗色输入值映射为较宽范围的亮色输入值（&lt;em&gt;与对数变换类似&lt;/em&gt;）；同时，将较宽范围的亮色输入值映射为较窄范围的输出值，$\gamma \gt$ 1的幂律变换与之效果完全相反。&lt;/p&gt;
&lt;p&gt;可以采用幂律变换提升图像细节的质量。&lt;/p&gt;
&lt;h4 id=&#34;灰度切割灰度级分层&#34;&gt;灰度切割（灰度级分层）
&lt;/h4&gt;&lt;p&gt;增强特定范围的对比度，用来突出图像中&lt;strong&gt;特定灰度范围&lt;/strong&gt;的亮度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  t =
  \begin{cases} 
  t_2,  &amp; s_1\le s \le s_2\\
  t_1,  &amp; \text{其他}
  \end{cases}
  $$&lt;/li&gt;
&lt;li&gt;
$$
  t =
  \begin{cases} 
  t_2,  &amp; s_1\le s \le s_2\\
  s,  &amp; \text{其他}
  \end{cases}
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;阈值化处理二值化处理灰度切割的特例&#34;&gt;阈值化处理（二值化处理，灰度切割的特例）
&lt;/h4&gt;$$
t =
\begin{cases} 
0,  &amp; s\lt s_1\\
L-1,  &amp; s\ge s_1
\end{cases}
$$&lt;p&gt;最终产生一个&lt;strong&gt;黑白图像&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;位图切割&#34;&gt;位图切割
&lt;/h4&gt;&lt;p&gt;$8$比特表示的图像看作$8$个单独的$1$比特平面（位图）组成，位面0表示最低位面，位面7表示最高位面。&lt;/p&gt;
&lt;p&gt;每个位面均为&lt;strong&gt;二值图像&lt;/strong&gt;，位面图像中像素的灰度值等于相应有效位的取值。&lt;/p&gt;
&lt;p&gt;可实现以下应用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;操作特定位面增强图像&lt;/li&gt;
&lt;li&gt;确定用于量化该图像的比特数的充分性&lt;/li&gt;
&lt;li&gt;图像压缩&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;实现方法&#34;&gt;实现方法
&lt;/h5&gt;&lt;p&gt;图像各像素的灰度值除以各有效位的权值$2^i$（$i$为有效位的序数，从$0$计数），商的整数部分为&lt;strong&gt;奇数&lt;/strong&gt;，则该灰度值在相应位面中映射为1，若为&lt;strong&gt;偶数&lt;/strong&gt;，则映射为$0$。（&lt;em&gt;可类比十进制转二进制的手算方法&lt;/em&gt;）&lt;/p&gt;
$$
\begin{aligned}
floor(121/2^7)=0 &amp;\quad\Rightarrow\quad 位面\ 7\ 中取值为\ 0 \\
floor(121/2^6)=1 &amp;\quad\Rightarrow\quad 位面\ 6\ 中取值为\ 1 \\
floor(121/2^5)=3 &amp;\quad\Rightarrow\quad 位面\ 5\ 中取值为\ 1 \\
floor(121/2^4)=7 &amp;\quad\Rightarrow\quad 位面\ 4\ 中取值为\ 1 \\
floor(121/2^3)=15 &amp;\quad\Rightarrow\quad 位面\ 3\ 中取值为\ 1 \\
floor(121/2^2)=30 &amp;\quad\Rightarrow\quad 位面\ 2\ 中取值为\ 0 \\
floor(121/2^1)=60 &amp;\quad\Rightarrow\quad 位面\ 1\ 中取值为\ 0 \\
floor(121/2^0)=121 &amp;\quad\Rightarrow\quad 位面\ 0\ 中取值为\ 1
\end{aligned}
$$&lt;h5 id=&#34;matlab实现代码&#34;&gt;MATLAB实现代码
&lt;/h5&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;fractal_iris.bmp&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rowcnt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;columncnt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;源图像&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rowcnt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columncnt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;mod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;floor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;double&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;^&lt;span class=&#34;nb&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;				&lt;span class=&#34;n&#34;&gt;bitmap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;				&lt;span class=&#34;n&#34;&gt;bitmap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;k&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;k&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bitmap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strcat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;位平面&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num2str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;直方图修正直方图均衡化&#34;&gt;直方图修正——直方图均衡化
&lt;/h2&gt;&lt;h3 id=&#34;直方图和累计直方图&#34;&gt;直方图和累计直方图
&lt;/h3&gt;&lt;h4 id=&#34;直方图&#34;&gt;直方图
&lt;/h4&gt;$$
\begin{aligned}
h(k)=n_k \quad k=0,1,\cdots,L-1
\end{aligned}
$$&lt;p&gt;其中，$n_k$是图像$f(x,y)$中具有灰度值$k$的像素的个数。&lt;/p&gt;
&lt;p&gt;是图像的一种统计表达，反映了图像中像素的灰度值的分布情况。&lt;/p&gt;
&lt;p&gt;若某图像的灰度直方图具有&lt;strong&gt;二峰性&lt;/strong&gt;，则表明这个图像较亮区域和较暗区域可以较好的分离。&lt;/p&gt;
&lt;h4 id=&#34;归一化直方图&#34;&gt;归一化直方图
&lt;/h4&gt;$$
\begin{aligned}
p(s_k)&amp;=\frac{n_k}{n} \quad s_k=\frac{k}{L-1},0\le s_k\le1
\end{aligned}
$$&lt;p&gt;其中，$n$为图像所有像素的数量。&lt;/p&gt;
&lt;h4 id=&#34;累计直方图&#34;&gt;累计直方图
&lt;/h4&gt;$$
H(k)=\sum_{i=0}^{k}n_i
$$&lt;p&gt;其中，$n_i$表示图像中灰度级等于$i$的像素点数量。&lt;/p&gt;
&lt;h4 id=&#34;归一化累计直方图&#34;&gt;归一化累计直方图
&lt;/h4&gt;$$
\begin{aligned}
P(s_k)=\sum_{i=0}^kp(s_i)
\end{aligned}
$$&lt;h3 id=&#34;直方图均衡化原理&#34;&gt;直方图均衡化原理
&lt;/h3&gt;&lt;p&gt;把图像的直方图变换为&lt;strong&gt;均匀分布&lt;/strong&gt;的形式，以此增强动态范围偏小的图像的反差，从而实现&lt;strong&gt;对比度增强&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;实质是选用合适的变换函数来&lt;strong&gt;修正图像灰度级的归一化直方图&lt;/strong&gt;$p(s_k)$，为了能从图像中获得尽量多的信息量（图像熵尽可能大），要求$p(s_k)$为&lt;strong&gt;常数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;增强函数$E_H(s)$需要满足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E_H(s)$为单值单增函数（&lt;em&gt;保持原有排列次序&lt;/em&gt;）&lt;/li&gt;
&lt;li&gt;$0\le E_H(s) \le L-1$（&lt;em&gt;灰度级动态范围一致&lt;/em&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;反变换也应该满足上述条件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;累积分布函数（CDF）&lt;strong&gt;满足以上条件：
$$
t_k=E_H(s_k)=\sum_{i=0}^{k}\frac{n_i}{n}=\sum_{i=0}^{k}p(s_i)
$$
例如一幅图像$64\times 64(n=4096)$，每个像素点用3比特表示（8个灰度级），像素点的灰度值分布如下：
$$
\begin{array}{c|cccccccc}
\text{灰度级}&amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7  \\
\hline 
\text{像素数量} &amp; 790 &amp; 1023 &amp; 850 &amp; 656 &amp; 329 &amp; 245 &amp; 122 &amp; 81
\end{array}
$$
实现&lt;/strong&gt;直方图均衡化&lt;/strong&gt;步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
$$
   \begin{array}{c|cccccccc}
   灰度级k &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7  \\
   \hline 
   归一化灰度级s_k &amp; \frac{0}{7} &amp; \frac{1}{7} &amp; \frac{2}{7} &amp; \frac{3}{7} &amp; \frac{4}{7} &amp; \frac{5}{7} &amp; \frac{6}{7} &amp; \frac{7}{7}  \\
   \hline 
   像素数量n_k &amp; 790 &amp; 1023 &amp; 850 &amp; 656 &amp; 329 &amp; 245 &amp; 122 &amp; 81 \\
   \hline
   \textbf{归一化直方图}p(s_k) &amp; 0.19 &amp; 0.25 &amp; 0.21 &amp; 0.16 &amp; 0.08 &amp; 0.06 &amp; 0.03 &amp; 0.02 
   \end{array}
   $$&lt;/li&gt;
&lt;li&gt;
$$
   \begin{array}{c|cccccccc}
   灰度级k &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7  \\
   \hline 
   归一化灰度级s_k &amp; \frac{0}{7} &amp; \frac{1}{7} &amp; \frac{2}{7} &amp; \frac{3}{7} &amp; \frac{4}{7} &amp; \frac{5}{7} &amp; \frac{6}{7} &amp; \frac{7}{7}  \\
   \hline 
   像素数量n_k &amp; 790 &amp; 1023 &amp; 850 &amp; 656 &amp; 329 &amp; 245 &amp; 122 &amp; 81 \\
   \hline
   归一化直方图p(s_k) &amp; 0.19 &amp; 0.25 &amp; 0.21 &amp; 0.16 &amp; 0.08 &amp; 0.06 &amp; 0.03 &amp; 0.02  \\
   \hline
   \textbf{归一化累积直方图}t_k &amp; 0.19 &amp; 0.44 &amp; 0.65 &amp; 0.81 &amp; 0.89 &amp; 0.95 &amp; 0.98 &amp; 1.00 
   \end{array}
   $$&lt;/li&gt;
&lt;li&gt;
$$
   \begin{array}{c|cccccccc}
   灰度级k &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7  \\
   \hline 
   归一化灰度级s_k &amp; \frac{0}{7} &amp; \frac{1}{7} &amp; \frac{2}{7} &amp; \frac{3}{7} &amp; \frac{4}{7} &amp; \frac{5}{7} &amp; \frac{6}{7} &amp; \frac{7}{7}  \\
   \hline 
   像素数量n_k &amp; 790 &amp; 1023 &amp; 850 &amp; 656 &amp; 329 &amp; 245 &amp; 122 &amp; 81 \\
   \hline
   归一化直方图p(s_k) &amp; 0.19 &amp; 0.25 &amp; 0.21 &amp; 0.16 &amp; 0.08 &amp; 0.06 &amp; 0.03 &amp; 0.02  \\
   \hline
   归一化累积直方图t_k &amp; 0.19 &amp; 0.44 &amp; 0.65 &amp; 0.81 &amp; 0.89 &amp; 0.95 &amp; 0.98 &amp; 1.00 \\
   \hline
   \textbf{扩展}t_k&#39; &amp; 1 &amp; 3 &amp; 5 &amp; 6 &amp; 6 &amp; 7 &amp; 7 &amp; 7
   \end{array}
   $$&lt;p&gt;
其中$t&amp;rsquo;_k$的选取：选择最靠近的一个灰度级的值，例如$0.19$离$\frac{1}{7}$最近，则修正的灰度级为$1$，以此类推。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;空间滤波机理&#34;&gt;空间滤波机理
&lt;/h2&gt;&lt;p&gt;组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个&lt;strong&gt;邻域&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;对领域内像素执行的&lt;strong&gt;预定义操作&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;滤波在邻域中心坐标产生一个新的像素，其值是滤波操作的结果。滤波器的中心访问图像中的每个像素后生成滤波后的图像。&lt;/p&gt;
&lt;p&gt;可根据执行的操作分为&lt;strong&gt;线性空间滤波器&lt;/strong&gt;和&lt;strong&gt;非线性空间滤波器&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;线性滤波&#34;&gt;线性滤波
&lt;/h2&gt;&lt;h3 id=&#34;技术分类和实现原理&#34;&gt;技术分类和实现原理
&lt;/h3&gt;&lt;h4 id=&#34;技术分类&#34;&gt;技术分类
&lt;/h4&gt;&lt;h5 id=&#34;平滑滤波&#34;&gt;平滑滤波
&lt;/h5&gt;&lt;p&gt;平滑线性空间滤波器使用滤波器模板确定的领域像素的平均灰度值代替邻域中心像素的值。降低了图像灰度的“尖锐”变化。&lt;/p&gt;
&lt;p&gt;应用：降低噪声、模糊处理&amp;hellip;&lt;/p&gt;
&lt;p&gt;影响：边缘模糊的负面效应&lt;/p&gt;
&lt;h5 id=&#34;锐化滤波&#34;&gt;锐化滤波
&lt;/h5&gt;&lt;p&gt;削弱图像中灰度缓慢变化的区域，同时使图像中灰度值发生突变的区域得到增强（或不变）。（即消除图像中的低频分量，同时增强（或不影响）高频分量。）&lt;/p&gt;
&lt;p&gt;效果：增强被模糊的细节或目标的边缘&lt;/p&gt;
&lt;p&gt;&lt;em&gt;与平滑滤波互逆。凸显细节，弱化背景。&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;实现原理模板卷积&#34;&gt;实现原理（模板卷积）
&lt;/h4&gt;$$
\begin{aligned}
g(x,y)=\sum_{s=-a}^{a}\sum_{t=-b}^bw(s,t)f(x+s,y+t)
\end{aligned}
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$w(s,t)$为滤波器系数，滤波器中心系数$w(0,0)$对准位置为$(x,y)$的像素；&lt;/li&gt;
&lt;li&gt;$m=2a+1$，$n=2b+1$，且$a$、$b$为正整数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;使用奇数尺寸的滤波器可更简化索引且更为直观，因为其滤波器的中心落在整数值上。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将滤波器在图像中漫游，并将滤波器中心与图像中某个像素位置重合；&lt;/li&gt;
&lt;li&gt;将滤波器中的各个系数与滤波器所覆盖的各对应像素的灰度值相乘；&lt;/li&gt;
&lt;li&gt;将2中的所有成绩结果进行相加，并将加法运算的结果赋给图像中对应滤波器中心位置的像素（滤波器的输出响应）。&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{bmatrix}
{ } &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; { } \\
{\cdots} &amp; {f(x-1,y-1)} &amp; {f(x-1,y)} &amp; {f(x-1,y+1)} &amp; {\cdots} \\
{\cdots} &amp; {f(x,y-1)} &amp; {f(x,y)} &amp; {f(x,y+1)} &amp; {\cdots}\\
{\cdots} &amp; {f(x+1,y-1)} &amp; {f(x+1,y)} &amp; {f(x+1,y+1)} &amp; {\cdots}\\
{ } &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; { }
\end{bmatrix}
$$$$
\begin{bmatrix}
{w(-1,-1)} &amp; {w(-1,0)} &amp; {w(-1,1)} \\
{w(0,-1)} &amp; {w(0,0)} &amp; {w(0,1)} \\
{w(1,-1)} &amp; {w(1,0)} &amp; {w(1,1)}\\
\end{bmatrix}
$$$$
\begin{aligned}
g(x,y) &amp;= w(-1,-1)f(x-1,y-1) &amp;&amp;+ w(-1,0)f(x-1,y) &amp;&amp;+ w(-1,1)f(x-1,y+1) \\
		&amp;+ w(0,-1)f(x,y-1) &amp;&amp;+ w(0,0)f(x,y) &amp;&amp;+ w(0,1)f(x,y+1) \\
		&amp;+ w(1,-1)f(x+1,y-1) &amp;&amp;+ w(1,0)f(x+1,y) &amp;&amp;+ w(1,1)f(x+1,y+1)
\end{aligned}
$$&lt;p&gt;&lt;em&gt;&lt;strong&gt;对于每一个滤波的结果，其参与运算的邻域灰度值均为原始图像对应邻域的灰度值，相当于在把结果存放在一个新的空白矩阵上，而非在原始图像上就地修改。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;线性平滑滤波器&#34;&gt;线性平滑滤波器
&lt;/h3&gt;&lt;h4 id=&#34;邻域平均&#34;&gt;邻域平均
&lt;/h4&gt;$$
g(x,y)=\frac{1}{mn}\sum_{(x,y)\in S}f(x,y)
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$S$为滤波器模板覆盖的像素邻域&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$mn$为邻域$S$中像素点数&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;算法简单，但会使图像产生模糊，且邻域越大，模糊越厉害。&lt;/p&gt;
&lt;h4 id=&#34;加权平均&#34;&gt;加权平均
&lt;/h4&gt;&lt;p&gt;滤波器模板中各个位置的系数采用不同的数值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;离模板中心近的像素权值大&lt;/li&gt;
&lt;li&gt;离模板中心远的像素权值小&lt;/li&gt;
&lt;li&gt;权值之和等于1&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
g(x,y)=\sum_{s=-a}^{a}\sum_{t=-b}^bw(s,t)f(x+s,y+t)
\end{aligned}
$$$$
H_1 = 
\frac{1}{9}
\begin{bmatrix}
{1} &amp; {1} &amp; {1} \\
{1} &amp; {1} &amp; {1} \\
{1} &amp; {1} &amp; {1} \\
\end{bmatrix}
\quad
H_2 = 
\frac{1}{10}
\begin{bmatrix}
{1} &amp; {1} &amp; {1} \\
{1} &amp; {2} &amp; {1} \\
{1} &amp; {1} &amp; {1} \\
\end{bmatrix}
\quad
H_3 = 
\frac{1}{16}
\begin{bmatrix}
{1} &amp; {2} &amp; {1} \\
{2} &amp; {4} &amp; {2} \\
{1} &amp; {2} &amp; {1} \\
\end{bmatrix}
$$&lt;h2 id=&#34;非线性滤波&#34;&gt;非线性滤波
&lt;/h2&gt;&lt;h3 id=&#34;非线性平滑滤波器&#34;&gt;非线性平滑滤波器
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;统计排序滤波器&lt;/strong&gt;的响应以滤波器所覆盖的图像区域中的所有像素的&lt;strong&gt;排序&lt;/strong&gt;为基础，然后使用&lt;strong&gt;统计排序的结果值&lt;/strong&gt;代替中心像素的值。其具备优秀的去噪能力，且比同尺寸的线性平滑滤波器的模糊程度明显要低。&lt;/p&gt;
&lt;h4 id=&#34;中值滤波器&#34;&gt;中值滤波器
&lt;/h4&gt;&lt;p&gt;使用像素邻域内灰度的&lt;strong&gt;中值&lt;/strong&gt;代替中心像素的值。其主要功能是拥有不同灰度的像素点看起来更接近于它的相邻点（去除孤立像素）。&lt;/p&gt;
&lt;p&gt;中值滤波器对处理&lt;strong&gt;椒盐噪声&lt;/strong&gt;（&lt;em&gt;椒噪声：灰度值较低，偏暗；盐噪声：灰度值较高，偏亮。极端情况是黑色和白色噪声&lt;/em&gt;）非常有效，因为这种噪声以黑白点的形式叠加在图像上。&lt;/p&gt;
&lt;h5 id=&#34;实现步骤&#34;&gt;实现步骤
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;将滤波器模板（无系数）在图像中漫游，并将模板中心与图像中某个像素位置重合；&lt;/li&gt;
&lt;li&gt;读取模板下各对应像素的灰度值；&lt;/li&gt;
&lt;li&gt;将灰度值按从小到大（或从大到小）的次序进行排序；&lt;/li&gt;
&lt;li&gt;确定排序结果的中值，将此中值赋予对应模板中心位置的像素。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\begin{bmatrix}
{ } &amp;amp; {\vdots} &amp;amp; {\vdots} &amp;amp; {\vdots} &amp;amp; { } \
{\cdots} &amp;amp; {10} &amp;amp; {20} &amp;amp; {20} &amp;amp; {\cdots} \
{\cdots} &amp;amp; {20} &amp;amp; {15} &amp;amp; {20} &amp;amp; {\cdots}\
{\cdots} &amp;amp; {20} &amp;amp; {25} &amp;amp; {100} &amp;amp; {\cdots}\
{ } &amp;amp; {\vdots} &amp;amp; {\vdots} &amp;amp; {\vdots} &amp;amp; { }
\end{bmatrix}&lt;/p&gt;
&lt;p&gt;\Rightarrow&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
{10} &amp;amp; {15} &amp;amp; {20} &amp;amp; {20} &amp;amp; \textbf{20} &amp;amp; {20} &amp;amp; {20} &amp;amp; {25} &amp;amp; {100}
\end{bmatrix}
$$&lt;/p&gt;
&lt;h5 id=&#34;模板选择&#34;&gt;模板选择
&lt;/h5&gt;&lt;p&gt;去噪效果与以下两个因素有关：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模板形状&lt;/li&gt;
&lt;li&gt;参与运算的像素数量&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{bmatrix}
{\cdot} &amp; {\cdot} &amp; {\cdot} &amp; {\cdot} &amp; {\cdot} \\
{\cdot} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\cdot} \\
{\cdot} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\cdot} \\
{\cdot} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\cdot} \\
{\cdot} &amp; {\cdot} &amp; {\cdot} &amp; {\cdot} &amp; {\cdot} 
\end{bmatrix}
\quad
\begin{bmatrix}
{\cdot} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\cdot} \\
{\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet}  \\
{\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} \\
{\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} \\
{\cdot} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\cdot} 
\end{bmatrix}
\quad
\begin{bmatrix}
{\cdot} &amp; {\cdot} &amp; {\bullet} &amp; {\cdot} &amp; {\cdot} \\
{\cdot} &amp; {\cdot} &amp; {\bullet} &amp; {\cdot} &amp; {\cdot} \\
{\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet} &amp; {\bullet}\\
{\cdot} &amp; {\cdot} &amp; {\bullet} &amp; {\cdot} &amp; {\cdot} \\
{\cdot} &amp; {\cdot} &amp; {\bullet} &amp; {\cdot} &amp; {\cdot} 
\end{bmatrix}
$$&lt;ul&gt;
&lt;li&gt;对于有缓变的较长轮廓线的图像，采用方形或圆形模板为宜；&lt;/li&gt;
&lt;li&gt;对于包含有尖顶角物体的图像，采用十字形模板为宜，且模板大小则以不超过图像中最小有效物体的尺寸为宜；&lt;/li&gt;
&lt;li&gt;对于包含点、线、尖细节较多的图像，则不适宜采用中值滤波。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;百分比滤波器&#34;&gt;百分比滤波器
&lt;/h4&gt;$$
g_{max}(x,y)=\mathop{max}\limits_{(s,t)\in N(x,y)}[f(s,t)]
$$$$
g_{min}(x,y)=\mathop{min}\limits_{(s,t)\in N(x,y)}[f(s,t)]
$$&lt;p&gt;
椒噪声有较低的灰度值，用最大值滤波器有较好的效果，而盐噪声反之。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;最大值滤波会细化黑色目标，最小值滤波会粗化黑色目标。&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;中点滤波器&#34;&gt;中点滤波器
&lt;/h4&gt;$$
g_{mid}(x,y)=\frac{1}{2}[g_{max}(x,y)+g_{min}(x,y)]
$$&lt;p&gt;结合了排序统计和求平均，对于高斯和均匀分布随机噪声有较好效果。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;中点滤波器得到的结果图像会产生模糊。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;非线性锐化滤波器&#34;&gt;非线性锐化滤波器
&lt;/h3&gt;&lt;p&gt;锐化处理目的是突出图像中灰度的&lt;strong&gt;过渡&lt;/strong&gt;部分。&lt;/p&gt;
&lt;p&gt;锐化处理可以用&lt;strong&gt;空间微分&lt;/strong&gt;来完成（微分算子的响应强度与像素的突变程度成正比）。即图像微分&lt;strong&gt;增强&lt;/strong&gt;边缘与其他突变（噪声、线），并&lt;strong&gt;削弱&lt;/strong&gt;灰度变化缓慢的区域。&lt;/p&gt;
&lt;p&gt;常用滤波器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于一阶微分的锐化滤波器&lt;/li&gt;
&lt;li&gt;基于二阶微分的锐化滤波器&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;数字图像微分&#34;&gt;数字图像微分
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;一阶微分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在&lt;strong&gt;恒定灰度区域&lt;/strong&gt;的一阶微分值&lt;strong&gt;为零&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;灰度台阶、灰度斜坡的起点处&lt;/strong&gt;一阶微分值&lt;strong&gt;非零&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;沿着灰度斜坡&lt;/strong&gt;的一阶微分值&lt;strong&gt;非零&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;二阶微分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在&lt;strong&gt;恒定灰度区域&lt;/strong&gt;的二阶微分值&lt;strong&gt;为零&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;灰度台阶、灰度斜坡的起点处&lt;/strong&gt;二阶微分值&lt;strong&gt;非零&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;沿着灰度斜坡&lt;/strong&gt;的二阶微分值&lt;strong&gt;为零&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于一维离散函数$f(x)$，采用差分计算其微分如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  \frac{\partial f}{\partial x}=f(x+1)-f(x)
  $$&lt;/li&gt;
&lt;li&gt;
$$
  \begin{aligned}
  \frac{\partial^2 f}{\partial x^2}&amp;=[f(x+1)-f(x)]-[f(x)-f(x-1)] \\
  &amp;= f(x+1)+f(x-1)-2f(x)
  \end{aligned}
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于二维的数字图$f(x,y)$，可以沿着两个空间轴处理偏微分。&lt;/p&gt;
$$
\begin{bmatrix}
6 &amp; 6 &amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 1 &amp; 1 &amp; 6 &amp; 6 &amp; 6 \\
恒定灰度 &amp; &amp; 斜坡起点 &amp;  &amp;  &amp; 斜坡 &amp;  &amp;  &amp;  &amp; 台 &amp; 阶 &amp;  &amp;  \\
0 &amp; 0 &amp; -1 &amp; -1 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 5 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -5 &amp; 0 &amp; 0
\end{bmatrix}
$$&lt;h4 id=&#34;基于一阶微分的锐化滤波器梯度算子&#34;&gt;基于一阶微分的锐化滤波器——梯度算子
&lt;/h4&gt;&lt;p&gt;基于一阶微分的锐化滤波常用&lt;strong&gt;梯度幅值&lt;/strong&gt;来实现。&lt;/p&gt;
&lt;h1 id=&#34;endbmatrixt&#34;&gt;对于图像$f$，在任意坐标$(x.y)$上的&lt;strong&gt;梯度&lt;/strong&gt;$\nabla f$定义为&lt;strong&gt;二维列向量&lt;/strong&gt;：
$$
\nabla f=
\begin{bmatrix}
Gx &amp;amp; Gy
\end{bmatrix}^T
&lt;/h1&gt;$$
梯度的**幅值**$|\nabla f|$：
$$$$
实际应用中，一般把梯度的幅值称为梯度，并采用绝对值近似求梯度幅值：
$$&lt;p&gt;
|\nabla f|=|G_x|+|G_y|=|\frac{\partial f}{\partial x}| + |\frac{\partial f}{\partial y}|
$$&lt;/p&gt;
&lt;h5 id=&#34;梯度一阶微分的近似计算方法滤波模板&#34;&gt;梯度（一阶微分）的近似计算方法（滤波模板）:
&lt;/h5&gt;&lt;h6 id=&#34;直接差分&#34;&gt;直接差分
&lt;/h6&gt;$$
   \begin{aligned}
   G_x=f(x+1,y)-f(x,y) \\
   G_y=f(x,y+1)-f(x,y)
   \end{aligned}
$$$$
   垂直方向
   \begin{bmatrix}
   \underline{-1} &amp; 0 \\
   1 &amp; 0 
   \end{bmatrix}
   \quad
   水平方向
   \begin{bmatrix}
   \underline{-1}&amp; 1 \\
   0 &amp; 0 
   \end{bmatrix}
$$&lt;h6 id=&#34;交叉差分&#34;&gt;交叉差分
&lt;/h6&gt;$$
   \begin{aligned}
   G_x=f(x+1,y+1)-f(x,y) \\
   G_y=f(x+1,y)-f(x,y+1)
   \end{aligned}
$$$$
   垂直方向
   \begin{bmatrix}
   \underline{-1} &amp; 0 \\
   0 &amp; 1 
   \end{bmatrix}
   \quad
   水平方向
   \begin{bmatrix}
   \underline{0} &amp; -1 \\
   1 &amp; 0 
   \end{bmatrix}
$$&lt;h6 id=&#34;sobel算子&#34;&gt;Sobel算子
&lt;/h6&gt;$$
   \begin{aligned}
   G_x = &amp;f(x+1,y-1)+2f(x+1,y)+f(x+1,y+1)\\
   &amp;-f(x-1,y-1)-2f(x-1,y)-f(x-1,y+1) \\
   G_y = &amp;f(x-1,y+1)+2f(x,y+1)+f(x+1,y+1)\\
   &amp;- f(x-1,y-1)-2f(x,y-1)-f(x+1,y-1)
   \end{aligned}
$$$$
   垂直方向
   \begin{bmatrix}
   -1 &amp; -2 &amp; -1 \\
   0 &amp; \underline{0} &amp; 0 \\
   1 &amp; 2 &amp; 1 
   \end{bmatrix}
   \quad
   水平方向
   \begin{bmatrix}
   -1 &amp; 0 &amp; 1 \\
   -2 &amp; \underline{0} &amp; 2 \\
   -1 &amp; 0 &amp; 1 
   \end{bmatrix}
$$&lt;p&gt;&lt;em&gt;下划线标出元素为滤波器模板的原点。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;可以看出，实现&lt;strong&gt;平滑&lt;/strong&gt;的滤波器系数之和为&lt;strong&gt;1&lt;/strong&gt;，实现&lt;strong&gt;锐化&lt;/strong&gt;的滤波器系数之和为&lt;strong&gt;0&lt;/strong&gt;。&lt;/em&gt;&lt;/p&gt;
&lt;h5 id=&#34;应用-1&#34;&gt;应用
&lt;/h5&gt;&lt;p&gt;工业检测、辅助人工检测缺陷，或更为通用的自动检测的预处理。&lt;/p&gt;
&lt;h4 id=&#34;基于二阶微分的锐化滤波器拉普拉斯算子&#34;&gt;基于二阶微分的锐化滤波器——拉普拉斯算子
&lt;/h4&gt;$$
\nabla ^2f=\frac{\partial ^2f}{\partial x^2}+\frac{\partial ^2f}{\partial y^2}
$$$$
\begin{aligned}
\frac{\partial ^2f}{\partial x^2}=f(x+1,y)+f(x-1,y)-2f(x,y)\\
\frac{\partial ^2f}{\partial y^2}=f(x,y+1)+f(x,y-1)-2f(x,y)
\end{aligned}
$$$$
\nabla ^2f=f(x-1,y)-2f(x,y)+f(x,y+1)+f(x,y-1)]-4f(x,y)
$$$$
(a)
\begin{bmatrix}
0 &amp; 1 &amp; 0 \\
1 &amp; -4 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}
$$$$
(b)
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; -8 &amp; 1 \\
1 &amp; 1 &amp; 1
\end{bmatrix}
\quad
(c)
\begin{bmatrix}
0 &amp; -1 &amp; 0 \\
-1 &amp; 4 &amp; -1 \\
0 &amp; -1 &amp; 0
\end{bmatrix}
\quad
(d)
\begin{bmatrix}
-1 &amp; -1 &amp; -1 \\
-1 &amp; 8 &amp; -1 \\
-1 &amp; -1 &amp; -1
\end{bmatrix}
$$&lt;p&gt;$(b)$为执行离散拉普拉斯变换的扩展模板，包括了对角方向的的领域像素;$(c)、(d)$为其他两种拉普拉斯变换的实现，仅符号相反，结果等效。&lt;/p&gt;
$$
g(x,y)=
\begin{cases}
f(x,y)-\nabla ^2f,若拉普拉斯模板中心系数为负 \\
f(x,y)+\nabla ^2f,若拉普拉斯模板中心系数为正
\end{cases}
$$&lt;p&gt;
将原始图像和拉普拉斯图像&lt;strong&gt;叠加&lt;/strong&gt;在一起，以增强细节。&lt;/p&gt;
&lt;h4 id=&#34;混合空间增强法&#34;&gt;混合空间增强法
&lt;/h4&gt;&lt;p&gt;若原始图像的灰度动态范围很窄并且伴随着很高的噪声，则采用单一的图像增强算法很难对其进行增强。&lt;/p&gt;
&lt;h1 id=&#34;傅里叶变换&#34;&gt;傅里叶变换
&lt;/h1&gt;&lt;h2 id=&#34;傅里叶变换及其反变换&#34;&gt;傅里叶变换及其反变换
&lt;/h2&gt;&lt;p&gt;空域$\stackrel{正变换}{\longrightarrow}$其他空间$\stackrel{反变换/逆变换}{\longrightarrow}$空域。&lt;/p&gt;
&lt;h3 id=&#34;一维连续傅里叶变换及反变换&#34;&gt;一维连续傅里叶变换及反变换
&lt;/h3&gt;$$
\begin{aligned}
F(\mu)=\int_{-\infty}^{\infty}f(t)e^{-j2\pi \mu t}dt \\
f(t)=\int_{-\infty}^{\infty}F(\mu)e^{j2\pi \mu t}d\mu
\end{aligned}
$$&lt;p&gt;其中，$j=\sqrt{-1}$&lt;/p&gt;
&lt;h3 id=&#34;二维连续傅里叶变换及反变换&#34;&gt;二维连续傅里叶变换及反变换
&lt;/h3&gt;$$
\begin{aligned}
F(\mu,v)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(t,z)e^{-j2\pi(\mu t+vz)}dtdz \\
f(t,z)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}F(\mu,v)e^{j2\pi(\mu t+vz)}d\mu dv
\end{aligned}
$$&lt;h3 id=&#34;一维dft及idft&#34;&gt;一维DFT及IDFT
&lt;/h3&gt;$$
\begin{aligned}
F_m=\frac{1}{M}\sum_{n=0}^{M-1}f_ne^{-j2\pi mn/M},\quad m=0,1,2,\cdots,M-1 \\
f_n=\sum_{m=0}^{M-1}F_me^{j2\pi mn/M},\quad n=0,1,2,\cdots,M-1
\end{aligned}
$$$$
\begin{aligned}
F(\mu)=\frac{1}{M}\sum_{x=0}^{M-1}f(x)e^{-j2\pi ux/M},\quad u=0,1,2,\cdots,M-1\\
f(x)=\sum_{u=0}^{M-1}F(u)e^{j2\pi ux/M},\quad x=0,1,2,\cdots,M-1
\end{aligned}
$$$$
e^{j\theta}=\cos\theta+j\sin\theta
$$$$
\begin{aligned}
F(u)&amp;=\frac{1}{M}\sum_{x=0}^{M-1}f(x)e^{-j2\pi ux/M} \\
&amp;=\frac{1}{M}\sum_{x=0}^{M-1}f(x)(\cos\frac{2\pi ux}{M}-j\sin\frac{2\pi ux}{M})
\end{aligned}
$$&lt;h4 id=&#34;傅里叶变换fu的极坐标表示&#34;&gt;傅里叶变换$F(u)$的极坐标表示
&lt;/h4&gt;$$
F(u)=|F(u)|e^{-j\varphi(u)}
$$&lt;p&gt;其中，&lt;/p&gt;
$$
\varphi(u)=\arctan[\frac{I(u)}{R(u)}]
$$&lt;p&gt;
$R(u)$和$I(u)$分别是$F(u)$的实部和虚部。&lt;/p&gt;
$$
|F(u)|=\sqrt{R(u)^2+I(u)^2}
$$$$
P(u)=|F(u)|^2=R(u)^2+I(u)^2
$$&lt;h3 id=&#34;二维dft及idft&#34;&gt;二维DFT及IDFT
&lt;/h3&gt;$$
F(u,v)=\frac{1}{MN}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)e^{-j2\pi(ux/M+vy/N)}
$$$$
u=0,1,2,\cdots,M-1 \\
v=0,1,2,\cdots,N-1
$$$$
f(x,y)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}F(u,v)e^{-j2\pi(ux/M+vy/N)}
$$$$
x=0,1,2,\cdots,M-1 \\
y=0,1,2,\cdots,N-1
$$&lt;p&gt;&lt;em&gt;在有些文献中，常数$1/MN$通常出现在DFT而非IDFT的前面。这时，这个常数的平方根应包含在正变换和反变换的前面，以便形成一个更为对称的变换对。只要使用一致，这种形式的任何表述就都是正确的。&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;二维dft的极坐标表示&#34;&gt;二维DFT的极坐标表示
&lt;/h4&gt;$$
F(u,v)=|F(u,v)|e^{-j\varphi(u,v)}
$$&lt;p&gt;其中，&lt;/p&gt;
$$
\varphi(u,v)=\arctan[\frac{I(u,v)}{R(u,v)}]
$$&lt;p&gt;
$R(u,v)$和$I(u,v)$分别是$F(u,v)$的实部和虚部。&lt;/p&gt;
$$
|F(u,v)|=\sqrt{R(u,v)^2+I(u,v)^2}
$$$$
P(u,v)=|F(u,v)|^2=R(u,v)^2+I(u,v)^2
$$&lt;h3 id=&#34;关于频谱fuv&#34;&gt;关于频谱$|F(u,v)|$
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;频谱描述图像中某种频率的成分数量；&lt;/li&gt;
&lt;li&gt;频谱中出现的明亮线反映了原始图像的灰度级变化方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;傅里叶变换的性质&#34;&gt;傅里叶变换的性质
&lt;/h2&gt;&lt;h3 id=&#34;平移&#34;&gt;平移
&lt;/h3&gt;&lt;h3 id=&#34;可分离性&#34;&gt;可分离性
&lt;/h3&gt;$$
\begin{aligned}
F(u,v)&amp;=\frac{1}{MN}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)e^{-j2\pi (ux/M+vy/N)} \\
&amp;= \frac{1}{M}\sum_{x=0}^{M-1}e^{-j2\pi ux/M}\frac{1}{N}\sum_{y=0}^{N-1}f(x,y)e^{-j2\pi vy/N} \\
&amp;= \frac{1}{M}\sum_{x=0}^{M-1}e^{-j2\pi ux/M}F(x,v)
\end{aligned}
$$$$
f(x,y) \stackrel{一维行变换}{\longrightarrow} F(x,v) \stackrel{一维列变换}{\longrightarrow} F(u,v)
$$&lt;p&gt;
二维IDFT与上述过程类似。&lt;/p&gt;
&lt;h3 id=&#34;平均值&#34;&gt;平均值
&lt;/h3&gt;$$
F(0,0)=\frac{1}{MN}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)
$$&lt;h2 id=&#34;快速傅里叶变换fft&#34;&gt;快速傅里叶变换（FFT）
&lt;/h2&gt;&lt;h1 id=&#34;频率域图像增强&#34;&gt;频率域图像增强
&lt;/h1&gt;&lt;h2 id=&#34;频率域滤波基础&#34;&gt;频率域滤波基础
&lt;/h2&gt;&lt;p&gt;在&lt;strong&gt;频率域&lt;/strong&gt;研究图像增强：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以利用频率成分和图像外表之间的对应关系；（一些在空间域表述困难的增强任务，在频率域中变得非常普通。）&lt;/li&gt;
&lt;li&gt;滤波在频率域更为直观，它可以解释空间域滤波的某些性质；（利用这些性质进行处理，再转换回图像空间，可以得到所需的效果。）&lt;/li&gt;
&lt;li&gt;空间域和频率域中的滤波器组成了傅里叶变换对。（可以在频率域指定滤波器，并对其执行反变换，最后在空间域使用该反变换的结果作为空域滤波器。）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;傅里叶变换的频率分量与图像空间特征&#34;&gt;傅里叶变换的频率分量与图像空间特征
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
$$
  F(0,0)=\frac{1}{MN}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)
  $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从变换的原点移开时，低频成分对应着图像中灰度慢变化的分量（图像的平滑部分）；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进一步偏离原点时，较高的频率成分对应图像中变化越来越快的灰度（边缘或噪声等尖锐部分）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;频率域滤波的基本步骤&#34;&gt;频率域滤波的基本步骤
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;用$(-1)^{x+y}$乘输入图像$f(x,y)$，使其&lt;strong&gt;原点中心化&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;对步骤1的结果执行&lt;strong&gt;DFT&lt;/strong&gt;，得到关于中心对称的频谱$F(u,v)$；&lt;/li&gt;
&lt;li&gt;生成一个&lt;strong&gt;实的、中心对称的频域滤波器&lt;/strong&gt;$H(u,v)$；&lt;/li&gt;
&lt;li&gt;对滤波器$H(u,v)$、频谱$F(u,v)$执行&lt;strong&gt;阵列相乘&lt;/strong&gt;（对应元素逐个进行相乘),形成乘积$G(u,v)=H(u,v)F(u,v)$，其中$G(m,n)=H(m,n)F(m,n)$，且$0\le m \le M-1,0\le n \le N-1$；&lt;/li&gt;
&lt;li&gt;对步骤4的结果$G(u,v)$执行&lt;strong&gt;反DFT&lt;/strong&gt;，并取其结果的&lt;strong&gt;实部&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;用$(-1)^{x+y}$乘步骤5的反DFT结果的实部，得到&lt;strong&gt;滤波结果&lt;/strong&gt;$g(x,y)$。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;频域滤波器如何作用于图像&#34;&gt;频域滤波器如何作用于图像
&lt;/h3&gt;&lt;h4 id=&#34;低通滤波器&#34;&gt;低通滤波器
&lt;/h4&gt;&lt;p&gt;使频谱的&lt;strong&gt;低频成分通过&lt;/strong&gt;，同时使其&lt;strong&gt;高频成分衰减&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;被低通滤波的图像比原始图像减少了尖锐的细节部分，突出了平滑过渡部分；&lt;/li&gt;
&lt;li&gt;对应于空间域滤波的平滑处理，如均值滤波器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;高通滤波器&#34;&gt;高通滤波器
&lt;/h4&gt;&lt;p&gt;使频谱的&lt;strong&gt;高频成分通过&lt;/strong&gt;，同时使其&lt;strong&gt;低频成分衰减&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;被高通滤波的图像比原始图像少了灰度级的平滑过渡，突出了边缘等细节部分；&lt;/li&gt;
&lt;li&gt;对应于空间域滤波的锐化处理，如梯度算子、拉普拉斯算子。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;频率域低通平滑滤波器&#34;&gt;频率域低通（平滑）滤波器
&lt;/h2&gt;&lt;p&gt;低通滤波器的作用：用于截断频谱中所有处于指定距离$D_0$之外的高频成分。&lt;/p&gt;
&lt;h3 id=&#34;理想低通滤波器ilpf&#34;&gt;理想低通滤波器（ILPF）
&lt;/h3&gt;$$
D(u,v)=\sqrt{(u-\frac{M}{2})^2+(v-\frac{N}{2})^2}
$$$$
H(u,v)=
\begin{cases}
1\quad D(u,v)\le D_0 \\
0\quad D(u,v)\gt D_0 \\
\end{cases}
$$&lt;p&gt;
&lt;em&gt;在半径为$D_0$的圆内，所有频率没有衰减地完全通过滤波器，而在此半径的圆之外的所有频率&lt;/em&gt;完全被衰减掉。&lt;/p&gt;
$$
P_T=\sum_{u=0}^{M-1}\sum_{v=0}^{N-1}P(u,v)
$$$$
P(u,v)=|F(u,v)|^2=R(u,v)^2+I(u,v)^2
$$&lt;p&gt;
原点位于频谱中心处，半径为$D_0$的圆包含$\alpha%$的总功率，&lt;/p&gt;
$$
\alpha=100[\sum_u\sum_vP(u,v)/P_T]
$$&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;随着滤波器&lt;strong&gt;半径的增大&lt;/strong&gt;，&lt;strong&gt;滤除的功率&lt;/strong&gt;越来越&lt;strong&gt;少&lt;/strong&gt;，导致的&lt;strong&gt;模糊&lt;/strong&gt;也越来越&lt;strong&gt;弱&lt;/strong&gt;。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;理想低通滤波器产生&lt;strong&gt;模糊&lt;/strong&gt;和&lt;strong&gt;振铃&lt;/strong&gt;现象，且模糊和振铃现象&lt;strong&gt;反比于截断频率&lt;/strong&gt;（即半径$D_0$）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;巴特沃斯低通滤波器blpf&#34;&gt;巴特沃斯低通滤波器（BLPF）
&lt;/h3&gt;$$
H(u,v)=\frac{1}{1+{[D(u,v)/D_0]}^{2n}}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;低阶&lt;/strong&gt;滤波器没有明显振铃现象（滤波器在低频和高频之间平滑过渡）。&lt;em&gt;且1阶BLPF核即没有振铃效应又没有负值。&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;高斯低通滤波器glpf&#34;&gt;高斯低通滤波器（GLPF）
&lt;/h3&gt;$$
H(u,v)=e^{-D(u,v)^2/2\sigma ^2}
$$&lt;p&gt;
$\sigma$是关于频谱中心的扩展度的度量。&lt;/p&gt;
$$
H(u,v)=e^{-D(u,v)^2/2D_0 ^2}
$$&lt;ul&gt;
&lt;li&gt;平滑效果稍差于相同截止频率的二阶BLPF；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;没有出现振铃现象&lt;/strong&gt;，优于BLPF。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;应用实例&#34;&gt;应用实例
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;用于机器识别系统识别字符的预处理；&lt;/li&gt;
&lt;li&gt;减少人脸图像的皮肤细纹核小斑点；&lt;/li&gt;
&lt;li&gt;消除卫星、航空图像中的不重要特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;频率域高通锐化滤波器&#34;&gt;频率域高通（锐化）滤波器
&lt;/h2&gt;&lt;p&gt;高通滤波器的作用：用于截断频谱中所有处于指定距离$D_0$之内的低频成分。&lt;/p&gt;
&lt;h3 id=&#34;理想高通滤波器ihpf&#34;&gt;理想高通滤波器（IHPF）
&lt;/h3&gt;$$
H(u,v)=
\begin{cases}
0\quad D(u,v)\le D_0 \\
1\quad D(u,v)\gt D_0
\end{cases}
$$&lt;ul&gt;
&lt;li&gt;振铃现象明显。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;巴特沃斯高通滤波器bhpf&#34;&gt;巴特沃斯高通滤波器（BHPF）
&lt;/h3&gt;$$
H(u,v)=\frac{1}{1+{[D_0/D(u,v)]}^{2n}}
$$&lt;ul&gt;
&lt;li&gt;BHPF的结果比IHPF的结果尖锐得多，边缘失真也小得多。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;高斯高通滤波器ghpf&#34;&gt;高斯高通滤波器（GHPF）
&lt;/h3&gt;$$
H(u,v)=1-e^{-D(u,v)^2/2D_0^2}
$$&lt;ul&gt;
&lt;li&gt;GHPF的结果比BHPF和IHPF的结果更尖锐，即使是对微小物体和细线条的滤波也是较清晰的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;高通滤波器与低通滤波器的关系&#34;&gt;高通滤波器与低通滤波器的关系
&lt;/h3&gt;$$
H_{HP}(u,v)=1-H_{LP}(u,v)
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;$H_{LP}(u,v)$：低通滤波器函数；$H_{HP}(u,v)$：高通滤波器函数。&lt;/p&gt;
&lt;p&gt;被低通滤波器衰减的频率成分能通过高通滤波器，反之亦然。&lt;/p&gt;
&lt;h3 id=&#34;高频提升和高频加强&#34;&gt;高频提升和高频加强
&lt;/h3&gt;&lt;p&gt;高通滤波效果等同于用原始图像的频谱减去低通滤波的结果图像频谱。&lt;/p&gt;
&lt;p&gt;图像经过高通滤波后，由于高通滤波器除去了傅里叶变换的零频率，其背景的平均强度减小到接近黑色。&lt;/p&gt;
&lt;p&gt;原始图像加到滤波后的结果图像，即&lt;strong&gt;高频提升滤波&lt;/strong&gt;或&lt;strong&gt;高频加强滤波&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;高频提升滤波&#34;&gt;高频提升滤波
&lt;/h4&gt;&lt;p&gt;将原始图像按一定比例加到滤波后的结果中，以保留原始图像的背景。&lt;/p&gt;
$$
f_{HB}(x,y)=A\times f(x,y)-f_{LP}(x,y),\quad A\ge1
$$$$
\begin{aligned}
f_{HB}(x,y)&amp;=(A-1)\times f(x,y)+f(x,y)-f_{LP}(x,y) \\
&amp;=(A-1)\times f(x,y)+f_{HP}(x,y)
\end{aligned}
$$$$
\begin{gather*}
f_{HB}(x,y)=(A-1)\times f(x,y)+f_{HP}(x,y) \\
\downarrow \\
F_{HB}(u,v)=(A-1)\times F(u,v)+F_{HP}(u,v) \\
\downarrow \\
F_{HB}(u,v)=(A-1)\times F(u,v)+H_{HP}(u,v)\times F(u,v) \\
\downarrow \\
F_{HB}(u,v)=[(A-1)+H_{HP}(u,v)]\times F(u,v)
\end{gather*}
$$$$
H_{HB}(u,v)=(A-1)+H_{HP}(u,v),\quad A\ge1,A=1时普通高通
$$$$
F_{HB}(u,v)=H_{HB}(u,v)\times F(u,v)
$$&lt;h4 id=&#34;高频加强滤波&#34;&gt;高频加强滤波
&lt;/h4&gt;&lt;p&gt;加强增强图像的高频成分。&lt;/p&gt;
&lt;p&gt;在高通滤波器函数前乘一个常数，再增加一个偏移量以便使零频率不被滤波器滤除掉。&lt;/p&gt;
$$
G(u,v)=H_{HP}(u,v)\times F(u,v)
$$$$
H_E(u,v)=k\times H_{HP}(u,v)+c
$$&lt;p&gt;
$k\ge 0$且$k\gt c$，$k$的典型值在$1.5$到$2.0$之间，$c$的典型值在$0.25$到$0.5$之间。&lt;/p&gt;
$$
\begin{aligned}
G_E(u,v)&amp;=H_E(u,v)\times F(u,v) \\
&amp;= [k\times H_{HP}(u,v)+c]\times F(u,v) \\
&amp;=k\times H_{HP}(u,v)\times F(u,v) + c\times F(u,v) \\
&amp;=k\times G(u,v)+c\times F(u,v)
\end{aligned}
$$&lt;h1 id=&#34;图像复原&#34;&gt;图像复原
&lt;/h1&gt;&lt;h2 id=&#34;图像退化复原过程的模型&#34;&gt;图像退化/复原过程的模型
&lt;/h2&gt;&lt;h3 id=&#34;图像退化与图像复原&#34;&gt;图像退化与图像复原
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;图像退化&lt;/strong&gt;是指图像在形式、存储、处理和传输过程中，由于成像系统、存储设备、处理方法和传输介质的不完善，从而导致的&lt;strong&gt;图像质量下降&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;引起图像退化的原因有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;成像系统的散焦；&lt;/li&gt;
&lt;li&gt;成像设备与物体的相对运动；&lt;/li&gt;
&lt;li&gt;成像器材的固有缺陷；&lt;/li&gt;
&lt;li&gt;外部干扰；&lt;/li&gt;
&lt;li&gt;&amp;hellip;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;图像复原&lt;/strong&gt;（图像恢复）指的是对退化的图像进行处理，试图恢复降质的图像。&lt;/p&gt;
&lt;p&gt;二者关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图像复原可以看作是图像退化的&lt;strong&gt;逆过程&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;实际情况中，退化过程往往并不知晓，这种复原称为&lt;strong&gt;盲目复原&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;图像模糊的同时，噪声和干扰也会同时存在。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;图像退化复原模型&#34;&gt;图像退化/复原模型
&lt;/h3&gt;$$
\begin{aligned}
f(x,y)\rightarrow 退化函数H \rightarrow &amp;\sum \stackrel{退化图像g(x,y)}{\longrightarrow} 复原滤波器 \rightarrow \hat{f}(x,y) \\
&amp;\uparrow 噪声n(x,y) \\ 
\end{aligned}
$$$$
g(x,y)=H[f(x,y)]+n(x,y)
$$&lt;p&gt;&lt;strong&gt;图像复原&lt;/strong&gt;：在给定$g(x,y)$和$H$的基础上得到对$f(x,y)$的某个近似，通常采用线性的、空间不变的复原技术。&lt;/p&gt;
&lt;p&gt;如果退化系统（函数）$H$是&lt;strong&gt;线性空间不变系统&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
$$
   \begin{aligned}
   &amp; H[k_1f_1(x,y)+k_2f_2(x,y)]=k_1H[f_1(x,y)]+k_2H[f_2(x,y)] \\
   \end{aligned}
   $$&lt;p&gt;
齐次性：$H[kf(x,y)]=kH[f(x,y)]$&lt;/p&gt;
&lt;p&gt;叠加性：$H[f_1(x,y)+f_2(x,y)]=H[f_1(x,y)]+H[f_2(x,y)]$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
$$
   H[f(x-a,y-b)]=g(x-a,y-b)\quad H[f(x,y)]=g(x,y)
   $$&lt;p&gt;
即图像中任一像素点通过退化系统时的响应只取决于该点的输入值，而与该点的位置无关。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;则退化图像可以表示为：&lt;/p&gt;
$$
g(x,y)=h(x,y)*f(x,y)+n(x,y)
$$$$
G(u,v)=H(u,v)F(u,v)+N(u,v)
$$&lt;h2 id=&#34;噪声模型&#34;&gt;噪声模型
&lt;/h2&gt;&lt;p&gt;图像中的噪声是&lt;strong&gt;随机&lt;/strong&gt;的，其&lt;strong&gt;灰度值的统计特征&lt;/strong&gt;可以用&lt;strong&gt;概率密度函数&lt;/strong&gt;（PDF）或相应的&lt;strong&gt;累积分布函数&lt;/strong&gt;（CDF）进行表征。&lt;/p&gt;
&lt;p&gt;对于退化图像中的噪声$n(x,y)$（&lt;em&gt;噪声的灰度值，非位置&lt;/em&gt;），有多种不同的统计模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;均匀（Uniform）噪声&lt;/li&gt;
&lt;li&gt;指数（Exponential）噪声&lt;/li&gt;
&lt;li&gt;高斯（Gaussian）噪声&lt;/li&gt;
&lt;li&gt;瑞利（Rayleigh）噪声&lt;/li&gt;
&lt;li&gt;伽马（爱尔兰）噪声&lt;/li&gt;
&lt;li&gt;脉冲（椒盐）噪声&lt;/li&gt;
&lt;li&gt;周期噪声&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;均匀噪声&#34;&gt;均匀噪声
&lt;/h3&gt;$$
\Large
p(z)=
\begin{cases}
\frac{1}{b-a}\quad &amp;a\le z\le b \\
0\quad &amp;其他
\end{cases}
$$$$
z=a+(b-a)\times U(0,1)
$$$$
\begin{aligned}
\mu &amp;= \frac{a+b}{2} \\
\sigma^2 &amp;= \frac{(b-a)^2}{12}
\end{aligned}
$$&lt;p&gt;
实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noisedIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;指数噪声&#34;&gt;指数噪声
&lt;/h3&gt;$$
\Large
p(z)=
\begin{cases}
ae^{-az}\quad &amp; z\ge 0\\
0\quad &amp; z\lt0
\end{cases}
$$$$
z=-\frac{1}{a}\times ln[1-U(0,1)]
$$&lt;p&gt;$$
\begin{aligned}
\mu&amp;amp;=\frac{1}{a} \
\sigma^2 &amp;amp;= \frac{1}{a^2}&lt;/p&gt;
&lt;p&gt;\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noiseIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;高斯噪声&#34;&gt;高斯噪声
&lt;/h3&gt;$$
\Large
p(z)=\frac{1}{\sqrt{2\pi}\sigma}\exp[{-\frac{(z-\mu)^2}{2\sigma^2}}]
$$$$
z=\mu+\sigma\times N(0,1)
$$&lt;p&gt;$N(0,1)$表示标准正态分布的随机数。&lt;/p&gt;
&lt;p&gt;灰度值有$70%$落在$[\mu-\sigma,\mu+\sigma]$范围内。&lt;/p&gt;
&lt;p&gt;实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noiseIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;瑞利噪声&#34;&gt;瑞利噪声
&lt;/h3&gt;$$
\Large
p(z)=
\begin{cases}
\frac{2}{b}(z-a)\exp[-\frac{(z-a)^2}{b}] &amp;z\ge a\\
0 &amp;z\lt a
\end{cases}
$$$$
z=a+\sqrt{-b\times ln[1-U(0,1)]}
$$$$
\begin{aligned}
\mu &amp;= a + \sqrt{\frac{\pi b}{4}} \\
\sigma^2&amp;=\frac{b(4-\pi)}{4}
\end{aligned}
$$&lt;p&gt;实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noiseIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;伽马噪声&#34;&gt;伽马噪声
&lt;/h3&gt;$$
\Large
p(z)=
\begin{cases}
\frac{a^bz^{b-1}}{(b-1)!}e^{-az} \quad &amp;z\gt0 \\
0 &amp;z\lt0
\end{cases}
$$$$
z=E_1+E_2+\cdots+E_b
$$$$
\begin{aligned}
\mu &amp;= \frac{b}{a} \\
\sigma^2 &amp;= \frac{b}{a^2}
\end{aligned}
$$&lt;p&gt;
实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noiseIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;脉冲噪声&#34;&gt;脉冲噪声
&lt;/h3&gt;$$
\Large
p(z)=
\begin{cases}
P_a \quad &amp;z=a \\
P_b &amp;z=b \\
0 &amp;其他
\end{cases}
$$&lt;p&gt;若$P_a$或$P_b$为零，则脉冲噪声称为单极脉冲；若$P_a$或$P_b$均不为零，则脉冲噪声成为双脉冲噪声或椒盐噪声。&lt;/p&gt;
&lt;p&gt;通常，$a$、$b$等于所允许的最小值和最大值。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;noiseIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noise&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;blackIm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;空间域滤波复原&#34;&gt;空间域滤波复原
&lt;/h2&gt;&lt;p&gt;当一幅图像中存在的唯一退化因素是噪声时，其退化模型如下：&lt;/p&gt;
$$
g(x,y)=f(x,y)+n(x,y)
$$$$
G(u,v)=F(u,v)+N(u,v)
$$&lt;p&gt;
可以选择空域滤波的方法来复原图像。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;均值滤波器、中点滤波器适合处理高斯或均匀分布等随机噪声；&lt;/li&gt;
&lt;li&gt;中值滤波器适合处理椒盐噪声；&lt;/li&gt;
&lt;li&gt;最大值滤波器适合处理“椒”噪声；&lt;/li&gt;
&lt;li&gt;最小值滤波器适合处理“盐”噪声。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;自适应滤波器&#34;&gt;自适应滤波器
&lt;/h3&gt;&lt;p&gt;自适应滤波行为基于由$m\times n$矩形窗口$S_{xy}$定义的区域内图像的统计特征。&lt;/p&gt;
&lt;p&gt;该类滤波器的响应基于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$g(x,y)$：图像$g$任意像素点的灰度值&lt;/li&gt;
&lt;li&gt;$\sigma_n^2$：被污染图像$g$的方差&lt;/li&gt;
&lt;li&gt;$m_L$：区域$S_{xy}$上像素点的灰度局部均值&lt;/li&gt;
&lt;li&gt;$\sigma_L^2$：区域$S_{xy}$上像素点的灰度局部方差&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;预期性能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若$\sigma_n^2=0$（零噪声），滤波器返回$g(x,y)$；&lt;/li&gt;
&lt;li&gt;若$\sigma_L^2$与$\sigma_n^2$高相关，滤波器返回$g(x,y)$的近似值；&lt;/li&gt;
&lt;li&gt;若$\sigma_L^2=\sigma_n^2$（局部性质和整个图像的性质相同），滤波器返回区域$S_{xy}$上像素的局部均值$m_L$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设噪声是加性和位置无关的，$\sigma_n^2 \le \sigma_L^2$。&lt;/p&gt;
$$
\hat{f}(x,y)=g(x,y)-\frac{\sigma_n^2}{\sigma_L^2}[g(x,y)-m_L]
$$&lt;p&gt;
$\sigma_n^2$是唯一事先需要知道的量。&lt;/p&gt;
&lt;h2 id=&#34;退化函数的估计&#34;&gt;退化函数的估计
&lt;/h2&gt;$$
G(u,v)=H(u,v)F(u,v)+N(u,v)
$$&lt;h3 id=&#34;图像观察估计法&#34;&gt;图像观察估计法
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;寻找简单结构、受噪声影响小的子图像$g_s(x,y)$；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;构造一个估计图像$\hat{f}_s(x,y)$，它和观察的子图像$g_s(x,y)$有相同大小和特性；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
$$
  H_s(u,v)=\frac{G_s(u,v)}{\hat{F}_s(u,v)}
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;试验估计法&#34;&gt;试验估计法
&lt;/h3&gt;$$
  H(u,v)=\frac{G(u,v)}{A}
$$&lt;p&gt;其中，$A$为常量，表示脉冲强度。&lt;/p&gt;
&lt;h3 id=&#34;模型估计法&#34;&gt;模型估计法
&lt;/h3&gt;&lt;h4 id=&#34;散焦模糊disk-blur&#34;&gt;散焦模糊（Disk Blur）
&lt;/h4&gt;$$
\large
h(x,y)=
\begin{cases}
\frac{1}{\pi R^2} \quad &amp;x^2+y^2\le R^2 \\
0 &amp;others
\end{cases}
$$$$
\Downarrow{DFT}
$$$$
H(u,v)=2\pi R \frac{J_1(R\sqrt{u^2+v^2})}{\sqrt{u^2+v^2}}
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$R$是散焦半径；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$J_1(\cdot)$是一阶第一类贝塞尔（Bessel）函数；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$H(u,v)$是圆对称的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;运动模糊motion-deblur&#34;&gt;运动模糊（Motion Deblur）
&lt;/h4&gt;$$
H(u,v)=\frac{T}{\pi (ua+vb)}sin[\pi (ua+vb)]e^{-j\pi (ua+vb)}
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T$为采集时间长度（曝光时间）；&lt;/li&gt;
&lt;li&gt;$a$、$b$分别为垂直、水平方向的运动距离。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;大气湍流模糊&#34;&gt;大气湍流模糊
&lt;/h4&gt;$$
H(u,v)=e^{-k(u^2+v^2)^{5/6}}
$$&lt;p&gt;其中，常数$k$与湍流的性质有关，$k$越大，湍流越剧烈。&lt;/p&gt;
&lt;h2 id=&#34;图像复原方法逆滤波&#34;&gt;图像复原方法——逆滤波
&lt;/h2&gt;$$
G(u,v)=H(u,v)F(u,v)+N(u,v)
$$$$
\hat{F}(u,v)=\frac{G(u,v)}{H(u,v)}
$$&lt;p&gt;
没有考虑噪声的处理。&lt;/p&gt;
&lt;h2 id=&#34;图像复原方法维纳滤波&#34;&gt;图像复原方法——维纳滤波
&lt;/h2&gt;$$
\min{MSE}=\min{\frac{1}{MN}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}[\hat{f}(x,y)-f(x,y)]^2}
$$$$
H_w(u,v)=\frac{1}{H(u,v)}\frac{|H(u,v)|^2}{|H(u,v)|^2+s\frac{|N(u,v)|^2}{|F(u,v)|^2}}
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H(u,v)$为退化函数；&lt;/li&gt;
&lt;li&gt;$|H(u,v)|^2$为$H(u,v)$的功率谱；&lt;/li&gt;
&lt;li&gt;$s$为最小二乘约束条件的拉格朗日常数；&lt;/li&gt;
&lt;li&gt;$|N(u,v)|^2$为噪声的功率谱；&lt;/li&gt;
&lt;li&gt;$|F(u,v)|^2$为未退化图像的功率谱。&lt;/li&gt;
&lt;li&gt;$\frac{|N(u,v)|^2}{|F(u,v)|^2}$为噪信功率比。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若退化图像具有较低的噪信功率比，则维纳滤波器$H_w(u,v)$近似为逆滤波器$\frac{1}{H(u,v)}$。如果噪声为0，则维纳滤波器退化为逆滤波。&lt;/p&gt;
$$
H_w(u,v)=\frac{1}{H(u,v)}\frac{|H(u,v)|^2}{|H(u,v)|^2+K}
$$&lt;h1 id=&#34;形态学图像处理&#34;&gt;形态学图像处理
&lt;/h1&gt;&lt;h2 id=&#34;概述-1&#34;&gt;概述
&lt;/h2&gt;&lt;p&gt;作用：&lt;strong&gt;简化图像数据&lt;/strong&gt;，去除图像中不重要的结构，仅保持图像的基本形状特性。&lt;/p&gt;
&lt;p&gt;基本思想：使用具有一定形态的&lt;strong&gt;结构元素&lt;/strong&gt;去&lt;strong&gt;度量和提取&lt;/strong&gt;图像中的对应&lt;strong&gt;形状&lt;/strong&gt;，以达到对图像进行处理和分析的目的。&lt;/p&gt;
&lt;p&gt;数学基础和所用语言：集合论&lt;/p&gt;
&lt;p&gt;基本运算：&lt;strong&gt;膨胀&lt;/strong&gt;、&lt;strong&gt;腐蚀&lt;/strong&gt;、&lt;strong&gt;开启&lt;/strong&gt;、&lt;strong&gt;闭合&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;集合论基础&#34;&gt;集合论基础
&lt;/h2&gt;&lt;h3 id=&#34;并交补差&#34;&gt;并、交、补、差
&lt;/h3&gt;$$
\begin{gather*}
&amp;C=A\cup B \\
&amp;D=A\cap B \\
&amp;A^c=\{w|w\notin A\} \\
&amp;A-B=\{w|w\in A, w\notin B\}=A\cap B^c
\end{gather*}
$$&lt;h3 id=&#34;反射与平移&#34;&gt;反射与平移
&lt;/h3&gt;&lt;h4 id=&#34;反射&#34;&gt;反射
&lt;/h4&gt;$$
\hat{B}=\{w|w=-b,b\in B\}
$$$$
B:(x,y)\rightarrow \hat{B}:(-x,-y)
$$&lt;h4 id=&#34;平移-1&#34;&gt;平移
&lt;/h4&gt;$$
(B)_z=\{c|c=b+z,b\in B\}
$$$$
B:(x,y)\rightarrow (B)_z:(x+z_1,y+z_2)
$$&lt;h3 id=&#34;二值图像的逻辑运算&#34;&gt;二值图像的逻辑运算
&lt;/h3&gt;$$
\begin{gather*}
NOT(A) \\
(A) AND (B) \\
(A)OR(B) \\
(A)XOR(B)
\end{gather*}
$$&lt;h2 id=&#34;二值图像形态学处理&#34;&gt;二值图像形态学处理
&lt;/h2&gt;&lt;p&gt;设$A$：像素集合，$B$：结构元素（成员是感兴趣目标的像素的集合），处理过程是用$B$对$A$进行操作。&lt;/p&gt;
&lt;p&gt;通过让$B$在$A$上平移，以便$B$的&lt;strong&gt;原点&lt;/strong&gt;访问$A$的每一个像素，以此得到一个新的像素集合。&lt;/p&gt;
&lt;p&gt;结构元素的&lt;strong&gt;原点&lt;/strong&gt;是形态学运算的&lt;strong&gt;参考点&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;原点可以包含在结构元素中，也可以不包含在结构元素中。&lt;/em&gt;&lt;/p&gt;
$$
\begin{bmatrix}
&amp; \cdot &amp;  \\
\cdot &amp; \bullet &amp; \cdot \\
 &amp; \cdot &amp;  \\
\end{bmatrix}
\quad
\begin{bmatrix}
\cdot &amp; \cdot &amp; \cdot \\
\cdot &amp; \bullet &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdot \\
\end{bmatrix}
\quad
\begin{bmatrix}
\cdot \\
\cdot \\
\bullet \\
\cdot \\
\cdot 
\end{bmatrix}
\quad
\begin{bmatrix}
&amp; &amp; &amp; \cdot &amp; &amp; &amp; \\
 &amp;  &amp; \cdot &amp; \cdot &amp; \cdot &amp;  &amp; \\
 &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;   \\
\cdot &amp; \cdot &amp; \cdot &amp; \bullet &amp; \cdot &amp; \cdot &amp; \cdot \\
 &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;   \\
 &amp;  &amp; \cdot &amp; \cdot &amp; \cdot &amp;  &amp; \\
&amp; &amp; &amp; \cdot &amp; &amp; &amp; \\
\end{bmatrix}
$$&lt;h2 id=&#34;膨胀和腐蚀&#34;&gt;膨胀和腐蚀
&lt;/h2&gt;&lt;h3 id=&#34;膨胀&#34;&gt;膨胀
&lt;/h3&gt;&lt;p&gt;效果：扩大图像中的物体。&lt;/p&gt;
$$
A\oplus B=\{z|(\hat{B})_z\cap A \ne \emptyset\}
$$$$
A\oplus B = \{z|[(\hat{B})_z\cap A]\subseteq A \}
$$&lt;p&gt;
即$A$被$B$膨胀的结果是满足上式的所有位移$z$的点（前景像素点）的集合。&lt;/p&gt;
&lt;p&gt;膨胀应用实例：桥接裂缝&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&amp;#34;&lt;span class=&#34;n&#34;&gt;broken_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tif&lt;/span&gt;&amp;#34;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imdilate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中结构元素$B:\begin{bmatrix}
0 &amp;amp; 1 &amp;amp; 0 \
1 &amp;amp; 1 &amp;amp; 1 \
0 &amp;amp; 1 &amp;amp; 0
\end{bmatrix}$&lt;/p&gt;
&lt;h3 id=&#34;腐蚀&#34;&gt;腐蚀
&lt;/h3&gt;&lt;p&gt;效果：缩小图像中的物体。&lt;/p&gt;
$$
A \ominus B = \{z|(B)_z \subseteq A\}
$$&lt;p&gt;
即，将结构元素$B$相对于集合$A$进行平移，只要平移后的结构元素都包含在集合$A$中，则这些位移$z$的点的集合（前景像素点）为腐蚀结果。
如果结构元素取$\begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 \
1 &amp;amp; 1 &amp;amp; 1 \
1 &amp;amp; 1 &amp;amp; 1
\end{bmatrix}$，腐蚀将使物体的边界沿周边减少一个像素。&lt;/p&gt;
&lt;p&gt;腐蚀可以去除&lt;strong&gt;小于结构元素&lt;/strong&gt;的物体。&lt;/p&gt;
&lt;p&gt;腐蚀应用实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;wirebond.tif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;square&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;%边长为11的方形结构元素&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imerode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;开启和闭合&#34;&gt;开启和闭合
&lt;/h2&gt;&lt;h3 id=&#34;开启&#34;&gt;开启
&lt;/h3&gt;$$
A \circ B=(A\ominus B)\oplus B
$$&lt;p&gt;
即先用$B$对$A$腐蚀，然后用$B$对腐蚀结果进行膨胀。&lt;/p&gt;
&lt;p&gt;性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A\circ B$是$A$的子集&lt;/li&gt;
&lt;li&gt;若$C \subseteq D$，则$C\circ B\subseteq D\circ B$&lt;/li&gt;
&lt;li&gt;$(A\circ B)\circ B=A \circ B$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;闭合&#34;&gt;闭合
&lt;/h3&gt;$$
A\bullet B=(A\oplus B)\ominus B
$$&lt;p&gt;
即先用$B$对$A$膨胀，然后用$B$对腐蚀结果进行腐蚀。&lt;/p&gt;
&lt;p&gt;性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A\bullet B$是$A$的子集&lt;/li&gt;
&lt;li&gt;若$C \subseteq D$，则$C\bullet B\subseteq D\bullet B$&lt;/li&gt;
&lt;li&gt;$(A\bullet B)\bullet B=A \bullet B$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实例matlab&#34;&gt;实例（MATLAB）
&lt;/h3&gt;&lt;h4 id=&#34;实例1&#34;&gt;实例1
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;shapes.tif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;square&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result_open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imopen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result_close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imclose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result_open_close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result_open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;实例2去除指纹图像上的杂散点&#34;&gt;实例2（去除指纹图像上的杂散点）
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A=imread(&amp;#39;noisy-fingerprint.tif&amp;#39;);
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;B=strel(&amp;#39;square&amp;#39;,3);
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;result_open=imopen(A,B);
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;result_open_close=imclose(result_open,B);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;形态学的主要应用&#34;&gt;形态学的主要应用
&lt;/h2&gt;&lt;h3 id=&#34;边界提取&#34;&gt;边界提取
&lt;/h3&gt;$$
b(A)=A-(A\ominus B)
$$&lt;p&gt;
其中，$B$是适当的结构元素。&lt;/p&gt;
&lt;p&gt;边界提取实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A=imread(&amp;#39;people.jpg&amp;#39;);
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;B=strel(&amp;#39;square&amp;#39;,3);
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;result=A-imerode(A,B);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;孔洞填充&#34;&gt;孔洞填充
&lt;/h3&gt;&lt;p&gt;孔洞：被&lt;strong&gt;前景像素&lt;/strong&gt;连成的边框所包围的&lt;strong&gt;背景区域&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;令$A$表示一个集合：其元素是$8$连通的边界，且每个边界包围一个孔洞；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;令$X_0$表示一个与包含$A$的相同大小的阵列，其初始状态为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包含每个孔洞中的一个指定位置处的前景像素点；&lt;/li&gt;
&lt;li&gt;除上述的前景像素点外，其余元素均为背景像素点。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
$$
  X_k=(X_{k-1}\oplus B)\cap A^c\quad k=1,2,3,\cdots
  $$&lt;ul&gt;
&lt;li&gt;其中，$B$是对称结构元素，$B=\begin{bmatrix}0 &amp;amp; 1 &amp;amp; 0 \ 1 &amp;amp; 1 &amp;amp; 1 \ 0 &amp;amp; 1 &amp;amp; 0\end{bmatrix}$；&lt;/li&gt;
&lt;li&gt;若$X_k=X_{k-1}$，则算法在迭代的第$k$步结束；&lt;/li&gt;
&lt;li&gt;集合$X_k$包含所有被填充的孔洞，$X_k$和$A$的并集则包含被填充的孔洞及其边界。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每一步运算中，膨胀结果与$A^c$的交集操作实现了将膨胀结果限制在感兴趣区域内，即条件膨胀。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;B对图像X的膨胀是B对X的&lt;strong&gt;前景元素&lt;/strong&gt;的膨胀。&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;图像缩放&#34;&gt;图像缩放
&lt;/h1&gt;&lt;h2 id=&#34;图像缩放的变换公式&#34;&gt;图像缩放的变换公式
&lt;/h2&gt;$$
\begin{aligned}
x&amp;=c_xx_0 \\
y&amp;=c_yy_0
\end{aligned}
$$$$
\begin{bmatrix}
x &amp; y &amp; 1
\end{bmatrix}=
\begin{bmatrix}
x_0 &amp; y_0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
c_x &amp; 0 &amp; 0\\
0 &amp; c_y &amp; 0\\
0 &amp; 0 &amp; 1
\end{bmatrix}=
\begin{bmatrix}
c_xx_0 &amp; c_yy_0 &amp; 1
\end{bmatrix}
$$&lt;h2 id=&#34;图像的缩小&#34;&gt;图像的缩小
&lt;/h2&gt;&lt;h3 id=&#34;图像缩小的实现方法&#34;&gt;图像缩小的实现方法
&lt;/h3&gt;&lt;p&gt;一个简单方法是等间隔地选取样本（重采样）。&lt;/p&gt;
$$
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 2 &amp; 0 &amp; 3 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 4 &amp; 0 &amp; 5 &amp; 0 &amp; 6  \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 7 &amp; 0 &amp; 8 &amp; 0 &amp; 9
\end{bmatrix}_{6\times 6}
\longrightarrow
\begin{bmatrix}
1 &amp; 2 &amp; 3\\
4 &amp; 5 &amp; 6\\
7 &amp; 8 &amp; 9
\end{bmatrix}_{3\times 3}
$$&lt;p&gt;
&lt;strong&gt;算法步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;确定重采样的行和列（采样间隔）&lt;/p&gt;
$$
   k_x=\frac{1}{c_x}\quad k_y=\frac{1}{c_y}
   $$&lt;/li&gt;
&lt;li&gt;
$$
   G(x,y)=F(int(k_x\times x),int(k_y\times y))
   $$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;图像的放大&#34;&gt;图像的放大
&lt;/h2&gt;&lt;h3 id=&#34;图像放大的实现方法&#34;&gt;图像放大的实现方法
&lt;/h3&gt;$$
\begin{bmatrix}
1 &amp; 2\\
3 &amp; 4
\end{bmatrix}_{2\times 2}
\longrightarrow
\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 2 &amp; 2 &amp; 2 \\
1 &amp; 1 &amp; 1 &amp; 2 &amp; 2 &amp; 2 \\
1 &amp; 1 &amp; 1 &amp; 2 &amp; 2 &amp; 2 \\
3 &amp; 3 &amp; 3 &amp; 4 &amp; 4 &amp; 4 \\
3 &amp; 3 &amp; 3 &amp; 4 &amp; 4 &amp; 4 \\
3 &amp; 3 &amp; 3 &amp; 4 &amp; 4 &amp; 4 \\
\end{bmatrix}_{6\times 6}
$$&lt;p&gt;
问题：容易出现马赛克效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;计算放大后图像的大小&lt;/p&gt;
&lt;p&gt;$M\times N \rightarrow c_xM\times C_yN$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
$$
   G(x,y)=F(\frac{x}{c_x},\frac{y}{c_y})
   $$&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{bmatrix}
11 &amp; 12 &amp; 13 \\
21 &amp; 22 &amp; 23 \\
31 &amp; 32 &amp; 33
\end{bmatrix}_{3\times 3}
\longrightarrow
\begin{bmatrix}
11 &amp; ? &amp; 12 &amp; ? &amp; 13 &amp; ? \\
21 &amp; ? &amp; 22 &amp; ? &amp; 23 &amp; ?\\
31 &amp; ? &amp; 32 &amp; ? &amp; 33 &amp; ?
\end{bmatrix}_{3\times 6}
$$$$
\begin{aligned}
G(0,0)=F(0,0) \quad G(0,1)=F(0,0.5)=?\\
G(1,0)=F(1,0) \quad G(1,1)=F(1,0.5)=?\\
G(2,0)=F(2,0) \quad G(2,1)=F(2,0.5)=?
\end{aligned}
$$&lt;h3 id=&#34;最近邻插值&#34;&gt;最近邻插值
&lt;/h3&gt;&lt;p&gt;将放大后未知的像素点坐标换算到原始图像，与原始图像上邻近的$4$个像素点比较，最靠近邻近点的像素值即为该未知像素点的像素值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$(u,v)(G)\rightarrow(x+\Delta{x},y+\Delta{y})$；&lt;/li&gt;
&lt;li&gt;计算$(x+\Delta{x},y+\Delta{y})$与$(x,y)、(x,y+1)、(x+1,y)、(x+1,y+1)$之间的距离，取距离最短的点的像素值作为$(u,v)$的像素值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;双线性插值&#34;&gt;双线性插值
&lt;/h3&gt;&lt;p&gt;将放大后未知的像素点坐标换算到原始图像，计算原始图像上$4$个邻近像素点$A、B、C、D$对$P$点的影响，$P$点灰度值由$4$个邻近点灰度值加权求和得到（权值可以用距离进行度量）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
  (x,y)、(x,y+1)、(x+1,y)、(x+1,y+1)
  $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由$A、B$两点插值计算出$e$点的灰度值的$F(x,y+\Delta{y})$；&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
F(x,y+\Delta{y})&amp;=\frac{\sqrt{(x-x)^2+((y+1)-(y+\Delta{y}))^2}}{\sqrt{(x-x)^2+((y+1)-y)^2}}F(x,y)\\
&amp;\quad\ +\frac{\sqrt{(x-x)^2+((y+\Delta{y})-y)^2}}{\sqrt{(x-x)^2+((y+1)-y)^2}}F(x,y+1) \\
&amp;= (1-\Delta{y})\times F(x,y)+\Delta{y}\times F(x,y+1)
\end{aligned}
$$$$
\begin{bmatrix}
B(x,y+1) &amp; \cdot &amp; \cdot \\
e(x,y+\Delta{y}) &amp; \cdot &amp; \cdot\\
A(x,y) &amp; \cdot &amp; \cdot
\end{bmatrix}
$$&lt;ul&gt;
&lt;li&gt;由$C、D$两点插值计算出$f$点的灰度值$F(x+1,y+\Delta{y})$；&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
F(x+1,y+\Delta{y})&amp;=\frac{\sqrt{((x+1)-(x+1))^2+((y+1)-(y+\Delta{y}))^2}}{\sqrt{((x+1)-(x+1))^2+((y+1)-y)^2}}F(x+1,y)\\
&amp;\quad\ +\frac{\sqrt{((x+1)-(x+1))^2+((y+\Delta{y})-y)^2}}{\sqrt{((x+1)-(x+1))^2+((y+1)-y)^2}}F(x+1,y+1) \\
&amp;= (1-\Delta{y})\times F(x+1,y)+\Delta{y}\times F(x+1,y+1)
\end{aligned}
$$$$
\begin{bmatrix}
\cdot &amp; \cdot &amp; D(x+1,y+1) \\
\cdot &amp; \cdot &amp; f(x+1,y+\Delta{y})\\
\cdot &amp; \cdot &amp; C(x+1,y)
\end{bmatrix}
$$&lt;ul&gt;
&lt;li&gt;由$e、f$两点插值计算出$P$点的灰度值$F(x+\Delta{x},y+\Delta{y})$。&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
F(x+\Delta{x},y+\Delta{y})&amp;=\frac{\sqrt{((x+1)-(x+\Delta{x}))^2+((y+\Delta{y})-(y+\Delta{y}))^2}}{\sqrt{((x+1)-x)^2+((y+\Delta{y})-(y+\Delta{y}))^2}}F(x,y+\Delta{y})\\
&amp;\quad\ +\frac{\sqrt{((x+\Delta{x})-x)^2+((y+\Delta{y})-(y+\Delta{y}))^2}}{\sqrt{((x+1)-x)^2+((y+\Delta{y})-(y+\Delta{y}))^2}}F(x+1,y+\Delta{y}) \\
&amp;= (1-\Delta{x})\times F(x,y+\Delta{y})+\Delta{x}\times F(x+1,y+\Delta{y})
\end{aligned}
$$$$
\begin{bmatrix}
\cdot &amp; \cdot &amp; \cdot \\
e(x,y+\Delta{y}) &amp; P(x+\Delta{x},y+\Delta{y}) &amp; f(x+1,y+\Delta{y})\\
\cdot &amp; \cdot &amp; \cdot
\end{bmatrix}
$$$$
\begin{bmatrix}
11 &amp; 12 &amp; 13 \\
21 &amp; 22 &amp; 23 \\
31 &amp; 32 &amp; 33
\end{bmatrix}_{3\times 3}
\longrightarrow
\begin{bmatrix}
? &amp; ? &amp; ? &amp; ? &amp; ? &amp; ? \\
? &amp; ? &amp; ? &amp; ? &amp; ? &amp; ?\\
? &amp; ? &amp; ? &amp; ? &amp; ? &amp; ?
\end{bmatrix}_{3\times 6}
$$$$
\begin{aligned}
X&amp;=
\begin{bmatrix}
0 &amp; 1 &amp; 2
\end{bmatrix}
\\
Y&amp;=
\begin{bmatrix}
0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5
\end{bmatrix}
\end{aligned}
$$$$
\begin{aligned}
X&amp;=
\begin{bmatrix}
0 &amp; 1 &amp; 2
\end{bmatrix}
\\
Y&amp;=
\begin{bmatrix}
0 &amp; 0.5 &amp; 1 &amp; 1.5 &amp; 2 &amp; 2.5
\end{bmatrix}
\end{aligned}
$$$$
\begin{bmatrix}
11 &amp; ? &amp; 12 &amp; ? &amp; 13 &amp; ? \\
21 &amp; ? &amp; 22 &amp; ? &amp; 23 &amp; ?\\
31 &amp; ? &amp; 32 &amp; ? &amp; 33 &amp; ?
\end{bmatrix}_{3\times 6}
$$$$
\begin{bmatrix}
F(0,0) &amp; F(0,1) \\
F(1,0) &amp; F(1,1)
\end{bmatrix}
$$&lt;p&gt;
$\cdots\cdots$&lt;/p&gt;
&lt;h3 id=&#34;双三次插值&#34;&gt;双三次插值
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;算法原理&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;未知像素点$P(u,v)(G)\rightarrow$ 原始图像空间$(x,y)$；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;确定原始图像上的$16$个邻近像素点；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;采用下式计算$P$点的灰度值$F(x,y)$：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
F(x,y)=\sum_{i=0}^3\sum_{j=0}^{3}a_{ij}x^iy^j
$$&lt;p&gt;其中，$16$个未知系数$a_{ij}$可由原始图像$(x,y)$处的$16$个邻近像素所确定的方程组进行求解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
$$
   \begin{bmatrix}
   \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; &amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; \bullet&amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; &amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot &amp;  &amp; \cdot \\
   \end{bmatrix}
   $$&lt;/li&gt;
&lt;li&gt;
$$
   \begin{bmatrix}
   \cdot &amp;  &amp; \cdot &amp; A\cdot &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; &amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp; B\cdot &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; \bullet&amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp; C\cdot &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; &amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp; D\cdot &amp; \cdot &amp;  &amp; \cdot 
   \end{bmatrix}
   $$$$
   F(x,y)=\sum_{j=0}^{3}a_jy^j
   $$&lt;/li&gt;
&lt;li&gt;
$$
   \begin{bmatrix}
   \cdot &amp;  &amp; \cdot &amp; A\cdot &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; &amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp; B\cdot &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; P\bullet&amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp; C\cdot &amp; \cdot &amp;  &amp; \cdot \\
   &amp; &amp; &amp; &amp; &amp; &amp; \\
   \cdot &amp;  &amp; \cdot &amp; D\cdot &amp; \cdot &amp;  &amp; \cdot 
   \end{bmatrix}
   $$$$
   F(x,y)=\sum_{i=0}^{3}b_ix^i
   $$&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;图像边缘检测&#34;&gt;图像边缘检测
&lt;/h1&gt;&lt;h2 id=&#34;概述-2&#34;&gt;概述
&lt;/h2&gt;&lt;p&gt;物体边界、表面方向的改变、不同的颜色、光照明暗的变化&amp;hellip;&lt;/p&gt;
&lt;p&gt;图像边缘是一组相连的像素集合，这些像素位于两个不同区域的边界上。边缘检测是一种典型的图像预处理过程。&lt;/p&gt;
&lt;h3 id=&#34;图像的边缘模型&#34;&gt;图像的边缘模型
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;台阶边缘&lt;/p&gt;
$$
   \begin{bmatrix}
   0 &amp; 0 &amp; 0 &amp; 3 &amp; 3 &amp; 3
   \end{bmatrix}
   $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;斜坡边缘&lt;/p&gt;
$$
   \begin{bmatrix}
   0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 3 &amp; 3
   \end{bmatrix}
   $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;屋顶边缘&lt;/p&gt;
$$
   \begin{bmatrix}
   0 &amp; 0 &amp; 1 &amp; 3 &amp; 1 &amp; 0 &amp; 0
   \end{bmatrix}
   $$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;无噪图像的导数与边缘的关系&#34;&gt;无噪图像的导数与边缘的关系
&lt;/h3&gt;$$
\begin{bmatrix}
f &amp; &amp;0 &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 3 &amp; 3 \\
f&#39; &amp; &amp;0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
f&#39;&#39; &amp; &amp;0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0
\end{bmatrix}
$$&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一阶导数的幅值可检测图像中某个点处是否存在一个边缘（峰值为边缘的位置）；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;二阶导数的符号可用于确定一个边缘像素位于该边缘偏暗的一侧还是偏亮的一侧；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于图像中的每条边缘，二阶导数生成两个值，同时二阶导数的零交叉点可用于定位粗边缘的中心。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;基本的边缘检测技术&#34;&gt;基本的边缘检测技术
&lt;/h2&gt;&lt;h3 id=&#34;图像梯度及其性质&#34;&gt;图像梯度及其性质
&lt;/h3&gt;$$
\begin{gather*}
\nabla f=
\begin{bmatrix}
g_x &amp; g_y
\end{bmatrix}^T=
\begin{bmatrix}
\frac{\partial f}{\partial x} &amp; \frac{\partial f}{\partial y}
\end{bmatrix}^T
\\
|\nabla f|=\sqrt{g_x^2+g_y^2}=\sqrt{(\frac{\partial f}{\partial x})^2 + (\frac{\partial f}{\partial y})^2}
\\
\alpha(x,y)=\arctan[\frac{g_y}{g_x}]
\end{gather*}
$$&lt;p&gt;任意点$(x,y)$处边缘的方向与该点处梯度的方向$\alpha(x,y)$正交。&lt;/p&gt;
&lt;h3 id=&#34;梯度算子直接差分算子&#34;&gt;梯度算子——直接差分算子
&lt;/h3&gt;$$
\begin{aligned}
g_x=f(x+1,y)-f(x,y) \\
g_y=f(x,y+1)-f(x,y)
\end{aligned}
$$$$
\begin{bmatrix}
\underline{-1} &amp; 0\\
1 &amp; 0
\end{bmatrix}
\quad
\begin{bmatrix}
\underline{-1} &amp; 1\\
0 &amp; 0
\end{bmatrix}
$$&lt;p&gt;
直接差分算子仅能检测&lt;strong&gt;水平、垂直方向&lt;/strong&gt;的边缘。&lt;/p&gt;
&lt;h3 id=&#34;梯度算子roberts算子&#34;&gt;梯度算子——Roberts算子
&lt;/h3&gt;$$
\begin{aligned}
g_x=f(x+1,y+1)-f(x,y) \\
g_y=f(x+1,y)-f(x,y+1)
\end{aligned}
$$$$
\begin{bmatrix}
\underline{-1} &amp; 0\\
0 &amp; 1
\end{bmatrix}
\quad
\begin{bmatrix}
\underline{0} &amp; -1\\
1 &amp; 0
\end{bmatrix}
$$&lt;p&gt;
Roberts算子可用于检测&lt;strong&gt;对角线方向&lt;/strong&gt;的边缘。&lt;/p&gt;
&lt;h3 id=&#34;梯度算子prewitt算子&#34;&gt;梯度算子——Prewitt算子
&lt;/h3&gt;$$
\begin{bmatrix}
-1 &amp; -1 &amp; -1 \\
0 &amp; \underline{0} &amp; 0\\
1 &amp; 1 &amp; 1
\end{bmatrix}
\quad
\begin{bmatrix}
-1 &amp; 0 &amp; 1 \\
-1 &amp; \underline{0} &amp; 1\\
-1 &amp; 0 &amp; 1
\end{bmatrix}
$$&lt;h3 id=&#34;梯度算子sobel算子&#34;&gt;梯度算子——Sobel算子
&lt;/h3&gt;$$
\begin{bmatrix}
-1 &amp; -2 &amp; -1 \\
0 &amp; \underline{0} &amp; 0\\
1 &amp; 2 &amp; 1
\end{bmatrix}
\quad
\begin{bmatrix}
-1 &amp; 0 &amp; 1 \\
-2 &amp; \underline{0} &amp; 2\\
-1 &amp; 0 &amp; 1
\end{bmatrix}
$$&lt;h3 id=&#34;梯度算子用于检测对角边缘的prewittsobel算子&#34;&gt;梯度算子——用于检测对角边缘的Prewitt、Sobel算子
&lt;/h3&gt;&lt;p&gt;对上述的Prewit模板和Sobel模板作出修改，以便它们沿对角线方向有最大的响应。&lt;/p&gt;
$$
45\degree 方向梯度
\begin{bmatrix}
0 &amp; 1 &amp; 1 \\
-1 &amp; \underline{0} &amp; 1\\
-1 &amp; -1 &amp; 0
\end{bmatrix}
\quad
-45\degree 方向梯度
\begin{bmatrix}
-1 &amp; -1 &amp; 0 \\
-1 &amp; \underline{0} &amp; 1\\
0 &amp; 1 &amp; 1
\end{bmatrix}
$$$$
45\degree 方向梯度
\begin{bmatrix}
0 &amp; 1 &amp; 2 \\
-1 &amp; \underline{0} &amp; 1\\
-2 &amp; -1 &amp; 0
\end{bmatrix}
\quad
-45\degree 方向梯度
\begin{bmatrix}
-2 &amp; -1 &amp; 0 \\
-1 &amp; \underline{0} &amp; 1\\
0 &amp; 1 &amp; 2
\end{bmatrix}
$$&lt;p&gt;
一些对边缘检测不必要的细节往往表现为噪声，处理方法为：对图像&lt;strong&gt;进行平滑处理后再进行边缘检测&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;参考程序（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im2double&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;building.tif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fspecial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;average&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;gx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;gy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;imGrad&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imGrad&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im2double&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;building.tif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fspecial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;average&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;template45&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;template135&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;grad45&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;template45&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;grad135&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;template135&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;im&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;grad45&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;grad135&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;先进的边缘检测技术&#34;&gt;先进的边缘检测技术
&lt;/h2&gt;&lt;h3 id=&#34;marr-hildreth马尔-希尔德雷斯边缘检测器&#34;&gt;Marr-Hildreth（马尔-希尔德雷斯）边缘检测器
&lt;/h3&gt;&lt;h4 id=&#34;基于二阶微分导数的边缘检测技术拉普拉斯算子&#34;&gt;基于二阶微分（导数）的边缘检测技术——拉普拉斯算子
&lt;/h4&gt;$$
\nabla^2f=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2}
$$$$
\begin{aligned}
\frac{\partial^2f}{\partial x^2}=f(x+1,y)+f(x-1,y)-2f(x,y) \\
\frac{\partial^2f}{\partial y^2}=f(x,y+1)+f(x,y-1)-2f(x,y)
\end{aligned}
$$$$
\begin{bmatrix}
0 &amp; -1 &amp; 0\\
-1 &amp; \underline{4} &amp; -1\\
0 &amp; -1 &amp; 0
\end{bmatrix}
$$&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
$$
    \begin{bmatrix}
    f&#39;&#39; &amp; &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 0
    \end{bmatrix}
    $$&lt;p&gt;
&lt;em&gt;连接$-1$和$1$，与轴线相交的点即为零交叉点。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以确定一个像素是在边缘暗的一边还是亮的一边。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对噪声具有敏感性；&lt;/li&gt;
&lt;li&gt;幅值产生双边缘；&lt;/li&gt;
&lt;li&gt;不能检测边缘的方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;marr-hildreth边缘检测器的提出及实现&#34;&gt;Marr-Hildreth边缘检测器的提出及实现
&lt;/h4&gt;$$
g(x,y)=[\nabla^2G(x,y)]*f(x,y)
$$$$
g(x,y)=\nabla^2[G(x,y)*f(x,y)]
$$&lt;p&gt;
即，先使用一个高斯平滑滤波器平滑图像，然后对该结果执行拉普拉斯变换，故Marr-Hildreth边缘检测算法&lt;strong&gt;实现步骤&lt;/strong&gt;如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用高斯滤波器对输入图像进行平滑滤波；&lt;/li&gt;
&lt;li&gt;计算由第一步骤得到的图像的拉普拉斯变换；&lt;/li&gt;
&lt;li&gt;寻找第二步骤所的图像的零交叉（由此得到的边缘为一个像素宽）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;满足上述要求的算子是$\nabla^2G$（高斯拉普拉斯算子，简称LoG算子），其中：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
$$
   \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}
   $$&lt;/li&gt;
&lt;li&gt;
$$
   G(x,y)=e^{-\frac{x^2+y^2}{2\sigma^2}}
   $$&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{aligned}
\nabla^2G(x,y)&amp;=\frac{\partial^2G(x,y)}{\partial x^2}+\frac{\partial^2G(x,y)}{\partial y^2}\\
&amp;=\frac{\partial}{\partial x}[\frac{-x}{\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}]+\frac{\partial}{\partial y}[\frac{-y}{\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}] \\
&amp;=[\frac{x^2}{\sigma^4}-\frac{1}{\sigma^2}]e^{-\frac{x^2+y^2}{2\sigma^2}}+[\frac{y^2}{\sigma^4}-\frac{1}{\sigma^2}]e^{-\frac{x^2+y^2}{2\sigma^2}}\\
&amp;=\frac{x^2+y^2-2\sigma^2}{\sigma^4}e^{-\frac{x^2+y^2}{2\sigma^2}}
\end{aligned}
$$$$
\begin{bmatrix}
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; -2 &amp; -1 &amp; 0 \\
-1 &amp; -2 &amp; 16 &amp; -2 &amp; -1 \\
0 &amp; -1 &amp; -2 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0
\end{bmatrix}
$$&lt;p&gt;
通过该模板得到的图像来寻找零交叉点以进行图像的边缘检测。&lt;/p&gt;
&lt;p&gt;实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;edge_LoG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;edge&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputImage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;log&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;寻找零交叉的方法&#34;&gt;寻找零交叉的方法
&lt;/h4&gt;&lt;p&gt;判定图像$g(x,y)$的任意像素$p$是否为零交叉点的一种方法如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在图像$g(x,y)$中找到一个以$p$为中心的$3\times 3$邻域；&lt;/li&gt;
&lt;li&gt;$p$像素为零交叉点意味着至少有&lt;strong&gt;两个相对&lt;/strong&gt;的邻域像素的符号不同，有4种要测试的情况：左/右、上/下和两个对角；&lt;/li&gt;
&lt;li&gt;如果相对的两个邻域像素的符号不同，而且它们的像素值与$p$的像素值的绝对值差值超过指定的&lt;strong&gt;阈值&lt;/strong&gt;。那么，$p$即为一个零交叉像素。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于阈值为$0$的零交叉检测，会产生严重的意大利通心粉效应：所有的边缘都形成闭环，使用正阈值可避免闭环边缘。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优点：
&lt;ul&gt;
&lt;li&gt;零交叉点图像中的边缘比梯度边缘细；&lt;/li&gt;
&lt;li&gt;抑制噪声能力和反干扰性能好。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;缺点：
&lt;ul&gt;
&lt;li&gt;边缘由零交叉点构成，而零交叉点计算比较复杂。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;canny坎尼边缘检测器&#34;&gt;Canny（坎尼）边缘检测器
&lt;/h3&gt;&lt;p&gt;坎尼边缘检测器是基于一阶微分的边缘检测方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实现步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用一个大小为$n\times n$的高斯滤波器平滑输入图像（&lt;strong&gt;$n$的取值应为大于或等于$6$倍高斯滤波器的标准差的最小奇整数&lt;/strong&gt;）；&lt;/li&gt;
&lt;li&gt;计算滤波后图像的梯度幅值和方向角度；&lt;/li&gt;
&lt;li&gt;对梯度幅值执行&lt;strong&gt;非极大值抑制&lt;/strong&gt;（剔除伪边缘点，保留候选边缘点）；&lt;/li&gt;
&lt;li&gt;对非极大值抑制的结果使用&lt;strong&gt;双阈值&lt;/strong&gt;检测边缘（从候选边缘点中选择真实边缘点）；&lt;/li&gt;
&lt;li&gt;采用&lt;strong&gt;连接分析&lt;/strong&gt;对双阈值边缘检测结果进行连接（得到连续完整的边缘）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;非极大值抑制non-maxima-suppression-nms&#34;&gt;非极大值抑制（Non-Maxima Suppression, NMS）
&lt;/h4&gt;&lt;p&gt;仅保留梯度幅值图像$M(x,y)$的极大值（严格上，保留梯度方向的极大值点），以实现边缘细化。&lt;/p&gt;
$$
\begin{bmatrix}
&amp; &amp; &amp; &amp; &amp; &amp; \cdot B&amp; &amp; &amp; &amp; \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; \cdot C&amp; &amp; &amp; \\
&amp; &amp; &amp; &amp; &amp; \cdot A&amp; &amp; &amp; &amp; &amp; \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\
Th-&amp;-&amp;-&amp;-&amp;-&amp;-&amp;-&amp;-&amp;-&amp;-&amp;- \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\
&amp; &amp; &amp; &amp;\cdot &amp; &amp; &amp; &amp; \cdot&amp; &amp; \\
&amp;\cdot &amp; &amp;\cdot &amp; &amp; &amp; &amp; &amp; &amp;\cdot &amp; \\
\cdot&amp; &amp;\cdot &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \cdot\\
\end{bmatrix}
$$&lt;p&gt;
&lt;strong&gt;实现步骤&lt;/strong&gt;：（假设仅保留梯度幅值极大值的结果为$N(x,y)$）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将$N(x,y)$初始化为原始的梯度幅值图像$M(x,y)$；&lt;/li&gt;
&lt;li&gt;对于每个点$N(x,y)$，在梯度方向和反梯度方向各找$n$个像素点，若$N(x,y)$不是这些点中的最大点，则将$N(x,y)$置零，否则保持$N(x,y)$不变。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;对nms结果使用双阈值检测边缘&#34;&gt;对NMS结果使用双阈值检测边缘
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;检测过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;指定两个阈值$T_H、T_L$：$T_H\gt T_L$（建议高阈值与低阈值比率为$2:1$或$3:1$）；&lt;/li&gt;
&lt;li&gt;使用高阈值$T_H$检测边缘，得到高阈值边缘图$E_H(x,y)$（边缘点少但可靠）；&lt;/li&gt;
&lt;li&gt;使用低阈值$T_L$检测边缘，得到低阈值边缘图$E_L(x,y)$（边缘点多但错误检测率高）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;对双阈值边缘检测结果进行边缘连接&#34;&gt;对双阈值边缘检测结果进行边缘连接
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;连接过程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将高阈值边缘图$E_H(x,y)$中相连的边缘点输出为一副边缘图像$E(x,y)$；&lt;/li&gt;
&lt;li&gt;对于$E(x,y)$中每条边，从端点出发在低阈值边缘图$E_L(x,y)$中寻找其延长的部分，直至与$E(x,y)$中另外一条边的端点相连（8连通性），否则认为$E_L(x,y)$中没有它延长的部分；&lt;/li&gt;
&lt;li&gt;将$E(x,y)$作为结果输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Canny边缘检测实例（MATLAB）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;edge_LoG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;edge&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputImage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;canny&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T1&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>2021 年度总结</title>
        <link>https://demo.stack.jimmycai.com/p/2021-%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/</link>
        <pubDate>Fri, 31 Dec 2021 22:54:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/2021-%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/</guid>
        <description>&lt;p&gt;今天是2021年的最后一天，在此总结一下这一年来的收获。&lt;/p&gt;
&lt;h1 id=&#34;回顾2021&#34;&gt;回顾2021
&lt;/h1&gt;&lt;h2 id=&#34;兴趣&#34;&gt;兴趣
&lt;/h2&gt;&lt;h3 id=&#34;摄影&#34;&gt;摄影📷
&lt;/h3&gt;&lt;p&gt;　　去年年初将主力机换为小米10 Pro，16mm-28mm-50mm-94mm的组合让我第一次体验到了不同焦段下的视觉效果，今年年初也如愿购入一台富士无反相机和一支XC16-50mmⅡ镜头，从XC35mm F2再到7artisans 55mm F1.4第一次体验了到大光定的魅力，转接EF-S 55-250mm成为第一支长焦镜头，7artisans 12mm F2.8又把视角从长焦带回超广角&amp;hellip;&lt;br&gt;
　　“摄影是一门用光的艺术”，这句话说的确实有道理，入坑摄影的这一年来，钱似乎已经被我用光了。但是相机已经成为了我出门必须要携带的工具，虽然开始的开始，总是忘记调整参数，大晚上的还傻乎乎把ISO拨到320、把光圈拨到F8，拍出来一片黑还抱怨相机不够“智能”，不能像手机一样一键出片，但年底的一次手机拍摄又让我疯狂想念用相机拍摄的日子。&lt;br&gt;
　　下面放几张今年拍的照片吧，拍的很一般，这一年出门的机会也不多，希望明年能多出去走走。&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/2021_0218_17253700.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;2021-1 潜江 森林公园&#34;
	
	
&gt;&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/IMG_20210418_212740.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;2021-4 武汉 得胜桥&#34;
	
	
&gt;&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/IMG_20210621_091038.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;2021-6 武汉 归元禅寺&#34;
	
	
&gt;&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/IMG_20211231_175958.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;2021-6 武汉 晴川阁&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;吉他&#34;&gt;吉他🎸
&lt;/h3&gt;&lt;p&gt;　　今年真正静下心来练琴的时间可能只有年初的寒假了，寒假之后只带了木吉他去学校，每天晚上回宿舍闲着了就拿起来简单弹一弹。去年年末在室友的疯狂输出下，学习了YoungsoKim的Like A Star。希望明年能多抽出时间练练琴、学学新歌，每次拿起琴都在磨同一首歌自己都有点听腻了。&lt;/p&gt;
&lt;h2 id=&#34;学习&#34;&gt;学习
&lt;/h2&gt;&lt;h3 id=&#34;研考&#34;&gt;研考📖
&lt;/h3&gt;&lt;p&gt;　　备战2022研考是我过去一年投入时间最多的一件事情，这可能也是我整个大学期间学习最认真的一段时间，期间经历了大起大落，也做出了很多我以前重来不敢想的决定。&lt;br&gt;
　　我所报考的学校和我最初的想法相差甚远。虽然最初就准备考数一英一408这套噩梦组合，但中间因为参加了几场大大小小的比赛，耽误了一些时间，就把专业课从4门缩到2门最后再到一门DS，九月份返校后，在仅结束完DS一轮的情况下，我决定将专业课重新改回为408，也就在那天，408考纲大改，仅DS就增加了并查集和红黑树两个大头。那段时间非常痛苦，庆幸的是熬过来了。&lt;br&gt;
　　写这篇文章时，初试已经过去几天了，粗略对了答案，虽然数学略有遗憾，但整体还是比较满意，没有辜负自己大半年的努力。&lt;/p&gt;
&lt;h3 id=&#34;竞赛&#34;&gt;竞赛🏆
&lt;/h3&gt;&lt;p&gt;　　本以为准备考研了就基本不会再打比赛了，但是今年上半年还是抽出了很多时间来参加比赛，一方面是弥补一下去年的遗憾，二来是打比赛真的会让人上瘾[表情]，下面列举一些今年参加的一些大大小小的比赛吧~&lt;br&gt;
　　- &lt;em&gt;&lt;strong&gt;山东省第二届数据应用创新创业大赛主赛场-疫情密切接触人员追踪 决赛Rank16&lt;/strong&gt;&lt;/em&gt;&lt;br&gt;
　　- &lt;em&gt;&lt;strong&gt;招商银行2021FinTech精英训练营 Rank53&lt;/strong&gt;&lt;/em&gt;&lt;br&gt;
　　- &lt;em&gt;&lt;strong&gt;中博教育财经求职力挑战赛 （当了一把摸鱼的混子）&lt;/strong&gt;&lt;/em&gt;&lt;br&gt;
　　- &lt;em&gt;&lt;strong&gt;2021中国高校计算机大赛-微信大数据挑战赛 Rank70 全国三等奖&lt;/strong&gt;&lt;/em&gt;&lt;br&gt;
　　- &lt;em&gt;&lt;strong&gt;第十三届全国大学生数学竞赛-湖北赛区-非数学类 一等奖&lt;/strong&gt;&lt;/em&gt;&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/1640968823066.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;2021-8 微信赛 刘老师发的纪念T恤&#34;
	
	
&gt;
　　明年如果能有机会读研，希望能有更多的时间参加比赛，也希望可以提高一下自己的能力，争取拿个top~&lt;/p&gt;
&lt;h2 id=&#34;生活&#34;&gt;生活
&lt;/h2&gt;&lt;h3 id=&#34;音乐&#34;&gt;音乐🎵
&lt;/h3&gt;&lt;p&gt;　　今年非常喜欢房东的猫，每次试图让自己安静下来就喜欢听她们的歌。本想着疫情好转，今年能在武汉看到她们的现场，但今年下半年她们没有在武汉安排巡演，年底25号的演出也因为研考错过了，希望明年可以去现场看到小黑和佩岭。&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/e8f8bd91gy1gwpj60z2oyj22gw1n9e81.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;
　　小马丁和STMPD RCRDS今年也发了很多高质量曲目，可惜因为疫情，这两年可能很难在国内看到马老板的演出了。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/e97a945agy1gxgxn6k12oj20u011c76p.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;饮食&#34;&gt;饮食🍕
&lt;/h3&gt;&lt;p&gt;　　今年的活动半径似乎并不是很大，少有的出门时间几乎都花在吃饭上了，那就说说今年去过的一些店吧~&lt;br&gt;
　　- &lt;strong&gt;北疆饭店 新疆菜&lt;/strong&gt;&lt;br&gt;
　　　　在银泰创意城。去过好多次，比较喜欢大盘鸡和羊肉串，味道正宗。&lt;br&gt;
　　- &lt;strong&gt;添好彩、茶港 茶餐厅&lt;/strong&gt;&lt;br&gt;
　　　　在武汉天地。最喜欢叉烧饭，吃多了有点小腻。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/IMG_20211231_220540.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;
　　- &lt;strong&gt;海鲜烧烤&lt;/strong&gt;&lt;br&gt;
　　　　在建安街。很多次和朋友吃饭都会来这家，价格略贵，但味道很好。最爱红糖糍粑，每次和朋友去都会直接叫两份。&lt;br&gt;
　　- &lt;strong&gt;东北菜馆&lt;/strong&gt;&lt;br&gt;
　　　　在东湖学院附近。价格实惠，量也很足，虽然卖的最好的锅包肉我觉得很难吃&amp;hellip;&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/IMG_20211231_220653.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;规划2022&#34;&gt;规划2022
&lt;/h1&gt;&lt;p&gt;　　2022年上半年的任务不多，完成毕业设计、（可能）准备复试，（可能）准备找份工作，其余的时间就做做比赛，学习一些新东西。&lt;br&gt;
　　我不太喜欢给自己立一些Flag，不患得患失，确定要做什么事情尽力去做吧~&lt;/p&gt;</description>
        </item>
        <item>
        <title>2021 中国高校计算机大赛 - 微信大数据挑战赛</title>
        <link>https://demo.stack.jimmycai.com/p/2021-%E4%B8%AD%E5%9B%BD%E9%AB%98%E6%A0%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E8%B5%9B-%E5%BE%AE%E4%BF%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%91%E6%88%98%E8%B5%9B/</link>
        <pubDate>Tue, 29 Jun 2021 21:15:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/2021-%E4%B8%AD%E5%9B%BD%E9%AB%98%E6%A0%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E8%B5%9B-%E5%BE%AE%E4%BF%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%91%E6%88%98%E8%B5%9B/</guid>
        <description>&lt;p&gt;多任务推荐系统赛题，初赛Rank 68/6768，复赛Rank 70。&lt;/p&gt;
&lt;h1 id=&#34;2021中国高校计算机大赛---微信大数据挑战赛&#34;&gt;2021中国高校计算机大赛 - 微信大数据挑战赛
&lt;/h1&gt;&lt;h2 id=&#34;赛题描述&#34;&gt;赛题描述
&lt;/h2&gt;&lt;p&gt;　　本次比赛基于脱敏和采样后的数据信息，对于给定的一定数量到访过微信视频号“热门推荐”的用户， 根据这些用户在视频号内的历史n天的行为数据，通过算法在测试集上预测出这些用户对于不同视频内容的互动行为（包括点赞、点击头像、收藏、转发等）的发生概率。 本次比赛以多个行为预测结果的加权uAUC值进行评分。&lt;br&gt;
　　比赛提供训练集用于训练模型，测试集用于评估模型效果，提交结果demo文件用于展示提交结果的格式。 所有数据文件格式都是带表头的.csv格式，不同字段列之间用英文逗号分隔。初赛与复赛的数据分布一致，数据规模不同。 初赛提供百万级训练数据，复赛提供千万级训练数据。&lt;/p&gt;
&lt;h2 id=&#34;baseline&#34;&gt;Baseline
&lt;/h2&gt;&lt;p&gt;　　主办方为本次比赛提供了一份基线:&lt;a class=&#34;link&#34; href=&#34;https://github.com/WeChat-Big-Data-Challenge-2021/WeChat_Big_Data_Challenge&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Wechat_Big_Data_Challenge_Baseline&lt;/a&gt;，该基线基于Wide &amp;amp; Deep模型实现，除6列原始id特征和feed时长特征外，在id特征的基础上构造了一些统计特征。Weight_uAUC线下0.657003，线上0.607908。&lt;br&gt;
　　官方提供的这份基线分为数据集生成、离线模型训练、离线模型评估、在线模型训练、生成线上提交结果几个步骤，流程比较复杂，且线上线下gap较大，达到了5个百分点（经验证是统计特征涉及时间穿越，删去此部分特征可以提高2-3个百，群里也有人在基线基础上调参也能得到线上0.65+的分数），故我并没有过多参考此份基线。&lt;br&gt;
　　我所使用的是讨论区深度匹配树大佬所开源的基于MMoE的多任务学习模型，由于MMoE的多任务训练机制，训练速度相比四个任务逐个建模大大提升，可以迅速验证一些特征的有效性。线上分数约为0.635。&lt;br&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/wechat_algo_stage1/MMoE.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;MMoE&#34;
	
	
&gt;
　　TensorFlow版：&lt;a class=&#34;link&#34; href=&#34;https://github.com/zanshuxun/WeChat_Big_Data_Challenge_DeepCTR_baseline&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;mmoe_tf&lt;/a&gt;，Pytorch版：&lt;a class=&#34;link&#34; href=&#34;https://github.com/dpoqb/wechat_big_data_baseline_pytorch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;mmoe_torch&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;统计特征&#34;&gt;统计特征
&lt;/h2&gt;&lt;p&gt;　　基线中仅使用了6列id类特征，第一想法是在这六列id的基础上构建统计特征，由于数据带有时间序列的性质，提取特征时要注意时间穿越问题。我根据user、feed等多侧的历史行为，构建了点击、曝光、CTR等相关特征，这些特征在CTR类的比赛中非常常见，Kaggle、Github上也有非常多优秀的开源代码，故这部分特征的具体提取不再赘述。&lt;br&gt;
　　但将此部分统计特征喂给nn时，nn几乎不收敛，loss波动极大，归一化后线上成绩也非常低，于是开始思考什么样的特征适合喂给nn，开始下一阶段的特征构建。&lt;/p&gt;
&lt;h2 id=&#34;512维多模态向量&#34;&gt;512维多模态向量
&lt;/h2&gt;&lt;p&gt;　　这部分特征的正确使用能够获得较大幅度的提升，我尝试了两种方法，第一种是直接merge到训练集上，不过直接merge极容易OOM（除非内存足够），第二种是给feedid的嵌入赋值权重，不过后者经过实验效果不佳，也可能是我使用的方式不对。我租用的服务器配置为2*P40 + 112G RAM，将512维多模态向量经过PCA降维到48维后并在训练集上送入nn进行训练，线上线下均有大幅度提升，仅加入多模态这部分embedding特征后，线上成绩可以直接突破0.65。&lt;/p&gt;
&lt;h2 id=&#34;tagkeyword多值离散特征&#34;&gt;tag、keyword：多值离散特征
&lt;/h2&gt;&lt;p&gt;　　这部分特征的处理方式有很多，例如作为序列提取通过word2vec提取embedding，将每个离散取值当成词，整个tag/keyword列表当作句子，获取每个词的词向量后做pooling操作得到该部分的embedding，此处可参考&lt;a class=&#34;link&#34; href=&#34;https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==&amp;amp;mid=2247496721&amp;amp;idx=1&amp;amp;sn=c7fab106254f555cbea64e8464c84074&amp;amp;chksm=c32aed9ef45d64887f23606031d052c3fdce868b975644ca62db52bfd70185d4ddd212f0364f&amp;amp;mpshare=1&amp;amp;scene=23&amp;amp;srcid=0701JaeEB4Z1IDrtYDj3zaDK&amp;amp;sharer_sharetime=1625107770080&amp;amp;sharer_shareid=8c3bd21461ee94c3d6cc62dff5de10ac#rd&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;序列问题必备特征工程——基于Word2Vec的文本向量&lt;/a&gt;，或者通过embedding_lookup，此处可参考&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/149014347&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;推荐算法-4.多值离散特征的embedding解决方案&lt;/a&gt;。&lt;br&gt;
　　该部分特征对分数的提升也能达到7-8个千，我的线上分数也达到了0.659。&lt;/p&gt;
&lt;h2 id=&#34;多种子融合&#34;&gt;多种子融合
&lt;/h2&gt;&lt;p&gt;　　由于tensorflow的内部机制，导致其无法完全固定随机种子，同特征同参数下训练结果有一定幅度波动，波动幅度大概有2-3个千，通过多次训练，取平均可以稳定结果，线上成绩有约2个千的小幅度提升。&lt;/p&gt;
&lt;h2 id=&#34;树模型&#34;&gt;树模型
&lt;/h2&gt;&lt;p&gt;　　我在0.664附近卡了近两周的时间，比赛中后期的时候，天才儿童在6.14的周周星分享中开源了一份线上成绩0.645的&lt;a class=&#34;link&#34; href=&#34;https://developers.weixin.qq.com/community/minihome/article/doc/0006467d05427892b94c341aa56813&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;梯度提升决策树模型基线&lt;/a&gt;，当时队伍中仅我自己一人，仅靠单模进入复赛不太稳定，而且初赛由于数据采样的原因，树模型会比nn更有优势，故决定转手再做一个树模型。&lt;br&gt;
　　树模型中主要构造了一些统计类特征，和前文提到的类似，主要包括曝光、转化、视频观看等情况的滑窗统计特征，以及包括曝光、偏好等&lt;strong&gt;全局信息统计特征&lt;/strong&gt;（全局统计也能上分..Orz）。&lt;br&gt;
　　我在此份基线的基础上添加了nn中使用的几个embedding特征，树模型单模单折分数做到0.655。和我此前0.664的nn仅5/5平均后，就能够有3个k的提升。&lt;/p&gt;
&lt;h2 id=&#34;基于embedding的衍生特征&#34;&gt;基于embedding的衍生特征
&lt;/h2&gt;&lt;p&gt;　　这时距离比赛结束还有一周的时间，我在群里找到了一个做nn的和一个做树模型的队有，单模分数都在0.664上下，我和另一个队友的sub简单融合后分数达到了0.670，提升较大，然后继续优化nn单模。&lt;br&gt;
　　基于前面提取的多个embedding特征，我在此基础上又提取了一些衍生特征，方式包括滑窗pooling等，收益较大，简单衍生后就能提升3个千。&lt;/p&gt;
&lt;h2 id=&#34;初赛b榜&#34;&gt;初赛B榜
&lt;/h2&gt;&lt;p&gt;　　B榜数据和A榜的user无重叠，分布一致，排行榜上普遍有3个k到6个k的下降，我们的B榜最终分数为0.67093。&lt;/p&gt;
&lt;h2 id=&#34;待解决的问题&#34;&gt;待解决的问题
&lt;/h2&gt;&lt;h3 id=&#34;部分feed冷启动&#34;&gt;部分feed冷启动
&lt;/h3&gt;&lt;p&gt;　　简单观察数据可发现，第15天仍有2607个feed冷启动（未在user_action中出现），样本量为72758。&lt;br&gt;
　　对于冷启动问题，目前的基本思路是做矩阵SVD分解，构建用户和商品的交互矩阵，对稀疏矩阵进行SVD分解，得到用户和商品的向量，将用户向量和商品向量作为特征拼接到用户和商品侧。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/wechat_algo_stage1/feed%E5%86%B7%E5%90%AF%E5%8A%A8&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;每日冷启动feed数量&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;模型结构&#34;&gt;模型结构
&lt;/h3&gt;&lt;p&gt;　　比赛初期GDY郭大使用Transformer输入原始的id类特征轻松上到了0.65+，从后面周周星分享的一些思路也可以看出，模型结构的修改可以带来比较大的收益，例如参考DIN对用户的长短期兴趣进行表征等。&lt;/p&gt;
&lt;h2 id=&#34;参考代码&#34;&gt;参考代码
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/meurice996/WBDC2021_Solution&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/meurice996/WBDC2021_Solution&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>2021 招商银行 FinTech 数据赛道</title>
        <link>https://demo.stack.jimmycai.com/p/2021-%E6%8B%9B%E5%95%86%E9%93%B6%E8%A1%8C-fintech-%E6%95%B0%E6%8D%AE%E8%B5%9B%E9%81%93/</link>
        <pubDate>Tue, 18 May 2021 00:48:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/2021-%E6%8B%9B%E5%95%86%E9%93%B6%E8%A1%8C-fintech-%E6%95%B0%E6%8D%AE%E8%B5%9B%E9%81%93/</guid>
        <description>&lt;p&gt;时间序列回归赛题，一个简单的方案分享+指标MAPE翻车记录，B榜Rank53。magic number yyds~&lt;/p&gt;
&lt;h1 id=&#34;2021招商银行-fintech-精英训练营---数据赛道&#34;&gt;2021招商银行 FinTech 精英训练营 - 数据赛道
&lt;/h1&gt;&lt;h2 id=&#34;赛题任务&#34;&gt;赛题任务
&lt;/h2&gt;&lt;p&gt;　　本次竞赛给出的数据包含日期、节假日信息、时间段、岗位（含2种岗位A、B）、业务类型和业务量数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;任务1：预测未来31天各岗位每天的业务量总量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;任务2：预测未来31天各岗位每天每半小时粒度的业务总量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A榜：提供2018年1月1日到2020年10月31日的训练数据（train_v1），选手提交2020年11月1日到2020年11月30日的预测结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;B榜：提供2018年1月1日到2020年11月30日的训练数据（train_v2），选手提交2020年12月1日到2020年12月31日的预测结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
MAPE = \frac{1}{N} \sum_{i=1}^{N} \left| \frac{Y_i - \hat{Y_i}}{Y_i + 1} \right| \times 100\%\\
\end{aligned}
$$&lt;h2 id=&#34;解决方案&#34;&gt;解决方案
&lt;/h2&gt;&lt;h3 id=&#34;任务1&#34;&gt;任务1
&lt;/h3&gt;&lt;p&gt;　　简单观察数据可发现，A、B岗位业务量差距较大，故A、B岗位分开建模。
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/fintech2021/A%E5%B2%97%E4%BD%8D%E6%97%A5%E4%B8%9A%E5%8A%A1%E9%87%8F.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;A岗位日业务量&#34;
	
	
&gt;
&lt;img src=&#34;https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/fintech2021/B%E5%B2%97%E4%BD%8D%E6%97%A5%E4%B8%9A%E5%8A%A1%E9%87%8F.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;B岗位日业务量&#34;
	
	
&gt;
　　对于日业务量，我均采用&lt;strong&gt;LightGBM回归模型&lt;/strong&gt;进行预测。&lt;br&gt;
　　考虑到特殊时期对业务量的影响（特别是2020年上半年的Covid-19疫情），仅采用&lt;strong&gt;2018年3月1日至2018年11月30日&lt;/strong&gt;的数据作为训练集，采用&lt;strong&gt;2018年12月1日至2018年12月30日&lt;/strong&gt;的数据作为验证集（B榜，A榜划分类似，向前推一个月即可）。&lt;br&gt;
　　特征工程方面，所做的工作并不太多，特征包括&lt;strong&gt;日期特征&lt;/strong&gt;（day_of_week、day_of_month&amp;hellip;）、&lt;strong&gt;节假日特征&lt;/strong&gt;（节假日类型、距离下个工作日的天数、距离下个节假日的天数&amp;hellip;）等。&lt;br&gt;
　　&lt;strong&gt;后处理&lt;/strong&gt;是本题上分的一个关键点。观察数据，可以大致推断出接近年底的业务量是一个逐渐升高的趋势，将模型所预测的结果拼接到原数据集后再对整体走势做可视化分析，可以对模型预测结果的合理性做出大致判断。最开始，我尝试对预测的所有结果都乘一个系数（大约1.25左右，具体根据训练集和使用的特征等的不同会有一定的差异），获得了比较大的收益，随后我又对这个系数更加细化，月上旬、中旬、下旬分别乘不同的系数（逐渐递增），也获得了一定的提升，继续细化这个粒度（按周、按日）应该还会有提升，不过我并没有做更多这方面的尝试。&lt;br&gt;
　　&lt;strong&gt;PS&lt;/strong&gt;：关于后处理还有一些比较不寻常的方法，比如老肥将每一条预测结果都加上大小为666的偏移量，同样取得了较好的线上分数&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;任务2&#34;&gt;任务2
&lt;/h3&gt;&lt;p&gt;　　任务2同样采用LightGBM回归模型，训练数据与任务1相同，特征方面主要就增加了periods。由于与任务1相比，任务2的误差较大，所以我将任务2的训练从直接预测业务量改为了&lt;strong&gt;预测当前时间段的业务量占当天全部业务量的比例&lt;/strong&gt;，利用到了任务1的预测结果来调整任务2，这样做还一个好处是，任务1、2基本是同增同减的状态。&lt;/p&gt;
&lt;h2 id=&#34;关于指标mape&#34;&gt;关于指标MAPE
&lt;/h2&gt;&lt;p&gt;　　本次比赛中，我一开始就错误的将MAPE当作Lgb的metric，后面一直都忽略了这点，导致任务2的预测结果问题很大，特别是业务量较少（接近0）时，对指标的影响非常大。当我任务1优化到0.059时，任务2的MAPE仍为0.20，最终B榜也仅位于第53名，将metric调整为MSE即可。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当实际值为零时，MAPE会采用未定义的值，例如在需求预测中可能会发生这种情况。此外，当实际值非常接近零时，它将采用极值。&lt;/li&gt;
&lt;li&gt;MAPE是不对称的，它对负误差（当预测值高于实际值时）要比对正误差施加更大的罚款。解释如下：对于过低的预测，百分比误差不能超过100％。虽然没有太高的预测上限。因此，MAPE将偏向于预测不足而不是过度预测的模型。&lt;/li&gt;
&lt;li&gt;MAPE假定变量的度量单位具有有意义的零值。因此，尽管预测需求并使用MAPE是有意义的，但当预测温度以摄氏度（不仅是那个）表示时，却没有意义，因为温度具有任意零点。&lt;/li&gt;
&lt;li&gt;MAPE并非到处都是可微的，在将其用作优化标准时可能会导致问题。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>Hello World</title>
        <link>https://demo.stack.jimmycai.com/p/hello-world/</link>
        <pubDate>Wed, 15 Jul 2020 12:29:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/p/hello-world/</guid>
        <description></description>
        </item>
        <item>
        <title>Links</title>
        <link>https://demo.stack.jimmycai.com/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/links/</guid>
        <description>&lt;p&gt;To use this feature, add &lt;code&gt;links&lt;/code&gt; section to frontmatter.&lt;/p&gt;
&lt;p&gt;This page&amp;rsquo;s frontmatter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;links&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub is the world&amp;#39;s largest software development platform.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://www.typescriptlang.org&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ts-logo-128.jpg&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;image&lt;/code&gt; field accepts both local and external images.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>https://demo.stack.jimmycai.com/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://demo.stack.jimmycai.com/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
