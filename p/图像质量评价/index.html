<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="本文总结了图像质量评估的相关内容，同时简要总结了基于深度学习的无参考图像质量评价方法。\n">
<title>图像质量评价</title>

<link rel='canonical' href='https://demo.stack.jimmycai.com/p/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/'>

<link rel="stylesheet" href="/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css"><meta property='og:title' content="图像质量评价">
<meta property='og:description' content="本文总结了图像质量评估的相关内容，同时简要总结了基于深度学习的无参考图像质量评价方法。\n">
<meta property='og:url' content='https://demo.stack.jimmycai.com/p/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/'>
<meta property='og:site_name' content='zn.yan'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2022-07-31T22:24:28&#43;00:00'/><meta property='article:modified_time' content='2022-07-31T22:24:28&#43;00:00'/>
<meta name="twitter:title" content="图像质量评价">
<meta name="twitter:description" content="本文总结了图像质量评估的相关内容，同时简要总结了基于深度学习的无参考图像质量评价方法。\n">
    <link rel="shortcut icon" href="/favicon.png" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu2195195251581500850.jpg" width="300"
                            height="301" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">zn.yan</a></h1>
            <h2 class="site-description">Lorem ipsum dolor sit amet, consectetur adipiscing elit.</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='mailto:yanzhn@outlook.com'
                        target="_blank"
                        title="Email"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" width="24" height="24" stroke-width="2"> <path d="M3 7a2 2 0 0 1 2 -2h14a2 2 0 0 1 2 2v10a2 2 0 0 1 -2 2h-14a2 2 0 0 1 -2 -2v-10z"></path> <path d="M3 7l9 6l9 -6"></path> </svg> 
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/znyan'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.instagram.com/yanzhongnuo/'
                        target="_blank"
                        title="Instagram"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" width="24" height="24" stroke-width="2"> <path d="M4 4m0 4a4 4 0 0 1 4 -4h8a4 4 0 0 1 4 4v8a4 4 0 0 1 -4 4h-8a4 4 0 0 1 -4 -4z"></path> <path d="M12 12m-3 0a3 3 0 1 0 6 0a3 3 0 1 0 -6 0"></path> <path d="M16.5 7.5l0 .01"></path> </svg> 
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='#ZgotmplZ'
                        target="_blank"
                        title="WeChat"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" width="24" height="24" stroke-width="2"> <path d="M16.5 10c3.038 0 5.5 2.015 5.5 4.5c0 1.397 -.778 2.645 -2 3.47l0 2.03l-1.964 -1.178a6.649 6.649 0 0 1 -1.536 .178c-3.038 0 -5.5 -2.015 -5.5 -4.5s2.462 -4.5 5.5 -4.5z"></path> <path d="M11.197 15.698c-.69 .196 -1.43 .302 -2.197 .302a8.008 8.008 0 0 1 -2.612 -.432l-2.388 1.432v-2.801c-1.237 -1.082 -2 -2.564 -2 -4.199c0 -3.314 3.134 -6 7 -6c3.782 0 6.863 2.57 7 5.785l0 .233"></path> <path d="M10 8h.01"></path> <path d="M7 8h.01"></path> <path d="M15 14h.01"></path> <path d="M18 14h.01"></path> </svg> 
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>Dark Mode</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#主观评估s-iqa">主观评估S-IQA</a></li>
    <li><a href="#客观评估o-iqa">客观评估O-IQA</a>
      <ol>
        <li><a href="#评价指标">评价指标</a>
          <ol>
            <li><a href="#rmse">RMSE</a></li>
            <li><a href="#plcc皮尔逊系数">PLCC（皮尔逊系数）</a></li>
            <li><a href="#srocc">SROCC</a></li>
            <li><a href="#krocc">KROCC</a></li>
          </ol>
        </li>
        <li><a href="#全参考评估">全参考评估</a>
          <ol>
            <li><a href="#mse">MSE</a></li>
            <li><a href="#psnr峰值信噪比">PSNR（峰值信噪比）</a></li>
            <li><a href="#ssim">SSIM</a></li>
          </ol>
        </li>
        <li><a href="#半参考评估">半参考评估</a>
          <ol>
            <li><a href="#rred">RRED</a></li>
            <li><a href="#osvp">OSVP</a></li>
          </ol>
        </li>
        <li><a href="#无参考评估">无参考评估</a>
          <ol>
            <li><a href="#传统图像清晰度评价算法">传统图像清晰度评价算法</a></li>
            <li><a href="#reblur">Reblur</a></li>
            <li><a href="#distortion-specific">Distortion specific</a></li>
            <li><a href="#nss">NSS</a></li>
            <li><a href="#bag-of-words">Bag of words</a></li>
            <li><a href="#completely-blind">Completely blind</a></li>
            <li><a href="#handcraft">Handcraft</a></li>
            <li><a href="#deep-learning">Deep learning</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>

  <ol>
    <li><a href="#score-based">Score-based</a>
      <ol>
        <li><a href="#iqa-cnnpatchcnn直接预测质量">IQA-CNN（patch/CNN/直接预测质量）</a></li>
        <li><a href="#dliqanssdbn预测分类置信度">DLIQA（NSS/DBN/预测分类+置信度）</a></li>
        <li><a href="#deepiqapatchcnn预测质量权重">Deepiqa（patch/CNN/预测质量+权重）</a></li>
        <li><a href="#bieconpatchcnnfr中间图">BIECON（patch/CNN/FR中间图）</a></li>
        <li><a href="#diqam-nrwadiqam-nrpatchcnn预测质量权重">DIQaM-NR/WaDIQaM-NR（patch/CNN/预测质量+权重）</a></li>
        <li><a href="#nimaimagecnn预测主观分数分布">NIMA（image/CNN/预测主观分数分布）</a></li>
        <li><a href="#hallucinated-iqaimagegan生成幻觉图像感知差异">Hallucinated-IQA（image/GAN/生成“幻觉”图像感知差异）</a></li>
        <li><a href="#ran4iqapatchgan">RAN4IQA（patch/GAN）</a></li>
        <li><a href="#sfapatch语义特征聚集">SFA（patch/语义特征聚集）</a></li>
      </ol>
    </li>
    <li><a href="#rank-based">Rank-based</a>
      <ol>
        <li><a href="#gao-et-al">Gao et al</a></li>
        <li><a href="#dipiq">DipIQ</a></li>
        <li><a href="#rankiqa">RankIQA</a></li>
        <li><a href="#lfma">LFMA</a></li>
      </ol>
    </li>
    <li><a href="#multi-task">Multi-task</a>
      <ol>
        <li><a href="#biqi">BIQI</a></li>
        <li><a href="#iqa-cnn">IQA-CNN++</a></li>
        <li><a href="#mrliq">MRLIQ</a></li>
        <li><a href="#meon">MEON</a></li>
      </ol>
    </li>
    <li><a href="#more">More</a>
      <ol>
        <li><a href="#ser-fiq">SER-FIQ</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" >
                机器学习
            </a>
        
            <a href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" >
                图像处理
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/">图像质量评价</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Jul 31, 2022</time>
            </div>
        

        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <p>本文总结了图像质量评估的相关内容，同时简要总结了基于深度学习的无参考图像质量评价方法。</p>
<h1 id="image-quality-assessment">Image Quality Assessment
</h1>$$
\text{QA}\left\{
\begin{aligned}
&视频质量评估\text{VQA} \\
&图像质量评估\text{IQA}
\left\{
\begin{aligned}
&主观评估\text{S-IQA}\ ——\ 主观评分\\
&客观评估\text{O-IQA}
\left\{
\begin{aligned}
&全参考评估\text{FR-IQA} \\
&半参考评估\text{RR-IQA} \\
&无参考评估\text{NR-IQA} \\
\end{aligned}
\right.
\end{aligned}
\right.
\end{aligned}
\right.
$$<h2 id="主观评估s-iqa">主观评估S-IQA
</h2><p>主观评估方法主要可分为两种：绝对评价和相对评价。</p>
<p>绝对评价是由观察者根据自己的知识和理解，按照某些特定评价性能对图像的绝对好坏进行评价。在具体执行过程中通常采用双刺激连续质量分级法（Double Stimulus Continuous Scale, DSCQS）将待评价图像和原始图像按一定规则交替播放持续一定时间给观察者，然后在播放后留出一定的时间间隔供观察者打分，最后将所有给出的分数取平均作为该序列的评价值。</p>
<p>相对评估中没有原始图像作为参考，是由观察者对一批待评价图像进行相互比较，从而判断出每个图像的优劣顺序，并给出相应的评价值。在具体执行过程中通常采用单刺激连续质量评价方法（Single Stimulus Continuous QualityEvaluation, SSCQE）将一批待评价图像按照一定的序列播放，此时观察者在观看图像的同时给出待评图像相应的评价分值。</p>
<p>平均意见得分（Mean Opinion Score, MOS）是图像质量最具代表性的主观评价方法，它通过对观察者的评价归一判断图像质量。类似的评价方式还有平均主观得分差异（Differential mean opinion score, DMOS）。</p>
<p>绝对评价尺度：</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th style="text-align: center">分数</th>
          <th style="text-align: center">质量尺度</th>
          <th style="text-align: center">妨碍尺度</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">5</td>
          <td style="text-align: center">丝毫看不出图像质量变坏</td>
          <td style="text-align: center">非常好</td>
      </tr>
      <tr>
          <td style="text-align: center">4</td>
          <td style="text-align: center">能看出图像质量变化但不妨碍观看</td>
          <td style="text-align: center">好</td>
      </tr>
      <tr>
          <td style="text-align: center">3</td>
          <td style="text-align: center">清楚看出图像质量变坏，对观看稍有妨碍</td>
          <td style="text-align: center">一般</td>
      </tr>
      <tr>
          <td style="text-align: center">2</td>
          <td style="text-align: center">对观看有妨碍</td>
          <td style="text-align: center">差</td>
      </tr>
      <tr>
          <td style="text-align: center">1</td>
          <td style="text-align: center">非常严重的妨碍观看</td>
          <td style="text-align: center">非常差</td>
      </tr>
  </tbody>
</table></div>
<p>相对评价尺度的评分标准：</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th style="text-align: center">分数</th>
          <th style="text-align: center">相对测量尺度</th>
          <th style="text-align: center">绝对测量尺度</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">5</td>
          <td style="text-align: center">一群中最好的</td>
          <td style="text-align: center">非常好</td>
      </tr>
      <tr>
          <td style="text-align: center">4</td>
          <td style="text-align: center">好于该群中平均水平</td>
          <td style="text-align: center">好</td>
      </tr>
      <tr>
          <td style="text-align: center">3</td>
          <td style="text-align: center">该群中的平均水平</td>
          <td style="text-align: center">一般</td>
      </tr>
      <tr>
          <td style="text-align: center">2</td>
          <td style="text-align: center">差于该群中平均水平</td>
          <td style="text-align: center">差</td>
      </tr>
      <tr>
          <td style="text-align: center">1</td>
          <td style="text-align: center">该群中最差的</td>
          <td style="text-align: center">非常差</td>
      </tr>
  </tbody>
</table></div>
<h2 id="客观评估o-iqa">客观评估O-IQA
</h2><h3 id="评价指标">评价指标
</h3><p>检验一种客观评估算法是否可靠的标准是它“是否与人的主观质量判断相一致”，为了确认某种客观评价指标与主观得分之间的一致性关系，常用四个指标：RMSE、PLCC、SROCC和KROCC。</p>
<h4 id="rmse">RMSE
</h4>$$
\text{RMSE}=\sqrt{\frac{\sum_{i=1}^n(s_i-p_i)^2}{n}}
$$<p>
<strong>RMSE越接近0，表示算法的性能越好</strong>。为了避免MOS取值不一样而导致RMSE的计算受影响，所以计算前需要归一化。</p>
<h4 id="plcc皮尔逊系数">PLCC（皮尔逊系数）
</h4>$$
\text{PLCC}=\frac{Cov(S,P)}{\sigma(S)\cdot\sigma(P)}=\frac{\sum_{i=1}^n(p_i-\bar{p})(s_i-\bar{s})}{\sqrt{\sum_{i=1}^n(p_i-\bar{p})^2}\cdot\sqrt{\sum_{i=1}^n(s_i-\bar{s})^2}}
$$<p>PLCC取值范围为$[-1,1]$，当PLCC的值为零时，表示两组数据完全不相关，<strong>PLCC的值大于0时表示正相关，值越大表示正相关性越强</strong>。</p>
<h4 id="srocc">SROCC
</h4><p>SROCC和KROCC将具体数值抽象为排序等级。</p>
$$
\text{SROCC}=1-\frac{6\sum_{i=1}^nd_i^2}{n(n^2-1)} \tag{1}
$$<p>
其中$d$为$S$和$P$的等级之差（$rank(s)-rank(p)$）。</p>
<p>SROCC计算过程可参考下例：</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th style="text-align: center">$S$</th>
          <th style="text-align: center">$P$</th>
          <th style="text-align: center">$rank(s)$</th>
          <th style="text-align: center">$rank(p)$</th>
          <th style="text-align: center">$d$</th>
          <th style="text-align: center">$d^2$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">56</td>
          <td style="text-align: center">66</td>
          <td style="text-align: center">9</td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">5</td>
          <td style="text-align: center">25</td>
      </tr>
      <tr>
          <td style="text-align: center">75</td>
          <td style="text-align: center">70</td>
          <td style="text-align: center">3</td>
          <td style="text-align: center">2</td>
          <td style="text-align: center">1</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">45</td>
          <td style="text-align: center">40</td>
          <td style="text-align: center">10</td>
          <td style="text-align: center">10</td>
          <td style="text-align: center">0</td>
          <td style="text-align: center">0</td>
      </tr>
      <tr>
          <td style="text-align: center">71</td>
          <td style="text-align: center">60</td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">7</td>
          <td style="text-align: center">-3</td>
          <td style="text-align: center">9</td>
      </tr>
      <tr>
          <td style="text-align: center">62</td>
          <td style="text-align: center">65</td>
          <td style="text-align: center">6</td>
          <td style="text-align: center">5</td>
          <td style="text-align: center">1</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">64</td>
          <td style="text-align: center">56</td>
          <td style="text-align: center">5</td>
          <td style="text-align: center">9</td>
          <td style="text-align: center">-4</td>
          <td style="text-align: center">16</td>
      </tr>
      <tr>
          <td style="text-align: center">58</td>
          <td style="text-align: center">59</td>
          <td style="text-align: center">8</td>
          <td style="text-align: center">8</td>
          <td style="text-align: center">0</td>
          <td style="text-align: center">0</td>
      </tr>
      <tr>
          <td style="text-align: center">80</td>
          <td style="text-align: center">77</td>
          <td style="text-align: center">1</td>
          <td style="text-align: center">1</td>
          <td style="text-align: center">0</td>
          <td style="text-align: center">0</td>
      </tr>
      <tr>
          <td style="text-align: center">76</td>
          <td style="text-align: center">67</td>
          <td style="text-align: center">2</td>
          <td style="text-align: center">3</td>
          <td style="text-align: center">-1</td>
          <td style="text-align: center">1</td>
      </tr>
      <tr>
          <td style="text-align: center">61</td>
          <td style="text-align: center">63</td>
          <td style="text-align: center">7</td>
          <td style="text-align: center">6</td>
          <td style="text-align: center">1</td>
          <td style="text-align: center">1</td>
      </tr>
  </tbody>
</table></div>
<p>带入$d^2$的值即可求得SROCC。</p>
$$
\text{SROCC}=\frac{\sum_{i=1}^n(kp_i-\bar{kp})(ks_i-\bar{ks})}{\sqrt{\sum_{i=1}^n(kp_i-\bar{kp})^2}\cdot\sqrt{\sum_{i=1}^n(ks_i-\bar{ks})^2}}\tag{2}
$$<p>其中$kp$、$ks$分别表示$rank(s)$、$rank(p)$。</p>
<p>例如，对于两个第二名，则将等级定位1.5（第一和第二的平均）；对于两个第三名，则将等级定位3.5。</p>
<p>可以看出，SROCC就是“等级”的PLCC。</p>
<h4 id="krocc">KROCC
</h4><p>将MOS和算法预测得分表示为数据对的形式：$(s_1,p_1),(s_2,p_2),\cdots,(s_n,p_n)$，从$n$个数据对中任选两对，组成$[(x_i,y_i),(x_j,y_j)]$（$i\neq j$），一共有$\frac{n(n+1)}{2}$对，将这些对按照下面的情况进行划分：</p>
<ul>
<li>$P$：$x_i\gt x_j\ and\ y_i\gt y_j$ 或 $x_i\lt x_j\ and\ y_i\lt y_j$ 同序对</li>
<li>$Q$：$x_i\gt x_j\ and\ y_i\lt y_j$ 或 $x_i\lt x_j\ and\ y_i\gt y_j$ 逆序对</li>
<li>$X_0$：$x_i=x_j\ and\ y_i\gt y_j$ 或 $x_i=x_j\ and\ y_i\lt y_j$</li>
<li>$Y_0$：$x_i\gt x_j\ and\ y_i=y_j$ 或 $x_i\lt x_j\ and\ y_i=y_j$</li>
<li>$XY_0$：$x_i=x_j\ and\ y_i=y_j$</li>
</ul>
$$
\text{KROCC}=\frac{P-Q}{\sqrt{P+Q+X_0}\cdot\sqrt{P+Q+Y_0}}
$$<h3 id="全参考评估">全参考评估
</h3><ul>
<li>MSE</li>
<li>PSNR</li>
<li>SSIM</li>
<li>VIF</li>
<li>FSIM</li>
<li>GMSD</li>
</ul>
<h4 id="mse">MSE
</h4>$$
\text{MSE}=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2
$$<h4 id="psnr峰值信噪比">PSNR（峰值信噪比）
</h4>$$
\text{PSNR}=10\lg\left(\frac{MAX_I^2}{MSE}\right)
$$<p>
其中，$MAX_I$为图片可能的最大像素值，例如对于8 bit存储的图像，$MAX_I=2^8-1=255$。</p>
<h4 id="ssim">SSIM
</h4><p>SSIM首先在文章Image Quality Assessment: From Error Visibility to Structural Similarity（IEEE-2004）被引入，作者提出两个要点：</p>
<ul>
<li>大多数图像质量评估技术依赖于量化参考图像和样本图像之间的误差。一种常用的度量是量化样本和参考图像之间对应的每个像素的值的差异（例如均方误差）。</li>
<li>人类视觉感知系统能够从一个场景中识别结构信息，从而识别从参考场景和样本场景中提取的信息之间的差异。因此，复制此行为的指标将在涉及区分样本图像和参考图像的任务中执行得更好。</li>
</ul>
<p>SSIM从一幅图像中提取3个关键特征：</p>
<ul>
<li>
$$
  \mu_x=\frac{1}{N}\sum_{i=1}^Nx_i
  $$<p>
其中$x_i$为图像$x$的第$i$个像素值。</p>
</li>
<li>
$$
  \sigma_x=\sqrt{\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_x)^2}
  $$</li>
<li>
<p>结构：通过一个合并公式来完成</p>
$$
  r(X,Y)=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}
  $$$$
  r=\frac{\sigma_{xy}}{\sigma_x\sigma_y}
  $$$$
  \sigma_{xy}=\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_x)(y_i-\mu_y)
  $$</li>
</ul>
$$
l(x,y)=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}
$$$$
c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}
$$$$
s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3}
$$$$
SSIM(x,y)=[l(x,y)]^\alpha\cdot[c(x,y)]^\beta\cdot [s(x,y)]^\gamma
$$<p>
$\alpha,\beta,\gamma$用来表示三个模块的重要性。</p>
$$
SSIM(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}
$$<h3 id="半参考评估">半参考评估
</h3><ul>
<li>RRED</li>
<li>OSVP</li>
</ul>
<h4 id="rred">RRED
</h4><ul>
<li>Reduced reference entropic differencing for image quality assessment（IEEE Trans. Image Process，2012）</li>
</ul>
<h4 id="osvp">OSVP
</h4><ul>
<li>Orientation selectivity based visual pattern for reduced-reference image quality assessment（Information Science，2016）</li>
</ul>
<h3 id="无参考评估">无参考评估
</h3><p>由于没有无失真源图像的参考信息，无参考质量评估方法仅根据失真图像来学习预测图像质量分数，难度大于全参考和部分参考评估方法。</p>
<h4 id="传统图像清晰度评价算法">传统图像清晰度评价算法
</h4><ul>
<li>Tenengrad梯度函数</li>
<li>SMD（灰度方差）函数</li>
<li>Brenner梯度函数</li>
<li>方差函数</li>
<li>能量梯度函数</li>
<li>Vollath函数</li>
</ul>
<h4 id="reblur">Reblur
</h4><p>如果一幅图像已经模糊了，那么再对它进行一次模糊处理，高频分量变化不大；但如果原图是清楚的，对它进行一次模糊处理，则高频分量变化会非常大。因此可以通过对待评测图像进行一次高斯模糊处理，得到该图像的退化图像，然后再比较原图像和退化图像相邻像素值的变化情况，根据变化的大小确定清晰度值的高低，计算结果越小表明图像越清晰，反之越模糊，这种思路可称作基于二次模糊的清晰度算法。</p>
<ul>
<li>NRSS（梯度结构相似度）</li>
</ul>
<h5 id="nrss">NRSS
</h5><p>NRSS算法的步骤如下：</p>
<ul>
<li>
<p>Step1. 为待评价图像构造参考图像：定义待评价图像为$I$，NRSS算法首先参考图像$I_r=LPF(I)$，即对待评价图像$I$进行低通滤波得到参考$I_r$；</p>
</li>
<li>
<p>Step2. 提取图像$I$和$I_r$的梯度信息：利用人眼对水平和竖直方向的边缘信息最为敏感的特性，使用Sobel算子分别提取水平和竖直方向的边缘信息，定义$I$和$I_r$的梯度图像是$G$和$G_r$；</p>
</li>
<li>
<p>Step3. 找出梯度图像$G$中梯度信息最丰富的$N$个图像块：通过计算方差找出梯度图像$G$中梯度信息最丰富的$N$个图像块，方差越大说明梯度信息越丰富，根据找到的$G$中的前$N$个块，找出对应的$G_r$的前$N$个块；</p>
</li>
<li>
$$
  \text{NRSS}=1-\frac{1}{N}\sum_{i=1}^NSSIM(x_i,y_i)
  $$</li>
</ul>
<h4 id="distortion-specific">Distortion specific
</h4><p>早期的传统方法通过假设存在特定某一类型的失真来评价图像质量，即量化特定失真类型，如块效应、模糊、振铃效应、噪声、压缩或传输损伤等。JNBM、CPBDM和LPCM专注于评价Blur类型的失真图像，NJQA和JPEG-NR分别评价噪声失真和JPEG压缩损伤失真。</p>
<ul>
<li>CPBDM</li>
<li>LPCM</li>
<li>NJQA</li>
<li>JPEG-NR</li>
</ul>
<h4 id="nss">NSS
</h4><p>近年来表现优良的无参考图像质量评估模型大部分都是基于自然场景统计特性 (Natural Scene Statistics, NSS)，在不对失真类型做任何假设的前提下设计提取图像特征，通过机器学习回归算法进行质量预测。所选特征具有广泛的感知相关性，且合适的回归模型能自适应地将特征映射到数据集中的主观质量分数，因此基于NSS特征的无参考图像质量评估方法比早期的模型更加通用和一般化。NSS表明经过适当规范化的高质量真实世界摄像图像会遵行一定的统计规律，基于NSS统计量的特征量更能准确预测图像失真。</p>
<ul>
<li>BRISQUE</li>
<li>GM-LOG</li>
<li>HIGRADE</li>
<li>FRIQUEE</li>
<li>VBLINDS[V]</li>
<li>VIDEVAL[V]</li>
</ul>
<h5 id="brisque">BRISQUE
</h5><ul>
<li>No-Reference Image Quality Assessment in the Spatial Domain（IEEE Trans. Image Process，2012）</li>
</ul>
<p>BRISQUE是首歌将图像的自然场景统计特性应用到图像质量评估上的模型。其思想是从图像中提取MSCN系数（mean subtracted contrast normalized, 均值减去对比度归一化），将MSCN系数拟合成非对称性广义高斯分布（AGGD），提取拟合的高斯分布的特征，输入到支持向量机SVM中做回归，从而得到图像质量的评估结果。</p>
<p>自然图像的像素强度分布与失真图像的像素强度分布不同。当我们对像素强度进行归一化并在这些归一化强度上计算分布时，分布上的差异更加明显。特别地，在归一化之后，自然图像的像素强度近似服从高斯分布（贝尔曲线），而非自然或失真图像的像素强度则不服从高斯分布。因此，分布曲线与理想高斯曲线的偏差是图像失真量的度量。</p>
<p>BRISQUE的整体流程有三步：</p>
<ul>
<li>Step1. 提取自然场景统计信息（NSS）</li>
<li>Step2. 计算特征向量</li>
<li>Step3. 预测图像质量分数</li>
</ul>
$$
\hat{I}(i,j)=\frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}
$$$$
\mu=\mathbf{W}*\mathbf{I}\quad\sigma=\sqrt{\mathbf{W}*(\mathbf{I}-\mu)^2}
$$$$
\begin{aligned}
H(i,j)&=\hat{H}(i,j)\cdot\hat{H(i,j+1)} \\
V(i,j)&=\hat{H}(i,j)\cdot\hat{H(i+1,j)} \\
D1(i,j)&=\hat{H}(i,j)\cdot\hat{H(i+1,j+1)} \\
D2(i,j)&=\hat{H}(i,j)\cdot\hat{H(i+1,j-1)} \\
\end{aligned}
$$<p>
至此，已从原始图像中生成了5张图像：1张MSCN图像和4张成对乘积图像。</p>
<p>接下来，我们将使用这5张图像来计算大小为$36\times1$的特征向量。</p>
$$
\begin{aligned}
f(x;\alpha,\sigma^2)=\frac{\alpha}{2\beta\Gamma(1/\alpha)}\exp\left(-\left(\frac{|x|}{\beta}\right)^2\right)
\end{aligned}
$$$$
\beta=\sigma\sqrt{\frac{\Gamma(1/\alpha)}{\Gamma(3/\alpha)}},\quad\Gamma(a)=\int_0^\infty t^{a-1}e^{-t}dt\quad (a\gt0).
$$$$
f(x;\nu,\sigma_l^2,\sigma_r^2)=
\left\{
\begin{aligned}
&\frac{\nu}{(\beta_l+\beta_r)\Gamma(1/\nu)}\exp\left(-\left(\frac{-x}{\beta_l}\right)^\nu\right)\quad x\lt 0 \\
&\frac{\nu}{(\beta_l+\beta_r)\Gamma(1/\nu)}\exp\left(-\left(\frac{-x}{\beta_r}\right)^\nu\right)\quad x\geqslant 0
\end{aligned}
\right.
$$$$
\beta_l=\sigma_l\sqrt{\frac{\Gamma(1/\nu)}{\Gamma(3/\nu)}},\quad \beta_r=\sigma_r\sqrt{\frac{\Gamma(1/\nu)}{\Gamma(3/\nu)}}.
$$<p>
将原始图像缩小到原始大小的一半，并重复相同过程，便得到了$36\times1$的特征向量。</p>
<p>将特征向量送入到机器学习算法中进行训练，即可使用模型对质量分数进行预测。</p>
<p>用广义高斯分布来拟合MSCN的分布，GGD的形状参数α和分布方差sigma。接下来对MSCN系数进行二阶分析，在垂直、水平、主对角和次对角方向上进行非对称广义高斯分布的拟合，分别得到表征分布形状的四个参数。这样在原图像尺度上得到18个特征。图像和视频本质上是多尺度的，失真可以在不同的尺度上表现得不同。因此在降采样2倍的图像上再次提取18维度的特征，这样BRISQUE的特征集是36维。通过机器学习训练出能够从高维特征映射到低维MOS分数上的回归模型。</p>
<h4 id="bag-of-words">Bag of words
</h4><p>不同于以上基于NSS特征提取模型，传统无参考图像质量评估的另一个方向是词袋 (Bag of Words, BOW) 模型。</p>
<ul>
<li>CORNIA</li>
<li>HOSA</li>
</ul>
<h5 id="cornia">CORNIA
</h5><ul>
<li>Unsupervised Feature Learning Framework for No-reference Image Quality Assessment（IEEE Conf. Comput. Vis. Pattern Recognit.，2012）</li>
</ul>
<h4 id="completely-blind">Completely blind
</h4><p>Completely Blind方法不需要在数据集上进行训练来学习特征到MOS分数的映射，而是能够通过待测图像或者视频直接输出得到质量分数。</p>
<ul>
<li>NIQE</li>
<li>IL-NIQE</li>
<li>SLEEQ[V]</li>
<li>STEM[V]</li>
</ul>
<h5 id="niqe">NIQE
</h5><ul>
<li>Making a &ldquo;Completely Blind&rdquo; Image Quality Analyzer（IEEE Signal Process，2013）</li>
</ul>
<p>NIQE（natural image quality evaluator）是Mittal等人提出的基于自然场景统计的盲图像质量评价模型。该方法仅利用从自然图像中观察到的统计规律进行失真偏差的度量，通过构建一系列质量相关的统计特征以实现对图像的质量预测。</p>
<p>NIQE算法有以下几个步骤：</p>
<ul>
<li>Step1. Spatial Domain NSS：提取NSS特征</li>
<li>Step2. Patch Selection：图像划分、选择</li>
<li>Step3. Characterizing Image Patches：提取Patches的特征</li>
<li>Step4. Multivatiate Gaussian Model（MVG）：拟合多元高斯分布</li>
<li>Step5. NIQE Index：计算NIQE分数</li>
</ul>
$$
\hat{I}(i,j)=\frac{I(i,j)-\mu(i,j)}{\sigma(i,j)+C}
$$<p>
其中，$i \in 1,2,\dots,M,j\in 1,2,\dots,N$，$M,N$是图像的高和宽，$C=1$是为了数值稳定的常数；$\omega={ \omega_{k,l}\vert k=-K,\dots,K,l=-L,\dots,L}$是高斯核。</p>
$$
\delta(b)={\sum\sum}_{(i,j)\in patch_b}\sigma(i,j) \quad b=1,2,\dots,P\times P
$$<p>
设定阈值$T$，若$\delta(b)\gt T$，则认为$patch_b$是锐利的。将所有图像块的最大锐利程度的 $p$倍设为阈值，其中$p \in [0.6, 0.9]$，论文中取值为0.75。将大于阈值的图像块保留，小于阈值的图像块淘汰掉。</p>
<p>选取一些patches后，类似于BRISQUE中的方法，在不同尺度下拟合GGD和AGGD得到36维特征。</p>
$$
f_X(x_1,\dots,x_k)=\frac{1}{(2\pi)^{k/2}\vert\Sigma\vert^{1/2}}\exp(-\frac{1}{2}(x-\nu)^T\Sigma^{-1}(x-\nu))
$$<p>
注：</p>
<ul>
<li>这个模型是由一组清晰图像得到的，用来计算低质量图像与它的距离；</li>
<li>采用高斯分布来处理这些特征的基本前提是假设这里所涉及的特征在真实的图像中所反映的也是服从高斯分布的。</li>
</ul>
$$
D(\nu_1,\nu_2,\Sigma_1,\Sigma_2)=\sqrt{(\nu_1-\nu_2)^T(\frac{\Sigma_1+\Sigma_2}{2})^{-1}(\nu_1-\nu_2)}
$$<p>
由上式即可得出最终得分，$\nu_1,\Sigma_1$是一组清晰图像得到的均值向量和协方差矩阵，$\nu_2,\Sigma_2$是输入的图像得到的均值向量和协方差矩阵。</p>
<h4 id="handcraft">Handcraft
</h4><ul>
<li>TLVQM[V]</li>
</ul>
<h4 id="deep-learning">Deep learning
</h4><ul>
<li>VSFA[V]</li>
<li>VMEON[V]</li>
<li>PVQ[V]</li>
<li>RAPIQUE[V]</li>
<li>CNNIQA</li>
<li>PaQ-2-PIQ</li>
<li>Hallucinated-IQA</li>
</ul>
<h1 id="nr-iqa中的deep-based方法">NR-IQA中的Deep-based方法
</h1><h2 id="score-based">Score-based
</h2>$$
\text{Score-based}\left\{
\begin{aligned}
&\text{Patch-wise} \\
&\text{Image-wise}
\end{aligned}
\right.
$$<p>Score-based方法可以分为Patch-wise和Image-wise。Image-wise直接将图片输入到CNN中；Patch-wise对图像进行分块并分别输入CNN，pooling后得到图像质量。</p>
<h3 id="iqa-cnnpatchcnn直接预测质量">IQA-CNN（patch/CNN/直接预测质量）
</h3><ul>
<li>L. Kang, P. Ye, Y. Li, and D. Doermann, “Convolutional neural networks for no-reference image quality assessment,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014.</li>
</ul>
<p>IQA-CNN是首个将空间卷积神经网络模型在图像质量评价领域应用的工作，IQA-CNN以patch作为输入，由一层卷积、最大最小池化以及两层全连接组成，将特征学习和回归集成到一个优化过程中。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/4.png" style="zoom:67%;" />
<h3 id="dliqanssdbn预测分类置信度">DLIQA（NSS/DBN/预测分类+置信度）
</h3><ul>
<li>W. Hou, X. Gao, D. Tao, and X. Li, “Blind image quality assessment via deep learning,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 6, pp. 1275–1286, Jun. 2015.</li>
</ul>
<p>心理学表明人类更加偏向于定性的评价而不是定量的评价，一幅图片打分为70还是75实际上是很难抉择的问题，用excellent、good、bad这样的语言定性更加自然。</p>
<p>DLIQA将BIQA（Bind Image Quality Assessment）作为一个五分类问题，输入图像用NSS特征表示，分类的依据是分类器得到的置信度，然后将分类和对应的置信度转为质量分数。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/5.png" style="zoom:67%;" />
<p>作者认为IQA问题并不适合用切成patch的方式来做，但直接输入整张图像维度太高，所以使用了可以表征自然图像和畸变图像的差异的NSS特征作为输入。</p>
<p>为了将分类结果和置信度转化为质量分数，首先有以下假设：</p>
<ul>
<li>每张图片都有一个内在质量$Q$；</li>
<li>每个训练有素的人在评估具有相同内在质量的图像时，都会给出相同的标签。</li>
</ul>
<p>主观评分的似然分布$P(L=\text{Excellent|Q}),P(L=\text{Good}|Q),P(L=\text{Fair}|Q),P(L=\text{Poor}|Q),P(L=\text{Bad}|Q)$和先验分布$P(Q)$如下：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/6.png" style="zoom:67%;" />
<p>然后通过三角形分布函数和平均分布来模拟各个似然函数和先验分布：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/11.png" style="zoom:67%;" />
$$
P(Q|L)\sim P(L|Q)P(Q)
$$$$
P(Q|X)=\int P(Q|L)P(L|X)dL
$$$$
\text{Quality}=\mathbb{E}[P(Q|X)]
$$<h3 id="deepiqapatchcnn预测质量权重">Deepiqa（patch/CNN/预测质量+权重）
</h3><ul>
<li>S. Bosse, D. Maniry, T. Wiegand, and W. Samek, “A deep neural network for image quality assessment,” IEEE International Conference on Image Processing, 2016.</li>
</ul>
<p>Deepiqa将无任何预处理的图像patch作为输入，通过池化获得最终的质量分数，网络的输出定义为2维，其中一维输出图像patch的质量分数的对应权重，通过该权重得到最终的质量分数。</p>
<h3 id="bieconpatchcnnfr中间图">BIECON（patch/CNN/FR中间图）
</h3><p>使用部分全参考图像质量评价方法中的局部质量图作为中间结果对模型进行训练。</p>
<h3 id="diqam-nrwadiqam-nrpatchcnn预测质量权重">DIQaM-NR/WaDIQaM-NR（patch/CNN/预测质量+权重）
</h3><ul>
<li>S. Bosse, D. Maniry, K.-R. M ̈uller, T. Wiegand, and W. Samek, &ldquo;Deep neural networks for no-reference and full-reference image quality assessment,&rdquo; IEEE Transactions on image processing, vol. 27, no. 1, pp. 206–219, 2017.</li>
<li><a class="link" href="https://ieeexplore.ieee.org/document/8063957"  target="_blank" rel="noopener"
    >Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment | IEEE Journals &amp; Magazine | IEEE Xplore</a></li>
</ul>
<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/1.png"
	
	
	
	loading="lazy"
	
	
></p>
<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/2.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>以上分别为FR和NR模型的结构，FR模型去掉孪生网络的分支就是NR模型。</p>
<p>FR模型的两个输入（distorted和reference）通过一个共享参数的VGG-Style网络，得到两个特征向量$f_r,f_d$，然后将$\text{concat}(f_r,f_d,f_r-f_d)$用于回归，一个网络用于回归质量，另一个用于回归权重，pooling后得到最终的图像质量。</p>
<h3 id="nimaimagecnn预测主观分数分布">NIMA（image/CNN/预测主观分数分布）
</h3><ul>
<li>H. Talebi, P. Milanfar, &ldquo;NIMA: neural image assessment,&rdquo; IEEE Trans. Image Process., vol. 27, no. 8, pp. 3998-4011, Aug. 2018.</li>
</ul>
<p>不同于常见的深度图像质量评价方法，NIMA对人类主观分数的分布$\hat{\mathbf{p}}$进行预测。</p>
$$
\mathbf{p}=[p_{s_1},\dots, p_{s_N}]\quad\sum_{i=1}^Np_{s_i}=1,s_1\leqslant s_i \leqslant s_N
$$<p>
其中$s_i$表示第$i$个分值，$N$表示分值总数。</p>
$$
\mu=\sum_{i=1}^Ns_i\cdot p_{s_i}
$$$$
\sigma=\sqrt{\sum_{i=1}^N(s_i-\mu)^2\cdotp_{s_i}}
$$<p>
NIMA中CNN后接的全连接为10维（AVA和TID数据集中均有$N=10$）。</p>
$$
EMD(\mathbf{p},\hat{\mathbf{p}})=\left(\frac{1}{N}\sum_{k=1}^N|CDF_\mathbf{p}(k)-CDF_{\hat{\mathbf{p}}}(k)|\right)^\frac{1}{r}
$$<p>
其中累计分布函数$CDF_\mathbf{p}(k)=\sum_{i=1}^k p_{s_i}$。</p>
<h3 id="hallucinated-iqaimagegan生成幻觉图像感知差异">Hallucinated-IQA（image/GAN/生成“幻觉”图像感知差异）
</h3><ul>
<li>K. Lin, G. Wang, “Hallucinated-IQA: no-reference image quality assessment via adversarial learning” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018.</li>
</ul>
<p>IQA在失真图像的基础上产生一个“幻觉”参考图像，通过捕捉失真图像和“幻觉”图像之间的感知差异来预测图像质量。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/7.png" style="zoom:50%;" />
<p>Hallucinated-IQA模型包括生成网络$G$、IQA判别网络$D$和回归网络$R$三个部分，结构如下图所示：</p>
<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/8.png"
	
	
	
	loading="lazy"
	
	
></p>
$$
\hat{\theta}=\arg\min_\theta \frac{1}{N}\sum_{i=1}^N(l_p(G_\theta(I_d^i), I_r^i)+l_s(G_\theta(I_d^i),I_r^i))
$$$$
l_s(G_\theta(I_d^i),I_r^i)=\Vert\phi(G_\theta(I_d^i))-\phi(I_r^i)\Vert^2
$$$$
l_s(G_\theta(I_d^i),I_r^i)=\lambda_1l_v(G_\theta(I_d^i),I_r^i)+\lambda_2l_q(G_\theta(I_d^i),I_r^i)
$$$$
\begin{aligned}
l_v&=\sum_{c_v=1}^{C_v}\frac{1}{W_jH_j}\sum_{x=1}^{W_j}\sum_{y=1}^{H_j}\Vert\phi_j(G_\theta(I_d^i))_{x,y}-\phi_j(I_r^i)_{x,y}\Vert^2 \\
l_q&=\sum_{c_q=1}^{C_q}\frac{1}{W_kH_k}\sum_{x=1}^{W_k}\sum_{y=1}^{H_k}\Vert\pi_k(G_\theta(I_d^i))_{x,y}-\pi_k(I_r^i)_{x,y}\Vert^2
\end{aligned}
$$<p>
其中$C$表示某一层的特征图数量，$W,H$表示特征图的维度，$\phi_j(\cdot)$表示VGG-19在第$j$层的特征图，$\pi_k(\cdot)$表示$R$在第$k$层的特征图。</p>
$$
\max_\omega\mathbb{E}[\log D_\omega(\mathbf{I_r})]+\mathbb{E}[\log(1-\vert D_\omega(G_\theta(\mathbf{I}_d))-\mathbf{d}_{fake}\vert)]
$$<p>
其中，$\mathbf{d}<em>{fake}$定义为：
$$
\mathbf{d}_{fake}^i=\left\{
\begin{aligned}
&1\quad \text{if}\ \Vert R(I_d^i,I_{sh}^i)-s^i\Vert_F\lt \epsilon\\
&0\quad \text{if}\ \Vert R(I_d^i,I_{sh}^i)-s^i\Vert_F\geqslant \epsilon
\end{aligned}
\right.
$$
该式所表达的是，当回归网络预测出来的分数与真实质量分数的差距大于阈值时，认为“幻觉”图像降低了回归网络的性能，$\mathbf{d}</em>{fake}$为0，此时为了最大化损失函数，需要IQA判别网络将生成图像判别为0，真实图像判别为1；当回归网络预测出来得分数与真实质量分数得差距小于阈值时，幻觉图像提升了回归网络的性能，$\mathbf{d}_{fake}$为1，此时为了最大化损失函数，需要辨别器将生成图像和真实图像均辨别为1。、</p>
$$
\mathcal{L}_{adv}=\mathbb{E}[\log(1-D_\omega(G_\theta(I_d)))]
$$$$
\mathcal{L}_G=\mu_1\mathcal{L}_p+\mu_2\mathcal{L}_s+\mu_3\mathcal{L}_{adv}
$$$$
\hat{\gamma}=\arg\min_r\frac{1}{N}\sum_{i=1}^Nl_r(\mathcal{R}(I_d^i,I_{map}^i),s^i)
$$<p>
其中$I_{map}=\vert I_d-G_{\hat{\theta}}(I_d)\vert$。</p>
<p>$R$的精确度在很大程度上取决于“幻觉”场景的合格性。具体地说，合格的幻觉图像作为代理参照可以帮助$R$探索失真图像的感知差异，而不合格的“幻觉”图像则会引入对$R$的偏差。</p>
$$
\mathcal{F}=f(\mathcal{H}_{5,2}(I_d))\otimes(\mathcal{R}_1(I_d,I_{map}))
$$$$
\mathcal{L_R}=\frac{1}{T}\sum_{t=1}^T\Vert\mathcal{R}_2(f(\mathcal{H}_{5,2}(I_d))\otimes (\mathcal{R}_1(I_d,I_{map})))-s^t\Vert_{\ell_1}
$$<h3 id="ran4iqapatchgan">RAN4IQA（patch/GAN）
</h3><ul>
<li>H. Ren, D. Chen, Y. Wang, “RAN4IQA: Restorative Adversarial Nets for No-reference Image Quality Assessment,” in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</li>
</ul>
<p>通过生成式对抗网络对输入的失真图像进行修复，基于修复收益（gain of restoration, GoR）提取失真图像和修复图像的特征并进行比较以感知质量。</p>
<h3 id="sfapatch语义特征聚集">SFA（patch/语义特征聚集）
</h3><ul>
<li>D. Li, T. Jiang, W. Lin, and M. Jiang, “Which has better visual quality: the clear blue sky or a blurry animal?,” IEEE Trans. Multimedia., vol. 21, no. 5, pp. 1221-1234, Nov. 2019.</li>
</ul>
<h2 id="rank-based">Rank-based
</h2><h3 id="gao-et-al">Gao et al
</h3><ul>
<li>F. Gao, D. Tao, X. Gao, and X. Li, “Learning to rank for blind image quality assessment,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 10, pp. 2275–2290, Oct. 2015.</li>
<li>[<a class="link" href="https://arxiv.org/abs/1309.0213v2"  target="_blank" rel="noopener"
    >1309.0213v2] Learning to Rank for Blind Image Quality Assessment (arxiv.org)</a></li>
</ul>
<p>Blind image quality assessment（BIQA）旨在预测图像质量分数，但图像质量分数的获取有一定的局限性：</p>
<ul>
<li>分数不精确；</li>
<li>主观判断不准确；</li>
<li>不同失真类别之间的质量尺度不一致；</li>
<li>大规模数据难以获取。</li>
</ul>
<p>作者提出基于偏好图像对（preference image pair, PIPs），对于一个偏好图像对而言，其标签表示的是图像一的质量好于图像二的质量。</p>
<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/9.png"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="dipiq">DipIQ
</h3><ul>
<li>K. Ma, W. Liu, T. Liu, Z. Wang and D. Tao, &ldquo;dipIQ: blind image quality assessment by learning-to-rank discriminable image pairs,&rdquo; IEEE Trans. Image Process., vol. 26, no. 8, pp. 3951-3963, Aug. 2017.</li>
</ul>
<h3 id="rankiqa">RankIQA
</h3><ul>
<li>X. Liu, J. V. D. Weijer, and A. D. Bagdanov, “RankIQA: learning from rankings for no-reference image quality assessment,” in Proc. IEEE Int. Conf. Comput. Vis. 2017.</li>
<li>X. Liu, J. V. D. Weijer, and A. D. Bagdanov, “Exploiting unlabeled data in cnns by self-supervised learning to rank,” IEEE Trans. Pattern. Anal. Mach. Intell., vol. 41, no. 8, pp. 1862-1878, Aug. 2019.</li>
</ul>
<h3 id="lfma">LFMA
</h3><ul>
<li>K. Ma, X. Liu, Y. Fang, and E. P. Simoncelli, “Blind image quality assessment by learning from multiple annotators,” IEEE International Conference on Image Processing, 2019.</li>
</ul>
<h2 id="multi-task">Multi-task
</h2><h3 id="biqi">BIQI
</h3><ul>
<li>A. K. Moorthy, A. C. Bovik, “A two-step framework for constructing blind image quality indices,” IEEE Signal Process. Lett., vol. 17, no. 5, pp. 513-516, May. 2010.</li>
</ul>
<h3 id="iqa-cnn">IQA-CNN++
</h3><ul>
<li>L. Kang, P. Ye, Y. Li, and D. Doermann, “Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks,” in Proc. IEEE Int. Conf. Image Process., 2015, pp. 2791–2795.</li>
</ul>
<h3 id="mrliq">MRLIQ
</h3><ul>
<li>L. Xu, J. Li, W. Lin, Y. Zhang and L. Ma, Y. Fang, and Y. Yan, “Multi-task rank learning for image quality assessment,” IEEE Trans. Circuits Syst. Video Technol., vol. 27, no. 9, pp. 1833-1843, Sep. 2017.</li>
</ul>
<h3 id="meon">MEON
</h3><ul>
<li>K. Ma, W. Liu, T. Liu, K. Zhang, Z. Duanmu, Z. Wang, and W. Zuo, &ldquo;End-to-end blind image quality assessment using deep neural networks,&rdquo; IEEE Trans. Image Process., vol. 27, no. 3, pp. 1202-1213, Mar. 2018.</li>
</ul>
<p>MEON是用于BIQA（Bind Image Quality Assessment）多任务端到端优化的深度神经网络，包括失真判别网络和质量预测网络两部分。</p>
<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/3.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>MEON的训练过程分为两个阶段：原始图像经过共享参数层，两个子网络一个用于失真类型判别，另一个用于质量预测，并且其中质量预测阶段使用了失真类型判别子网络的输出。</p>
$$
y_i(m,n)=\frac{x_i(m,n)}{\left(\beta_i+\sum_{j=1}^S\gamma_{ij}x_j(m,n)^2\right)^\frac{1}{2}}
$$<p>
$y_i$是根据$x_i$在空间位置$(m,n)$的激活响应，$\beta$和$\gamma$在训练过程中进行优化，GDN操作是一个可微的变换。</p>
<h2 id="more">More
</h2><h3 id="ser-fiq">SER-FIQ
</h3><ul>
<li>SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness</li>
<li>[<a class="link" href="https://arxiv.org/abs/2003.09373"  target="_blank" rel="noopener"
    >2003.09373] SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness (arxiv.org)</a></li>
</ul>
<p>SER-FIQ用于人脸图像质量评估，作者认为人脸图像的质量来自于图像的embedding的鲁棒性，同一张图片经过人脸识别模型的不同的子网络得到的不同的embedding的方差反映了人脸图像的质量。</p>
$$
X(I)=\{x_s\},s\in 1,2,\dots,m
$$$$
q(X(I))=2\sigma\left(-\frac{2}{m^2}\sum_{i\lt j} d(x_i,x_j)\right)
$$<p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/IQA/10.png" style="zoom:75%;" /></p>
</section>


    <footer class="article-footer">
    

    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/p/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
        
        

        <div class="article-details">
            <h2 class="article-title">图神经网络</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/">
        
        

        <div class="article-details">
            <h2 class="article-title">长短期记忆网络</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/yolo-v1-v7/">
        
        

        <div class="article-details">
            <h2 class="article-title">YOLO v1 - v7</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/">
        
        

        <div class="article-details">
            <h2 class="article-title">异常检测</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/">
        
        

        <div class="article-details">
            <h2 class="article-title">自编码器</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2024 zn.yan
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
